{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# import linear algebra and data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import matplotlib for plotting\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import helper libraries\n",
    "import requests\n",
    "from io import BytesIO # Use When expecting bytes-like objects\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from os import path\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "# import PIL for image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"src/\")\n",
    "import image_utils\n",
    "from image_utils import add_flipped_and_rotated_images\n",
    "from simple_conv_nn import SimpleCNN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Function loads quick draw dataset. If no data is loaded yet, the datasets\n",
    "    are loaded from the web. If there are already loaded datasets, then data\n",
    "    is loaded from the disk (pickle files).\n",
    "\n",
    "    INPUTS: None\n",
    "\n",
    "    OUTPUT:\n",
    "        X_train - train dataset\n",
    "        y_train - train dataset labels\n",
    "        X_test - test dataset\n",
    "        y_test - test dataset labels\n",
    "    \"\"\"\n",
    "    print(\"Loading data \\n\")\n",
    "\n",
    "    # Check for already loaded datasets\n",
    "    if not(path.exists('xtrain_doodle.pickle')):\n",
    "        # Load from web\n",
    "        print(\"Loading data from the web \\n\")\n",
    "\n",
    "        # Classes we will load\n",
    "        categories = ['bee', 'cat', 'cow', 'dog', 'duck', 'horse', 'pig', 'rabbit', 'snake', 'whale']\n",
    "\n",
    "        # Dictionary for URL and class labels\n",
    "        URL_DATA = {}\n",
    "        for category in categories:\n",
    "            URL_DATA[category] = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/' + category +'.npy'\n",
    "\n",
    "        # Load data for classes in dictionary\n",
    "        classes_dict = {}\n",
    "        for key, value in URL_DATA.items():\n",
    "            response = requests.get(value)\n",
    "            classes_dict[key] = np.load(BytesIO(response.content))\n",
    "\n",
    "        # Generate labels and add labels to loaded data\n",
    "        for i, (key, value) in enumerate(classes_dict.items()):\n",
    "            value = value.astype('float32')/255.\n",
    "            if i == 0:\n",
    "                classes_dict[key] = np.c_[value, np.zeros(len(value))]\n",
    "            else:\n",
    "                classes_dict[key] = np.c_[value,i*np.ones(len(value))]\n",
    "\n",
    "        # Create a dict with label codes\n",
    "        label_dict = {0:'bee', 1:'cat', 2:'cow', 3:'dog', 4:'duck',\n",
    "                      5:'horse', 6:'pig', 7:'rabbit', 8:'snake', 9:'whale'}\n",
    "\n",
    "        lst = []\n",
    "        for key, value in classes_dict.items():\n",
    "            lst.append(value[:3000])\n",
    "        doodles = np.concatenate(lst)\n",
    "\n",
    "        # Split the data into features and class labels (X & y respectively)\n",
    "        y = doodles[:,-1].astype('float32')\n",
    "        X = doodles[:,:784]\n",
    "\n",
    "        # Split each dataset into train/test splits\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "    else:\n",
    "        # Load data from pickle files\n",
    "        print(\"Loading data from pickle files \\n\")\n",
    "\n",
    "        file = open(\"xtrain_doodle.pickle\",'rb')\n",
    "        X_train = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"xtest_doodle.pickle\",'rb')\n",
    "        X_test = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"ytrain_doodle.pickle\",'rb')\n",
    "        y_train = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"ytest_doodle.pickle\",'rb')\n",
    "        y_test = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, classes_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "categories = ['bee', 'cat', 'cow', 'dog', 'duck', 'horse', 'pig', 'rabbit', 'snake', 'whale']\n",
    "\n",
    "URL_DATA = {}\n",
    "for category in categories:\n",
    "    URL_DATA[category] = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/' + category +'.npy'\n",
    "\n",
    "URL_DATA"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bee': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bee.npy',\n",
       " 'cat': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cat.npy',\n",
       " 'cow': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cow.npy',\n",
       " 'dog': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy',\n",
       " 'duck': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/duck.npy',\n",
       " 'horse': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/horse.npy',\n",
       " 'pig': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pig.npy',\n",
       " 'rabbit': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rabbit.npy',\n",
       " 'snake': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/snake.npy',\n",
       " 'whale': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/whale.npy'}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "classes_dict = {}\n",
    "for key, value in URL_DATA.items():\n",
    "    response = requests.get(value)\n",
    "    classes_dict[key] = np.load(BytesIO(response.content))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "classes_dict"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bee': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'cat': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'cow': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'dog': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'duck': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'horse': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'pig': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'rabbit': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'snake': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'whale': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "len(classes_dict['dog'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "152159"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "len(classes_dict['cat'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "123202"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Generate labels and add labels to loaded data\n",
    "for i, (key, value) in enumerate(classes_dict.items()):\n",
    "    value = value.astype('float32')/255.\n",
    "    if i == 0:\n",
    "        classes_dict[key] = np.c_[value, np.zeros(len(value))]\n",
    "    else:\n",
    "        classes_dict[key] = np.c_[value,i*np.ones(len(value))]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "classes_dict['bee']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "classes_dict['bee'][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.33725491,\n",
       "       0.40000001, 0.40000001, 0.21960784, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00784314,\n",
       "       0.5411765 , 0.99607843, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.79215688, 0.23529412, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.34509805, 1.        , 0.5529412 ,\n",
       "       0.13333334, 0.09019608, 0.07843138, 0.38039216, 0.84313726,\n",
       "       0.96078432, 0.16078432, 0.        , 0.12156863, 0.23529412,\n",
       "       0.34117648, 0.1882353 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.72156864, 0.8509804 , 0.00392157, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.92941177, 0.66274512,\n",
       "       0.7019608 , 1.        , 1.        , 1.        , 0.97647059,\n",
       "       0.30980393, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10588235, 0.98823529, 0.48235294,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00392157, 0.92156863, 0.99215686, 0.9254902 , 0.4509804 ,\n",
       "       0.24313726, 0.16470589, 0.75294119, 0.97647059, 0.21176471,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.46666667, 0.99215686, 0.11764706, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.36078432, 1.        ,\n",
       "       0.80784315, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.10980392, 1.        , 0.39215687, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.76862746, 0.75686276,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01176471, 0.90980393, 1.        , 0.17254902, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 1.        ,\n",
       "       0.39215687, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.9254902 , 0.56078434, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.41176471, 1.        ,\n",
       "       0.80392158, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00784314, 0.69411767, 0.97647059, 0.18039216, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08235294, 1.        ,\n",
       "       0.40392157, 0.        , 0.12156863, 0.33725491, 0.40000001,\n",
       "       0.56862748, 0.98039216, 1.        , 0.46666667, 0.        ,\n",
       "       0.        , 0.        , 0.00784314, 0.56078434, 1.        ,\n",
       "       0.3764706 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.24313726, 1.        , 0.51764709, 0.72156864,\n",
       "       0.97254902, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.98039216, 0.95294118, 0.70980394, 0.29411766,\n",
       "       0.78431374, 1.        , 0.52941179, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.39607844,\n",
       "       1.        , 0.96078432, 1.        , 0.86274511, 0.94509804,\n",
       "       0.7647059 , 1.        , 0.74117649, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99607843, 0.9254902 , 0.32941177,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32549021, 0.96470588, 1.        ,\n",
       "       0.627451  , 0.89019608, 0.64313728, 0.88627452, 1.        ,\n",
       "       0.44313726, 0.28235295, 0.90588236, 1.        , 0.92941177,\n",
       "       0.93333334, 1.        , 0.67450982, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.61960787, 0.96862745, 1.        , 0.34901962, 1.        ,\n",
       "       0.32941177, 0.74901962, 1.        , 0.29019609, 0.12156863,\n",
       "       0.98431373, 1.        , 0.43921569, 0.00392157, 0.56470591,\n",
       "       1.        , 0.04705882, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.9137255 , 0.8509804 ,\n",
       "       0.90980393, 0.49803922, 0.97254902, 0.09803922, 0.98039216,\n",
       "       1.        , 0.07843138, 0.54509807, 0.97254902, 1.        ,\n",
       "       0.47843137, 0.6901961 , 0.67058825, 0.96078432, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.64313728, 0.49019608, 0.36470589, 0.16862746, 0.00784314,\n",
       "       0.05490196, 1.        , 0.93333334, 0.63529414, 0.70588237,\n",
       "       0.79215688, 0.36078432, 1.        , 0.87450981, 0.03921569,\n",
       "       0.92941177, 0.66274512, 1.        , 0.85882354, 0.96862745,\n",
       "       0.88235295, 0.77254903, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.18039216, 0.80392158, 0.9137255 ,\n",
       "       1.        , 1.        , 0.94509804, 0.65882355, 1.        ,\n",
       "       1.        , 0.36862746, 0.89019608, 0.60392159, 0.69411767,\n",
       "       0.98431373, 0.61960787, 0.41960785, 1.        , 0.37254903,\n",
       "       1.        , 0.83529413, 0.76862746, 0.99607843, 0.44705883,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 0.33725491,\n",
       "       0.55686277, 0.59607846, 1.        , 1.        , 0.27058825,\n",
       "       1.        , 0.4509804 , 0.96862745, 1.        , 0.44705883,\n",
       "       0.84313726, 0.74901962, 0.3137255 , 1.        , 0.26666668,\n",
       "       0.52156866, 0.99607843, 0.12156863, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.18431373,\n",
       "       1.        , 1.        , 0.36862746, 1.        , 0.50980395,\n",
       "       1.        , 1.        , 0.52156866, 1.        , 0.3137255 ,\n",
       "       0.41568628, 1.        , 0.07058824, 0.76862746, 0.78431374,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10980392, 0.98039216, 1.        ,\n",
       "       0.49019608, 0.99607843, 0.65098041, 0.9137255 , 1.        ,\n",
       "       0.76078433, 0.87058824, 0.01176471, 0.51372552, 0.96862745,\n",
       "       0.13333334, 0.99215686, 0.4509804 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.65490198, 1.        , 0.8509804 , 0.87058824,\n",
       "       0.85490197, 0.79215688, 1.        , 0.96470588, 0.53333336,\n",
       "       0.        , 0.6156863 , 0.86666667, 0.74509805, 0.93725491,\n",
       "       0.09411765, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.52941179,\n",
       "       0.97254902, 1.        , 0.94117647, 0.99607843, 0.73333335,\n",
       "       0.96862745, 1.        , 0.32549021, 0.        , 0.72941178,\n",
       "       0.90588236, 1.        , 0.34117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.60000002, 0.88235295, 0.98431373,\n",
       "       0.96862745, 1.        , 0.98823529, 0.98431373, 1.        ,\n",
       "       0.89803922, 0.81960785, 0.98431373, 0.99607843, 0.50980395,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.6156863 , 0.78823531, 0.77254903, 0.60784316, 0.97647059,\n",
       "       0.52549022, 0.66274512, 1.        , 0.66274512, 0.66666669,\n",
       "       0.93725491, 0.61176473, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.05882353, 0.09411765,\n",
       "       0.        , 0.10588235, 0.26666668, 0.        , 0.01568628,\n",
       "       0.24705882, 0.        , 0.        , 0.05882353, 0.01568628,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "label_dict = {0:'bee', 1:'cat', 2:'cow', 3:'dog', 4:'duck',\n",
    "              5:'horse', 6:'pig', 7:'rabbit', 8:'snake', 9:'whale'}\n",
    "\n",
    "lst = []\n",
    "for key, value in classes_dict.items():\n",
    "    lst.append(value[:3000])\n",
    "doodles = np.concatenate(lst)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "len(doodles)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "doodles[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.33725491,\n",
       "       0.40000001, 0.40000001, 0.21960784, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00784314,\n",
       "       0.5411765 , 0.99607843, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.79215688, 0.23529412, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.34509805, 1.        , 0.5529412 ,\n",
       "       0.13333334, 0.09019608, 0.07843138, 0.38039216, 0.84313726,\n",
       "       0.96078432, 0.16078432, 0.        , 0.12156863, 0.23529412,\n",
       "       0.34117648, 0.1882353 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.72156864, 0.8509804 , 0.00392157, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.92941177, 0.66274512,\n",
       "       0.7019608 , 1.        , 1.        , 1.        , 0.97647059,\n",
       "       0.30980393, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10588235, 0.98823529, 0.48235294,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00392157, 0.92156863, 0.99215686, 0.9254902 , 0.4509804 ,\n",
       "       0.24313726, 0.16470589, 0.75294119, 0.97647059, 0.21176471,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.46666667, 0.99215686, 0.11764706, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.36078432, 1.        ,\n",
       "       0.80784315, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.10980392, 1.        , 0.39215687, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.76862746, 0.75686276,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01176471, 0.90980393, 1.        , 0.17254902, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 1.        ,\n",
       "       0.39215687, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.9254902 , 0.56078434, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.41176471, 1.        ,\n",
       "       0.80392158, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00784314, 0.69411767, 0.97647059, 0.18039216, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08235294, 1.        ,\n",
       "       0.40392157, 0.        , 0.12156863, 0.33725491, 0.40000001,\n",
       "       0.56862748, 0.98039216, 1.        , 0.46666667, 0.        ,\n",
       "       0.        , 0.        , 0.00784314, 0.56078434, 1.        ,\n",
       "       0.3764706 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.24313726, 1.        , 0.51764709, 0.72156864,\n",
       "       0.97254902, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.98039216, 0.95294118, 0.70980394, 0.29411766,\n",
       "       0.78431374, 1.        , 0.52941179, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.39607844,\n",
       "       1.        , 0.96078432, 1.        , 0.86274511, 0.94509804,\n",
       "       0.7647059 , 1.        , 0.74117649, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99607843, 0.9254902 , 0.32941177,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32549021, 0.96470588, 1.        ,\n",
       "       0.627451  , 0.89019608, 0.64313728, 0.88627452, 1.        ,\n",
       "       0.44313726, 0.28235295, 0.90588236, 1.        , 0.92941177,\n",
       "       0.93333334, 1.        , 0.67450982, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.61960787, 0.96862745, 1.        , 0.34901962, 1.        ,\n",
       "       0.32941177, 0.74901962, 1.        , 0.29019609, 0.12156863,\n",
       "       0.98431373, 1.        , 0.43921569, 0.00392157, 0.56470591,\n",
       "       1.        , 0.04705882, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.9137255 , 0.8509804 ,\n",
       "       0.90980393, 0.49803922, 0.97254902, 0.09803922, 0.98039216,\n",
       "       1.        , 0.07843138, 0.54509807, 0.97254902, 1.        ,\n",
       "       0.47843137, 0.6901961 , 0.67058825, 0.96078432, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.64313728, 0.49019608, 0.36470589, 0.16862746, 0.00784314,\n",
       "       0.05490196, 1.        , 0.93333334, 0.63529414, 0.70588237,\n",
       "       0.79215688, 0.36078432, 1.        , 0.87450981, 0.03921569,\n",
       "       0.92941177, 0.66274512, 1.        , 0.85882354, 0.96862745,\n",
       "       0.88235295, 0.77254903, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.18039216, 0.80392158, 0.9137255 ,\n",
       "       1.        , 1.        , 0.94509804, 0.65882355, 1.        ,\n",
       "       1.        , 0.36862746, 0.89019608, 0.60392159, 0.69411767,\n",
       "       0.98431373, 0.61960787, 0.41960785, 1.        , 0.37254903,\n",
       "       1.        , 0.83529413, 0.76862746, 0.99607843, 0.44705883,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 0.33725491,\n",
       "       0.55686277, 0.59607846, 1.        , 1.        , 0.27058825,\n",
       "       1.        , 0.4509804 , 0.96862745, 1.        , 0.44705883,\n",
       "       0.84313726, 0.74901962, 0.3137255 , 1.        , 0.26666668,\n",
       "       0.52156866, 0.99607843, 0.12156863, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.18431373,\n",
       "       1.        , 1.        , 0.36862746, 1.        , 0.50980395,\n",
       "       1.        , 1.        , 0.52156866, 1.        , 0.3137255 ,\n",
       "       0.41568628, 1.        , 0.07058824, 0.76862746, 0.78431374,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10980392, 0.98039216, 1.        ,\n",
       "       0.49019608, 0.99607843, 0.65098041, 0.9137255 , 1.        ,\n",
       "       0.76078433, 0.87058824, 0.01176471, 0.51372552, 0.96862745,\n",
       "       0.13333334, 0.99215686, 0.4509804 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.65490198, 1.        , 0.8509804 , 0.87058824,\n",
       "       0.85490197, 0.79215688, 1.        , 0.96470588, 0.53333336,\n",
       "       0.        , 0.6156863 , 0.86666667, 0.74509805, 0.93725491,\n",
       "       0.09411765, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.52941179,\n",
       "       0.97254902, 1.        , 0.94117647, 0.99607843, 0.73333335,\n",
       "       0.96862745, 1.        , 0.32549021, 0.        , 0.72941178,\n",
       "       0.90588236, 1.        , 0.34117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.60000002, 0.88235295, 0.98431373,\n",
       "       0.96862745, 1.        , 0.98823529, 0.98431373, 1.        ,\n",
       "       0.89803922, 0.81960785, 0.98431373, 0.99607843, 0.50980395,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.6156863 , 0.78823531, 0.77254903, 0.60784316, 0.97647059,\n",
       "       0.52549022, 0.66274512, 1.        , 0.66274512, 0.66666669,\n",
       "       0.93725491, 0.61176473, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.05882353, 0.09411765,\n",
       "       0.        , 0.10588235, 0.26666668, 0.        , 0.01568628,\n",
       "       0.24705882, 0.        , 0.        , 0.05882353, 0.01568628,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "y = doodles[:,-1].astype('float32')\n",
    "X = doodles[:,:784]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 9., 9., 9.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "X_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "len(X_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21000"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "X_train[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.10196079, 0.66666669, 0.08627451, 0.        ,\n",
       "       0.        , 0.26274511, 0.67058825, 0.14901961, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.10588235,\n",
       "       0.96078432, 0.63529414, 0.        , 0.32156864, 0.99215686,\n",
       "       0.85882354, 0.14117648, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.47843137, 0.99607843,\n",
       "       0.15686275, 0.80392158, 0.79215688, 0.02352941, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14117648, 1.        , 0.40784314, 0.99607843,\n",
       "       0.43921569, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.01176471,\n",
       "       0.97254902, 0.68627453, 1.        , 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00784314, 0.39215687, 0.6156863 , 0.94509804, 0.95294118,\n",
       "       0.96862745, 0.83529413, 0.81960785, 0.66666669, 0.51372552,\n",
       "       0.25490198, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01960784, 0.16862746,\n",
       "       0.02745098, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.1882353 , 0.79607844, 1.        ,\n",
       "       0.87450981, 0.84313726, 0.78823531, 0.70588237, 0.63921571,\n",
       "       0.66666669, 0.81960785, 0.97254902, 1.        , 0.58431375,\n",
       "       0.01176471, 0.        , 0.        , 0.10196079, 0.47450981,\n",
       "       0.79215688, 0.97254902, 1.        , 0.43529412, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.06666667,\n",
       "       0.9254902 , 0.87843138, 0.30980393, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.05882353, 0.82745099, 1.        , 0.87450981, 0.56862748,\n",
       "       0.69803923, 0.98431373, 0.99215686, 0.99607843, 1.        ,\n",
       "       0.71764708, 0.04313726, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.5411765 , 0.98431373, 0.35294119,\n",
       "       0.47843137, 0.28235295, 0.00392157, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "       0.28235295, 0.7019608 , 0.92156863, 0.86666667, 0.49803922,\n",
       "       0.67058825, 1.        , 0.87843138, 0.27058825, 0.09019608,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.12156863,\n",
       "       0.96862745, 0.58823532, 0.47843137, 1.        , 1.        ,\n",
       "       0.3764706 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.627451  , 0.94117647,\n",
       "       1.        , 1.        , 0.99215686, 0.25490198, 0.        ,\n",
       "       0.        , 0.        , 0.54509807, 0.96862745, 0.09803922,\n",
       "       0.70588237, 1.        , 1.        , 0.7019608 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.12941177, 0.89019608,\n",
       "       0.95686275, 0.18039216, 0.        , 0.        , 0.00784314,\n",
       "       0.89411765, 0.66666669, 0.1882353 , 1.        , 0.47450981,\n",
       "       0.89019608, 0.61176473, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.02352941,\n",
       "       0.32156864, 0.78039217, 0.98039216, 0.28235295, 0.        ,\n",
       "       0.        , 0.        , 0.23921569, 1.        , 0.30980393,\n",
       "       0.17254902, 1.        , 0.65882355, 0.99607843, 0.43137255,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.16862746, 0.55686277, 0.9137255 , 1.        , 0.87058824,\n",
       "       0.31764707, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.40000001, 1.        , 0.08235294, 0.        , 0.6901961 ,\n",
       "       1.        , 0.80000001, 0.03137255, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.05882353, 0.40392157, 0.78823531, 1.        , 0.96470588,\n",
       "       0.63921571, 0.25882354, 0.00392157, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.29411766, 1.        ,\n",
       "       0.40784314, 0.        , 0.05490196, 0.32549021, 0.07843138,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00392157, 0.25490198, 0.63529414, 0.96470588, 1.        ,\n",
       "       0.79215688, 0.40784314, 0.05882353, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.01176471, 0.68235296, 0.98823529, 0.49019608,\n",
       "       0.05882353, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00784314, 0.13333334, 0.49019608, 0.86666667, 1.        ,\n",
       "       0.91764706, 0.56078434, 0.17254902, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.02745098, 0.64313728, 0.99215686, 0.95294118, 0.81568629,\n",
       "       0.75294119, 0.83137256, 0.9137255 , 0.99215686, 1.        ,\n",
       "       0.98823529, 0.70588237, 0.32549021, 0.02352941, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.16470589, 0.5411765 , 0.67450982, 0.72549021, 0.64313728,\n",
       "       0.56470591, 0.48235294, 0.37254903, 0.10196079, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "len(X_train[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "X_train[0][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "X_train[0][1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "len(y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21000"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "def save_data(X_train, y_train, X_test, y_test, force = False):\n",
    "    \n",
    "    # Check for already saved files\n",
    "    if not(path.exists('data/xtrain.pickle')) or force:\n",
    "        # Save X_train dataset as a pickle file\n",
    "        with open('data/xtrain.pickle', 'wb') as f:\n",
    "            pickle.dump(X_train, f)\n",
    "\n",
    "        # Save X_test dataset as a pickle file\n",
    "        with open('data/xtest.pickle', 'wb') as f:\n",
    "            pickle.dump(X_test, f)\n",
    "\n",
    "        # Save y_train dataset as a pickle file\n",
    "        with open('data/ytrain.pickle', 'wb') as f:\n",
    "            pickle.dump(y_train, f)\n",
    "\n",
    "        # Save y_test dataset as a pickle file\n",
    "        with open('data/ytest.pickle', 'wb') as f:\n",
    "            pickle.dump(y_test, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "save_data(X_train, y_train, X_test, y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving data \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "type(X_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "type(X_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "type(y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "len(X_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21000"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "len(X_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def build_model(input_size, output_size, architecture = 'nn', dropout = 0.0):\n",
    "    if (architecture == 'nn'):\n",
    "        # Build a feed-forward network\n",
    "        model = nn.Sequential(OrderedDict([\n",
    "                              ('fc1', nn.Linear(input_size, 128)),\n",
    "                              ('relu1', nn.ReLU()),\n",
    "                              ('fc2', nn.Linear(128, 100)),\n",
    "                              ('bn2', nn.BatchNorm1d(num_features=100)),\n",
    "                              ('relu2', nn.ReLU()),\n",
    "                              ('dropout', nn.Dropout(dropout)),\n",
    "                              ('fc3', nn.Linear(100, 64)),\n",
    "                              ('bn3', nn.BatchNorm1d(num_features=64)),\n",
    "                              ('relu3', nn.ReLU()),\n",
    "                              ('logits', nn.Linear(64, output_size))]))\n",
    "    else:\n",
    "        if (architecture == 'conv'):\n",
    "            # Build a simple convolutional network\n",
    "            model = SimpleCNN(64, 10)\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "rnn_model = build_model(input_size=784, output_size=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "rnn_model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=100, bias=True)\n",
       "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (fc3): Linear(in_features=100, out_features=64, bias=True)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (logits): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "cnn_model = build_model(input_size = 784, output_size=10, architecture='conv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "cnn_model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(1, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3528, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "X_train = pickle.load(open('data/xtrain.pickle', 'rb'))\n",
    "X_test = pickle.load(open('data/xtest.pickle', 'rb'))\n",
    "y_train = pickle.load(open('data/ytrain.pickle', 'rb'))\n",
    "y_test = pickle.load(open('data/ytest.pickle', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "def shuffle(X_train, y_train):\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "    y_train_shuffled = y_train_shuffled.reshape((X_train.shape[0], 1))\n",
    "\n",
    "    X_train_shuffled = torch.from_numpy(X_train_shuffled).float()\n",
    "    y_train_shuffled = torch.from_numpy(y_train_shuffled).long()\n",
    "\n",
    "    return X_train_shuffled, y_train_shuffled"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "X_train_shuffled, y_train_shuffled = shuffle(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "type(X_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "type(y_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "def fit_model(model, X_train, y_train, epochs = 100, n_chunks = 1000, learning_rate = 0.003, weight_decay = 0, optimizer = 'SGD'):\n",
    "\n",
    "    print(\"Fitting model with epochs = {epochs}, learning rate = {lr}\\n\"\\\n",
    "    .format(epochs = epochs, lr = learning_rate))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if (optimizer == 'SGD'):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "    print_every = 100\n",
    "    steps = 0\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        images = torch.chunk(X_train, n_chunks)\n",
    "        labels = torch.chunk(y_train, n_chunks)\n",
    "        for i in range(n_chunks):\n",
    "            steps += 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward passes\n",
    "            output = model.forward(images[i])\n",
    "            loss = criterion(output, labels[i].squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if steps % print_every == 0:\n",
    "                print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                      \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "                running_loss = 0\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "fit_model(rnn_model, X_train_shuffled, y_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting model with epochs = 100, learning rate = 0.003\n",
      "\n",
      "Epoch: 1/100...  Loss: 2.2037\n",
      "Epoch: 1/100...  Loss: 2.0504\n",
      "Epoch: 1/100...  Loss: 1.9417\n",
      "Epoch: 1/100...  Loss: 1.8689\n",
      "Epoch: 1/100...  Loss: 1.8064\n",
      "Epoch: 1/100...  Loss: 1.7327\n",
      "Epoch: 1/100...  Loss: 1.6904\n",
      "Epoch: 1/100...  Loss: 1.7019\n",
      "Epoch: 1/100...  Loss: 1.6900\n",
      "Epoch: 1/100...  Loss: 1.6214\n",
      "Epoch: 2/100...  Loss: 1.5917\n",
      "Epoch: 2/100...  Loss: 1.5950\n",
      "Epoch: 2/100...  Loss: 1.5655\n",
      "Epoch: 2/100...  Loss: 1.5290\n",
      "Epoch: 2/100...  Loss: 1.5380\n",
      "Epoch: 2/100...  Loss: 1.4726\n",
      "Epoch: 2/100...  Loss: 1.4581\n",
      "Epoch: 2/100...  Loss: 1.5071\n",
      "Epoch: 2/100...  Loss: 1.5234\n",
      "Epoch: 2/100...  Loss: 1.4467\n",
      "Epoch: 3/100...  Loss: 1.4421\n",
      "Epoch: 3/100...  Loss: 1.4452\n",
      "Epoch: 3/100...  Loss: 1.4261\n",
      "Epoch: 3/100...  Loss: 1.3853\n",
      "Epoch: 3/100...  Loss: 1.4136\n",
      "Epoch: 3/100...  Loss: 1.3427\n",
      "Epoch: 3/100...  Loss: 1.3311\n",
      "Epoch: 3/100...  Loss: 1.3868\n",
      "Epoch: 3/100...  Loss: 1.4151\n",
      "Epoch: 3/100...  Loss: 1.3340\n",
      "Epoch: 4/100...  Loss: 1.3405\n",
      "Epoch: 4/100...  Loss: 1.3395\n",
      "Epoch: 4/100...  Loss: 1.3229\n",
      "Epoch: 4/100...  Loss: 1.2806\n",
      "Epoch: 4/100...  Loss: 1.3188\n",
      "Epoch: 4/100...  Loss: 1.2470\n",
      "Epoch: 4/100...  Loss: 1.2386\n",
      "Epoch: 4/100...  Loss: 1.2892\n",
      "Epoch: 4/100...  Loss: 1.3260\n",
      "Epoch: 4/100...  Loss: 1.2440\n",
      "Epoch: 5/100...  Loss: 1.2563\n",
      "Epoch: 5/100...  Loss: 1.2529\n",
      "Epoch: 5/100...  Loss: 1.2386\n",
      "Epoch: 5/100...  Loss: 1.1970\n",
      "Epoch: 5/100...  Loss: 1.2418\n",
      "Epoch: 5/100...  Loss: 1.1658\n",
      "Epoch: 5/100...  Loss: 1.1628\n",
      "Epoch: 5/100...  Loss: 1.2064\n",
      "Epoch: 5/100...  Loss: 1.2498\n",
      "Epoch: 5/100...  Loss: 1.1667\n",
      "Epoch: 6/100...  Loss: 1.1825\n",
      "Epoch: 6/100...  Loss: 1.1749\n",
      "Epoch: 6/100...  Loss: 1.1647\n",
      "Epoch: 6/100...  Loss: 1.1225\n",
      "Epoch: 6/100...  Loss: 1.1712\n",
      "Epoch: 6/100...  Loss: 1.0955\n",
      "Epoch: 6/100...  Loss: 1.0921\n",
      "Epoch: 6/100...  Loss: 1.1319\n",
      "Epoch: 6/100...  Loss: 1.1789\n",
      "Epoch: 6/100...  Loss: 1.0950\n",
      "Epoch: 7/100...  Loss: 1.1110\n",
      "Epoch: 7/100...  Loss: 1.1074\n",
      "Epoch: 7/100...  Loss: 1.0973\n",
      "Epoch: 7/100...  Loss: 1.0536\n",
      "Epoch: 7/100...  Loss: 1.1049\n",
      "Epoch: 7/100...  Loss: 1.0311\n",
      "Epoch: 7/100...  Loss: 1.0284\n",
      "Epoch: 7/100...  Loss: 1.0627\n",
      "Epoch: 7/100...  Loss: 1.1086\n",
      "Epoch: 7/100...  Loss: 1.0265\n",
      "Epoch: 8/100...  Loss: 1.0443\n",
      "Epoch: 8/100...  Loss: 1.0426\n",
      "Epoch: 8/100...  Loss: 1.0316\n",
      "Epoch: 8/100...  Loss: 0.9901\n",
      "Epoch: 8/100...  Loss: 1.0374\n",
      "Epoch: 8/100...  Loss: 0.9746\n",
      "Epoch: 8/100...  Loss: 0.9653\n",
      "Epoch: 8/100...  Loss: 1.0006\n",
      "Epoch: 8/100...  Loss: 1.0436\n",
      "Epoch: 8/100...  Loss: 0.9637\n",
      "Epoch: 9/100...  Loss: 0.9794\n",
      "Epoch: 9/100...  Loss: 0.9807\n",
      "Epoch: 9/100...  Loss: 0.9724\n",
      "Epoch: 9/100...  Loss: 0.9240\n",
      "Epoch: 9/100...  Loss: 0.9774\n",
      "Epoch: 9/100...  Loss: 0.9222\n",
      "Epoch: 9/100...  Loss: 0.9061\n",
      "Epoch: 9/100...  Loss: 0.9422\n",
      "Epoch: 9/100...  Loss: 0.9766\n",
      "Epoch: 9/100...  Loss: 0.9070\n",
      "Epoch: 10/100...  Loss: 0.9142\n",
      "Epoch: 10/100...  Loss: 0.9178\n",
      "Epoch: 10/100...  Loss: 0.9135\n",
      "Epoch: 10/100...  Loss: 0.8622\n",
      "Epoch: 10/100...  Loss: 0.9140\n",
      "Epoch: 10/100...  Loss: 0.8659\n",
      "Epoch: 10/100...  Loss: 0.8498\n",
      "Epoch: 10/100...  Loss: 0.8867\n",
      "Epoch: 10/100...  Loss: 0.9131\n",
      "Epoch: 10/100...  Loss: 0.8448\n",
      "Epoch: 11/100...  Loss: 0.8528\n",
      "Epoch: 11/100...  Loss: 0.8584\n",
      "Epoch: 11/100...  Loss: 0.8543\n",
      "Epoch: 11/100...  Loss: 0.8054\n",
      "Epoch: 11/100...  Loss: 0.8539\n",
      "Epoch: 11/100...  Loss: 0.8116\n",
      "Epoch: 11/100...  Loss: 0.7970\n",
      "Epoch: 11/100...  Loss: 0.8312\n",
      "Epoch: 11/100...  Loss: 0.8514\n",
      "Epoch: 11/100...  Loss: 0.7906\n",
      "Epoch: 12/100...  Loss: 0.7922\n",
      "Epoch: 12/100...  Loss: 0.7968\n",
      "Epoch: 12/100...  Loss: 0.8002\n",
      "Epoch: 12/100...  Loss: 0.7492\n",
      "Epoch: 12/100...  Loss: 0.7960\n",
      "Epoch: 12/100...  Loss: 0.7593\n",
      "Epoch: 12/100...  Loss: 0.7409\n",
      "Epoch: 12/100...  Loss: 0.7765\n",
      "Epoch: 12/100...  Loss: 0.7937\n",
      "Epoch: 12/100...  Loss: 0.7399\n",
      "Epoch: 13/100...  Loss: 0.7317\n",
      "Epoch: 13/100...  Loss: 0.7392\n",
      "Epoch: 13/100...  Loss: 0.7480\n",
      "Epoch: 13/100...  Loss: 0.7009\n",
      "Epoch: 13/100...  Loss: 0.7377\n",
      "Epoch: 13/100...  Loss: 0.7062\n",
      "Epoch: 13/100...  Loss: 0.6879\n",
      "Epoch: 13/100...  Loss: 0.7198\n",
      "Epoch: 13/100...  Loss: 0.7361\n",
      "Epoch: 13/100...  Loss: 0.6875\n",
      "Epoch: 14/100...  Loss: 0.6748\n",
      "Epoch: 14/100...  Loss: 0.6822\n",
      "Epoch: 14/100...  Loss: 0.6958\n",
      "Epoch: 14/100...  Loss: 0.6558\n",
      "Epoch: 14/100...  Loss: 0.6833\n",
      "Epoch: 14/100...  Loss: 0.6545\n",
      "Epoch: 14/100...  Loss: 0.6326\n",
      "Epoch: 14/100...  Loss: 0.6701\n",
      "Epoch: 14/100...  Loss: 0.6841\n",
      "Epoch: 14/100...  Loss: 0.6346\n",
      "Epoch: 15/100...  Loss: 0.6256\n",
      "Epoch: 15/100...  Loss: 0.6295\n",
      "Epoch: 15/100...  Loss: 0.6423\n",
      "Epoch: 15/100...  Loss: 0.6114\n",
      "Epoch: 15/100...  Loss: 0.6311\n",
      "Epoch: 15/100...  Loss: 0.6044\n",
      "Epoch: 15/100...  Loss: 0.5828\n",
      "Epoch: 15/100...  Loss: 0.6183\n",
      "Epoch: 15/100...  Loss: 0.6390\n",
      "Epoch: 15/100...  Loss: 0.5861\n",
      "Epoch: 16/100...  Loss: 0.5765\n",
      "Epoch: 16/100...  Loss: 0.5761\n",
      "Epoch: 16/100...  Loss: 0.5949\n",
      "Epoch: 16/100...  Loss: 0.5682\n",
      "Epoch: 16/100...  Loss: 0.5802\n",
      "Epoch: 16/100...  Loss: 0.5564\n",
      "Epoch: 16/100...  Loss: 0.5389\n",
      "Epoch: 16/100...  Loss: 0.5703\n",
      "Epoch: 16/100...  Loss: 0.5888\n",
      "Epoch: 16/100...  Loss: 0.5404\n",
      "Epoch: 17/100...  Loss: 0.5290\n",
      "Epoch: 17/100...  Loss: 0.5287\n",
      "Epoch: 17/100...  Loss: 0.5443\n",
      "Epoch: 17/100...  Loss: 0.5252\n",
      "Epoch: 17/100...  Loss: 0.5346\n",
      "Epoch: 17/100...  Loss: 0.5115\n",
      "Epoch: 17/100...  Loss: 0.4978\n",
      "Epoch: 17/100...  Loss: 0.5210\n",
      "Epoch: 17/100...  Loss: 0.5378\n",
      "Epoch: 17/100...  Loss: 0.4965\n",
      "Epoch: 18/100...  Loss: 0.4833\n",
      "Epoch: 18/100...  Loss: 0.4825\n",
      "Epoch: 18/100...  Loss: 0.4992\n",
      "Epoch: 18/100...  Loss: 0.4894\n",
      "Epoch: 18/100...  Loss: 0.4913\n",
      "Epoch: 18/100...  Loss: 0.4667\n",
      "Epoch: 18/100...  Loss: 0.4594\n",
      "Epoch: 18/100...  Loss: 0.4758\n",
      "Epoch: 18/100...  Loss: 0.4912\n",
      "Epoch: 18/100...  Loss: 0.4516\n",
      "Epoch: 19/100...  Loss: 0.4391\n",
      "Epoch: 19/100...  Loss: 0.4386\n",
      "Epoch: 19/100...  Loss: 0.4539\n",
      "Epoch: 19/100...  Loss: 0.4542\n",
      "Epoch: 19/100...  Loss: 0.4455\n",
      "Epoch: 19/100...  Loss: 0.4195\n",
      "Epoch: 19/100...  Loss: 0.4241\n",
      "Epoch: 19/100...  Loss: 0.4335\n",
      "Epoch: 19/100...  Loss: 0.4463\n",
      "Epoch: 19/100...  Loss: 0.4130\n",
      "Epoch: 20/100...  Loss: 0.3980\n",
      "Epoch: 20/100...  Loss: 0.3969\n",
      "Epoch: 20/100...  Loss: 0.4090\n",
      "Epoch: 20/100...  Loss: 0.4185\n",
      "Epoch: 20/100...  Loss: 0.4032\n",
      "Epoch: 20/100...  Loss: 0.3785\n",
      "Epoch: 20/100...  Loss: 0.3883\n",
      "Epoch: 20/100...  Loss: 0.3902\n",
      "Epoch: 20/100...  Loss: 0.4061\n",
      "Epoch: 20/100...  Loss: 0.3706\n",
      "Epoch: 21/100...  Loss: 0.3637\n",
      "Epoch: 21/100...  Loss: 0.3531\n",
      "Epoch: 21/100...  Loss: 0.3708\n",
      "Epoch: 21/100...  Loss: 0.3775\n",
      "Epoch: 21/100...  Loss: 0.3634\n",
      "Epoch: 21/100...  Loss: 0.3460\n",
      "Epoch: 21/100...  Loss: 0.3513\n",
      "Epoch: 21/100...  Loss: 0.3542\n",
      "Epoch: 21/100...  Loss: 0.3716\n",
      "Epoch: 21/100...  Loss: 0.3340\n",
      "Epoch: 22/100...  Loss: 0.3293\n",
      "Epoch: 22/100...  Loss: 0.3168\n",
      "Epoch: 22/100...  Loss: 0.3395\n",
      "Epoch: 22/100...  Loss: 0.3399\n",
      "Epoch: 22/100...  Loss: 0.3284\n",
      "Epoch: 22/100...  Loss: 0.3126\n",
      "Epoch: 22/100...  Loss: 0.3141\n",
      "Epoch: 22/100...  Loss: 0.3255\n",
      "Epoch: 22/100...  Loss: 0.3328\n",
      "Epoch: 22/100...  Loss: 0.3037\n",
      "Epoch: 23/100...  Loss: 0.2913\n",
      "Epoch: 23/100...  Loss: 0.2838\n",
      "Epoch: 23/100...  Loss: 0.3039\n",
      "Epoch: 23/100...  Loss: 0.3045\n",
      "Epoch: 23/100...  Loss: 0.2939\n",
      "Epoch: 23/100...  Loss: 0.2731\n",
      "Epoch: 23/100...  Loss: 0.2883\n",
      "Epoch: 23/100...  Loss: 0.2938\n",
      "Epoch: 23/100...  Loss: 0.3054\n",
      "Epoch: 23/100...  Loss: 0.2721\n",
      "Epoch: 24/100...  Loss: 0.2606\n",
      "Epoch: 24/100...  Loss: 0.2529\n",
      "Epoch: 24/100...  Loss: 0.2722\n",
      "Epoch: 24/100...  Loss: 0.2747\n",
      "Epoch: 24/100...  Loss: 0.2615\n",
      "Epoch: 24/100...  Loss: 0.2406\n",
      "Epoch: 24/100...  Loss: 0.2568\n",
      "Epoch: 24/100...  Loss: 0.2586\n",
      "Epoch: 24/100...  Loss: 0.2678\n",
      "Epoch: 24/100...  Loss: 0.2431\n",
      "Epoch: 25/100...  Loss: 0.2332\n",
      "Epoch: 25/100...  Loss: 0.2246\n",
      "Epoch: 25/100...  Loss: 0.2407\n",
      "Epoch: 25/100...  Loss: 0.2453\n",
      "Epoch: 25/100...  Loss: 0.2304\n",
      "Epoch: 25/100...  Loss: 0.2168\n",
      "Epoch: 25/100...  Loss: 0.2273\n",
      "Epoch: 25/100...  Loss: 0.2291\n",
      "Epoch: 25/100...  Loss: 0.2383\n",
      "Epoch: 25/100...  Loss: 0.2167\n",
      "Epoch: 26/100...  Loss: 0.2036\n",
      "Epoch: 26/100...  Loss: 0.1945\n",
      "Epoch: 26/100...  Loss: 0.2112\n",
      "Epoch: 26/100...  Loss: 0.2140\n",
      "Epoch: 26/100...  Loss: 0.2079\n",
      "Epoch: 26/100...  Loss: 0.1892\n",
      "Epoch: 26/100...  Loss: 0.2014\n",
      "Epoch: 26/100...  Loss: 0.2006\n",
      "Epoch: 26/100...  Loss: 0.2116\n",
      "Epoch: 26/100...  Loss: 0.1839\n",
      "Epoch: 27/100...  Loss: 0.1761\n",
      "Epoch: 27/100...  Loss: 0.1714\n",
      "Epoch: 27/100...  Loss: 0.1884\n",
      "Epoch: 27/100...  Loss: 0.1923\n",
      "Epoch: 27/100...  Loss: 0.1829\n",
      "Epoch: 27/100...  Loss: 0.1716\n",
      "Epoch: 27/100...  Loss: 0.1760\n",
      "Epoch: 27/100...  Loss: 0.1769\n",
      "Epoch: 27/100...  Loss: 0.1907\n",
      "Epoch: 27/100...  Loss: 0.1655\n",
      "Epoch: 28/100...  Loss: 0.1506\n",
      "Epoch: 28/100...  Loss: 0.1475\n",
      "Epoch: 28/100...  Loss: 0.1601\n",
      "Epoch: 28/100...  Loss: 0.1657\n",
      "Epoch: 28/100...  Loss: 0.1655\n",
      "Epoch: 28/100...  Loss: 0.1496\n",
      "Epoch: 28/100...  Loss: 0.1565\n",
      "Epoch: 28/100...  Loss: 0.1533\n",
      "Epoch: 28/100...  Loss: 0.1659\n",
      "Epoch: 28/100...  Loss: 0.1473\n",
      "Epoch: 29/100...  Loss: 0.1296\n",
      "Epoch: 29/100...  Loss: 0.1296\n",
      "Epoch: 29/100...  Loss: 0.1433\n",
      "Epoch: 29/100...  Loss: 0.1465\n",
      "Epoch: 29/100...  Loss: 0.1398\n",
      "Epoch: 29/100...  Loss: 0.1333\n",
      "Epoch: 29/100...  Loss: 0.1383\n",
      "Epoch: 29/100...  Loss: 0.1354\n",
      "Epoch: 29/100...  Loss: 0.1453\n",
      "Epoch: 29/100...  Loss: 0.1250\n",
      "Epoch: 30/100...  Loss: 0.1138\n",
      "Epoch: 30/100...  Loss: 0.1122\n",
      "Epoch: 30/100...  Loss: 0.1226\n",
      "Epoch: 30/100...  Loss: 0.1217\n",
      "Epoch: 30/100...  Loss: 0.1222\n",
      "Epoch: 30/100...  Loss: 0.1125\n",
      "Epoch: 30/100...  Loss: 0.1187\n",
      "Epoch: 30/100...  Loss: 0.1149\n",
      "Epoch: 30/100...  Loss: 0.1248\n",
      "Epoch: 30/100...  Loss: 0.1065\n",
      "Epoch: 31/100...  Loss: 0.0980\n",
      "Epoch: 31/100...  Loss: 0.0973\n",
      "Epoch: 31/100...  Loss: 0.1032\n",
      "Epoch: 31/100...  Loss: 0.1045\n",
      "Epoch: 31/100...  Loss: 0.1007\n",
      "Epoch: 31/100...  Loss: 0.0976\n",
      "Epoch: 31/100...  Loss: 0.0976\n",
      "Epoch: 31/100...  Loss: 0.0954\n",
      "Epoch: 31/100...  Loss: 0.1081\n",
      "Epoch: 31/100...  Loss: 0.0913\n",
      "Epoch: 32/100...  Loss: 0.0830\n",
      "Epoch: 32/100...  Loss: 0.0823\n",
      "Epoch: 32/100...  Loss: 0.0890\n",
      "Epoch: 32/100...  Loss: 0.0885\n",
      "Epoch: 32/100...  Loss: 0.0878\n",
      "Epoch: 32/100...  Loss: 0.0802\n",
      "Epoch: 32/100...  Loss: 0.0847\n",
      "Epoch: 32/100...  Loss: 0.0814\n",
      "Epoch: 32/100...  Loss: 0.0910\n",
      "Epoch: 32/100...  Loss: 0.0766\n",
      "Epoch: 33/100...  Loss: 0.0705\n",
      "Epoch: 33/100...  Loss: 0.0687\n",
      "Epoch: 33/100...  Loss: 0.0739\n",
      "Epoch: 33/100...  Loss: 0.0730\n",
      "Epoch: 33/100...  Loss: 0.0719\n",
      "Epoch: 33/100...  Loss: 0.0684\n",
      "Epoch: 33/100...  Loss: 0.0712\n",
      "Epoch: 33/100...  Loss: 0.0689\n",
      "Epoch: 33/100...  Loss: 0.0751\n",
      "Epoch: 33/100...  Loss: 0.0648\n",
      "Epoch: 34/100...  Loss: 0.0582\n",
      "Epoch: 34/100...  Loss: 0.0585\n",
      "Epoch: 34/100...  Loss: 0.0636\n",
      "Epoch: 34/100...  Loss: 0.0626\n",
      "Epoch: 34/100...  Loss: 0.0611\n",
      "Epoch: 34/100...  Loss: 0.0577\n",
      "Epoch: 34/100...  Loss: 0.0602\n",
      "Epoch: 34/100...  Loss: 0.0562\n",
      "Epoch: 34/100...  Loss: 0.0636\n",
      "Epoch: 34/100...  Loss: 0.0544\n",
      "Epoch: 35/100...  Loss: 0.0491\n",
      "Epoch: 35/100...  Loss: 0.0487\n",
      "Epoch: 35/100...  Loss: 0.0526\n",
      "Epoch: 35/100...  Loss: 0.0526\n",
      "Epoch: 35/100...  Loss: 0.0510\n",
      "Epoch: 35/100...  Loss: 0.0492\n",
      "Epoch: 35/100...  Loss: 0.0517\n",
      "Epoch: 35/100...  Loss: 0.0485\n",
      "Epoch: 35/100...  Loss: 0.0544\n",
      "Epoch: 35/100...  Loss: 0.0461\n",
      "Epoch: 36/100...  Loss: 0.0425\n",
      "Epoch: 36/100...  Loss: 0.0412\n",
      "Epoch: 36/100...  Loss: 0.0459\n",
      "Epoch: 36/100...  Loss: 0.0450\n",
      "Epoch: 36/100...  Loss: 0.0434\n",
      "Epoch: 36/100...  Loss: 0.0420\n",
      "Epoch: 36/100...  Loss: 0.0444\n",
      "Epoch: 36/100...  Loss: 0.0421\n",
      "Epoch: 36/100...  Loss: 0.0470\n",
      "Epoch: 36/100...  Loss: 0.0398\n",
      "Epoch: 37/100...  Loss: 0.0371\n",
      "Epoch: 37/100...  Loss: 0.0353\n",
      "Epoch: 37/100...  Loss: 0.0399\n",
      "Epoch: 37/100...  Loss: 0.0380\n",
      "Epoch: 37/100...  Loss: 0.0378\n",
      "Epoch: 37/100...  Loss: 0.0358\n",
      "Epoch: 37/100...  Loss: 0.0386\n",
      "Epoch: 37/100...  Loss: 0.0368\n",
      "Epoch: 37/100...  Loss: 0.0415\n",
      "Epoch: 37/100...  Loss: 0.0351\n",
      "Epoch: 38/100...  Loss: 0.0322\n",
      "Epoch: 38/100...  Loss: 0.0313\n",
      "Epoch: 38/100...  Loss: 0.0351\n",
      "Epoch: 38/100...  Loss: 0.0334\n",
      "Epoch: 38/100...  Loss: 0.0336\n",
      "Epoch: 38/100...  Loss: 0.0321\n",
      "Epoch: 38/100...  Loss: 0.0345\n",
      "Epoch: 38/100...  Loss: 0.0327\n",
      "Epoch: 38/100...  Loss: 0.0365\n",
      "Epoch: 38/100...  Loss: 0.0310\n",
      "Epoch: 39/100...  Loss: 0.0285\n",
      "Epoch: 39/100...  Loss: 0.0271\n",
      "Epoch: 39/100...  Loss: 0.0311\n",
      "Epoch: 39/100...  Loss: 0.0291\n",
      "Epoch: 39/100...  Loss: 0.0295\n",
      "Epoch: 39/100...  Loss: 0.0286\n",
      "Epoch: 39/100...  Loss: 0.0303\n",
      "Epoch: 39/100...  Loss: 0.0290\n",
      "Epoch: 39/100...  Loss: 0.0323\n",
      "Epoch: 39/100...  Loss: 0.0278\n",
      "Epoch: 40/100...  Loss: 0.0258\n",
      "Epoch: 40/100...  Loss: 0.0243\n",
      "Epoch: 40/100...  Loss: 0.0281\n",
      "Epoch: 40/100...  Loss: 0.0262\n",
      "Epoch: 40/100...  Loss: 0.0263\n",
      "Epoch: 40/100...  Loss: 0.0260\n",
      "Epoch: 40/100...  Loss: 0.0272\n",
      "Epoch: 40/100...  Loss: 0.0261\n",
      "Epoch: 40/100...  Loss: 0.0291\n",
      "Epoch: 40/100...  Loss: 0.0251\n",
      "Epoch: 41/100...  Loss: 0.0231\n",
      "Epoch: 41/100...  Loss: 0.0219\n",
      "Epoch: 41/100...  Loss: 0.0252\n",
      "Epoch: 41/100...  Loss: 0.0238\n",
      "Epoch: 41/100...  Loss: 0.0236\n",
      "Epoch: 41/100...  Loss: 0.0238\n",
      "Epoch: 41/100...  Loss: 0.0245\n",
      "Epoch: 41/100...  Loss: 0.0237\n",
      "Epoch: 41/100...  Loss: 0.0264\n",
      "Epoch: 41/100...  Loss: 0.0228\n",
      "Epoch: 42/100...  Loss: 0.0211\n",
      "Epoch: 42/100...  Loss: 0.0199\n",
      "Epoch: 42/100...  Loss: 0.0229\n",
      "Epoch: 42/100...  Loss: 0.0216\n",
      "Epoch: 42/100...  Loss: 0.0215\n",
      "Epoch: 42/100...  Loss: 0.0217\n",
      "Epoch: 42/100...  Loss: 0.0222\n",
      "Epoch: 42/100...  Loss: 0.0213\n",
      "Epoch: 42/100...  Loss: 0.0240\n",
      "Epoch: 42/100...  Loss: 0.0210\n",
      "Epoch: 43/100...  Loss: 0.0194\n",
      "Epoch: 43/100...  Loss: 0.0182\n",
      "Epoch: 43/100...  Loss: 0.0210\n",
      "Epoch: 43/100...  Loss: 0.0198\n",
      "Epoch: 43/100...  Loss: 0.0197\n",
      "Epoch: 43/100...  Loss: 0.0200\n",
      "Epoch: 43/100...  Loss: 0.0204\n",
      "Epoch: 43/100...  Loss: 0.0197\n",
      "Epoch: 43/100...  Loss: 0.0220\n",
      "Epoch: 43/100...  Loss: 0.0193\n",
      "Epoch: 44/100...  Loss: 0.0180\n",
      "Epoch: 44/100...  Loss: 0.0168\n",
      "Epoch: 44/100...  Loss: 0.0194\n",
      "Epoch: 44/100...  Loss: 0.0183\n",
      "Epoch: 44/100...  Loss: 0.0181\n",
      "Epoch: 44/100...  Loss: 0.0184\n",
      "Epoch: 44/100...  Loss: 0.0189\n",
      "Epoch: 44/100...  Loss: 0.0182\n",
      "Epoch: 44/100...  Loss: 0.0203\n",
      "Epoch: 44/100...  Loss: 0.0179\n",
      "Epoch: 45/100...  Loss: 0.0166\n",
      "Epoch: 45/100...  Loss: 0.0156\n",
      "Epoch: 45/100...  Loss: 0.0179\n",
      "Epoch: 45/100...  Loss: 0.0170\n",
      "Epoch: 45/100...  Loss: 0.0168\n",
      "Epoch: 45/100...  Loss: 0.0171\n",
      "Epoch: 45/100...  Loss: 0.0172\n",
      "Epoch: 45/100...  Loss: 0.0169\n",
      "Epoch: 45/100...  Loss: 0.0187\n",
      "Epoch: 45/100...  Loss: 0.0168\n",
      "Epoch: 46/100...  Loss: 0.0154\n",
      "Epoch: 46/100...  Loss: 0.0145\n",
      "Epoch: 46/100...  Loss: 0.0166\n",
      "Epoch: 46/100...  Loss: 0.0159\n",
      "Epoch: 46/100...  Loss: 0.0157\n",
      "Epoch: 46/100...  Loss: 0.0160\n",
      "Epoch: 46/100...  Loss: 0.0159\n",
      "Epoch: 46/100...  Loss: 0.0158\n",
      "Epoch: 46/100...  Loss: 0.0173\n",
      "Epoch: 46/100...  Loss: 0.0155\n",
      "Epoch: 47/100...  Loss: 0.0144\n",
      "Epoch: 47/100...  Loss: 0.0136\n",
      "Epoch: 47/100...  Loss: 0.0154\n",
      "Epoch: 47/100...  Loss: 0.0147\n",
      "Epoch: 47/100...  Loss: 0.0146\n",
      "Epoch: 47/100...  Loss: 0.0149\n",
      "Epoch: 47/100...  Loss: 0.0148\n",
      "Epoch: 47/100...  Loss: 0.0148\n",
      "Epoch: 47/100...  Loss: 0.0161\n",
      "Epoch: 47/100...  Loss: 0.0144\n",
      "Epoch: 48/100...  Loss: 0.0135\n",
      "Epoch: 48/100...  Loss: 0.0127\n",
      "Epoch: 48/100...  Loss: 0.0144\n",
      "Epoch: 48/100...  Loss: 0.0138\n",
      "Epoch: 48/100...  Loss: 0.0137\n",
      "Epoch: 48/100...  Loss: 0.0140\n",
      "Epoch: 48/100...  Loss: 0.0138\n",
      "Epoch: 48/100...  Loss: 0.0138\n",
      "Epoch: 48/100...  Loss: 0.0149\n",
      "Epoch: 48/100...  Loss: 0.0134\n",
      "Epoch: 49/100...  Loss: 0.0127\n",
      "Epoch: 49/100...  Loss: 0.0119\n",
      "Epoch: 49/100...  Loss: 0.0135\n",
      "Epoch: 49/100...  Loss: 0.0130\n",
      "Epoch: 49/100...  Loss: 0.0129\n",
      "Epoch: 49/100...  Loss: 0.0132\n",
      "Epoch: 49/100...  Loss: 0.0129\n",
      "Epoch: 49/100...  Loss: 0.0129\n",
      "Epoch: 49/100...  Loss: 0.0139\n",
      "Epoch: 49/100...  Loss: 0.0126\n",
      "Epoch: 50/100...  Loss: 0.0119\n",
      "Epoch: 50/100...  Loss: 0.0113\n",
      "Epoch: 50/100...  Loss: 0.0128\n",
      "Epoch: 50/100...  Loss: 0.0123\n",
      "Epoch: 50/100...  Loss: 0.0121\n",
      "Epoch: 50/100...  Loss: 0.0125\n",
      "Epoch: 50/100...  Loss: 0.0121\n",
      "Epoch: 50/100...  Loss: 0.0122\n",
      "Epoch: 50/100...  Loss: 0.0130\n",
      "Epoch: 50/100...  Loss: 0.0118\n",
      "Epoch: 51/100...  Loss: 0.0113\n",
      "Epoch: 51/100...  Loss: 0.0107\n",
      "Epoch: 51/100...  Loss: 0.0122\n",
      "Epoch: 51/100...  Loss: 0.0117\n",
      "Epoch: 51/100...  Loss: 0.0115\n",
      "Epoch: 51/100...  Loss: 0.0118\n",
      "Epoch: 51/100...  Loss: 0.0114\n",
      "Epoch: 51/100...  Loss: 0.0114\n",
      "Epoch: 51/100...  Loss: 0.0122\n",
      "Epoch: 51/100...  Loss: 0.0112\n",
      "Epoch: 52/100...  Loss: 0.0107\n",
      "Epoch: 52/100...  Loss: 0.0102\n",
      "Epoch: 52/100...  Loss: 0.0115\n",
      "Epoch: 52/100...  Loss: 0.0110\n",
      "Epoch: 52/100...  Loss: 0.0108\n",
      "Epoch: 52/100...  Loss: 0.0112\n",
      "Epoch: 52/100...  Loss: 0.0108\n",
      "Epoch: 52/100...  Loss: 0.0108\n",
      "Epoch: 52/100...  Loss: 0.0115\n",
      "Epoch: 52/100...  Loss: 0.0106\n",
      "Epoch: 53/100...  Loss: 0.0101\n",
      "Epoch: 53/100...  Loss: 0.0097\n",
      "Epoch: 53/100...  Loss: 0.0109\n",
      "Epoch: 53/100...  Loss: 0.0105\n",
      "Epoch: 53/100...  Loss: 0.0103\n",
      "Epoch: 53/100...  Loss: 0.0107\n",
      "Epoch: 53/100...  Loss: 0.0103\n",
      "Epoch: 53/100...  Loss: 0.0102\n",
      "Epoch: 53/100...  Loss: 0.0110\n",
      "Epoch: 53/100...  Loss: 0.0100\n",
      "Epoch: 54/100...  Loss: 0.0096\n",
      "Epoch: 54/100...  Loss: 0.0092\n",
      "Epoch: 54/100...  Loss: 0.0103\n",
      "Epoch: 54/100...  Loss: 0.0100\n",
      "Epoch: 54/100...  Loss: 0.0098\n",
      "Epoch: 54/100...  Loss: 0.0101\n",
      "Epoch: 54/100...  Loss: 0.0098\n",
      "Epoch: 54/100...  Loss: 0.0097\n",
      "Epoch: 54/100...  Loss: 0.0104\n",
      "Epoch: 54/100...  Loss: 0.0095\n",
      "Epoch: 55/100...  Loss: 0.0091\n",
      "Epoch: 55/100...  Loss: 0.0088\n",
      "Epoch: 55/100...  Loss: 0.0098\n",
      "Epoch: 55/100...  Loss: 0.0095\n",
      "Epoch: 55/100...  Loss: 0.0093\n",
      "Epoch: 55/100...  Loss: 0.0096\n",
      "Epoch: 55/100...  Loss: 0.0093\n",
      "Epoch: 55/100...  Loss: 0.0092\n",
      "Epoch: 55/100...  Loss: 0.0099\n",
      "Epoch: 55/100...  Loss: 0.0091\n",
      "Epoch: 56/100...  Loss: 0.0087\n",
      "Epoch: 56/100...  Loss: 0.0084\n",
      "Epoch: 56/100...  Loss: 0.0094\n",
      "Epoch: 56/100...  Loss: 0.0090\n",
      "Epoch: 56/100...  Loss: 0.0089\n",
      "Epoch: 56/100...  Loss: 0.0092\n",
      "Epoch: 56/100...  Loss: 0.0088\n",
      "Epoch: 56/100...  Loss: 0.0088\n",
      "Epoch: 56/100...  Loss: 0.0094\n",
      "Epoch: 56/100...  Loss: 0.0087\n",
      "Epoch: 57/100...  Loss: 0.0083\n",
      "Epoch: 57/100...  Loss: 0.0080\n",
      "Epoch: 57/100...  Loss: 0.0090\n",
      "Epoch: 57/100...  Loss: 0.0087\n",
      "Epoch: 57/100...  Loss: 0.0085\n",
      "Epoch: 57/100...  Loss: 0.0088\n",
      "Epoch: 57/100...  Loss: 0.0084\n",
      "Epoch: 57/100...  Loss: 0.0084\n",
      "Epoch: 57/100...  Loss: 0.0090\n",
      "Epoch: 57/100...  Loss: 0.0083\n",
      "Epoch: 58/100...  Loss: 0.0080\n",
      "Epoch: 58/100...  Loss: 0.0076\n",
      "Epoch: 58/100...  Loss: 0.0086\n",
      "Epoch: 58/100...  Loss: 0.0083\n",
      "Epoch: 58/100...  Loss: 0.0081\n",
      "Epoch: 58/100...  Loss: 0.0084\n",
      "Epoch: 58/100...  Loss: 0.0081\n",
      "Epoch: 58/100...  Loss: 0.0081\n",
      "Epoch: 58/100...  Loss: 0.0086\n",
      "Epoch: 58/100...  Loss: 0.0079\n",
      "Epoch: 59/100...  Loss: 0.0076\n",
      "Epoch: 59/100...  Loss: 0.0073\n",
      "Epoch: 59/100...  Loss: 0.0083\n",
      "Epoch: 59/100...  Loss: 0.0080\n",
      "Epoch: 59/100...  Loss: 0.0078\n",
      "Epoch: 59/100...  Loss: 0.0081\n",
      "Epoch: 59/100...  Loss: 0.0077\n",
      "Epoch: 59/100...  Loss: 0.0077\n",
      "Epoch: 59/100...  Loss: 0.0082\n",
      "Epoch: 59/100...  Loss: 0.0076\n",
      "Epoch: 60/100...  Loss: 0.0073\n",
      "Epoch: 60/100...  Loss: 0.0070\n",
      "Epoch: 60/100...  Loss: 0.0079\n",
      "Epoch: 60/100...  Loss: 0.0076\n",
      "Epoch: 60/100...  Loss: 0.0074\n",
      "Epoch: 60/100...  Loss: 0.0077\n",
      "Epoch: 60/100...  Loss: 0.0074\n",
      "Epoch: 60/100...  Loss: 0.0074\n",
      "Epoch: 60/100...  Loss: 0.0079\n",
      "Epoch: 60/100...  Loss: 0.0072\n",
      "Epoch: 61/100...  Loss: 0.0070\n",
      "Epoch: 61/100...  Loss: 0.0068\n",
      "Epoch: 61/100...  Loss: 0.0076\n",
      "Epoch: 61/100...  Loss: 0.0073\n",
      "Epoch: 61/100...  Loss: 0.0072\n",
      "Epoch: 61/100...  Loss: 0.0074\n",
      "Epoch: 61/100...  Loss: 0.0072\n",
      "Epoch: 61/100...  Loss: 0.0071\n",
      "Epoch: 61/100...  Loss: 0.0076\n",
      "Epoch: 61/100...  Loss: 0.0070\n",
      "Epoch: 62/100...  Loss: 0.0067\n",
      "Epoch: 62/100...  Loss: 0.0065\n",
      "Epoch: 62/100...  Loss: 0.0074\n",
      "Epoch: 62/100...  Loss: 0.0070\n",
      "Epoch: 62/100...  Loss: 0.0069\n",
      "Epoch: 62/100...  Loss: 0.0071\n",
      "Epoch: 62/100...  Loss: 0.0069\n",
      "Epoch: 62/100...  Loss: 0.0068\n",
      "Epoch: 62/100...  Loss: 0.0073\n",
      "Epoch: 62/100...  Loss: 0.0067\n",
      "Epoch: 63/100...  Loss: 0.0065\n",
      "Epoch: 63/100...  Loss: 0.0063\n",
      "Epoch: 63/100...  Loss: 0.0071\n",
      "Epoch: 63/100...  Loss: 0.0067\n",
      "Epoch: 63/100...  Loss: 0.0066\n",
      "Epoch: 63/100...  Loss: 0.0069\n",
      "Epoch: 63/100...  Loss: 0.0066\n",
      "Epoch: 63/100...  Loss: 0.0066\n",
      "Epoch: 63/100...  Loss: 0.0070\n",
      "Epoch: 63/100...  Loss: 0.0065\n",
      "Epoch: 64/100...  Loss: 0.0063\n",
      "Epoch: 64/100...  Loss: 0.0061\n",
      "Epoch: 64/100...  Loss: 0.0069\n",
      "Epoch: 64/100...  Loss: 0.0065\n",
      "Epoch: 64/100...  Loss: 0.0064\n",
      "Epoch: 64/100...  Loss: 0.0066\n",
      "Epoch: 64/100...  Loss: 0.0064\n",
      "Epoch: 64/100...  Loss: 0.0064\n",
      "Epoch: 64/100...  Loss: 0.0067\n",
      "Epoch: 64/100...  Loss: 0.0062\n",
      "Epoch: 65/100...  Loss: 0.0060\n",
      "Epoch: 65/100...  Loss: 0.0058\n",
      "Epoch: 65/100...  Loss: 0.0066\n",
      "Epoch: 65/100...  Loss: 0.0063\n",
      "Epoch: 65/100...  Loss: 0.0061\n",
      "Epoch: 65/100...  Loss: 0.0064\n",
      "Epoch: 65/100...  Loss: 0.0062\n",
      "Epoch: 65/100...  Loss: 0.0061\n",
      "Epoch: 65/100...  Loss: 0.0065\n",
      "Epoch: 65/100...  Loss: 0.0060\n",
      "Epoch: 66/100...  Loss: 0.0058\n",
      "Epoch: 66/100...  Loss: 0.0057\n",
      "Epoch: 66/100...  Loss: 0.0064\n",
      "Epoch: 66/100...  Loss: 0.0061\n",
      "Epoch: 66/100...  Loss: 0.0059\n",
      "Epoch: 66/100...  Loss: 0.0061\n",
      "Epoch: 66/100...  Loss: 0.0060\n",
      "Epoch: 66/100...  Loss: 0.0059\n",
      "Epoch: 66/100...  Loss: 0.0063\n",
      "Epoch: 66/100...  Loss: 0.0058\n",
      "Epoch: 67/100...  Loss: 0.0056\n",
      "Epoch: 67/100...  Loss: 0.0055\n",
      "Epoch: 67/100...  Loss: 0.0062\n",
      "Epoch: 67/100...  Loss: 0.0059\n",
      "Epoch: 67/100...  Loss: 0.0057\n",
      "Epoch: 67/100...  Loss: 0.0059\n",
      "Epoch: 67/100...  Loss: 0.0058\n",
      "Epoch: 67/100...  Loss: 0.0058\n",
      "Epoch: 67/100...  Loss: 0.0061\n",
      "Epoch: 67/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0055\n",
      "Epoch: 68/100...  Loss: 0.0053\n",
      "Epoch: 68/100...  Loss: 0.0060\n",
      "Epoch: 68/100...  Loss: 0.0057\n",
      "Epoch: 68/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0057\n",
      "Epoch: 68/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0059\n",
      "Epoch: 68/100...  Loss: 0.0054\n",
      "Epoch: 69/100...  Loss: 0.0053\n",
      "Epoch: 69/100...  Loss: 0.0052\n",
      "Epoch: 69/100...  Loss: 0.0058\n",
      "Epoch: 69/100...  Loss: 0.0055\n",
      "Epoch: 69/100...  Loss: 0.0054\n",
      "Epoch: 69/100...  Loss: 0.0055\n",
      "Epoch: 69/100...  Loss: 0.0054\n",
      "Epoch: 69/100...  Loss: 0.0054\n",
      "Epoch: 69/100...  Loss: 0.0057\n",
      "Epoch: 69/100...  Loss: 0.0052\n",
      "Epoch: 70/100...  Loss: 0.0051\n",
      "Epoch: 70/100...  Loss: 0.0050\n",
      "Epoch: 70/100...  Loss: 0.0057\n",
      "Epoch: 70/100...  Loss: 0.0053\n",
      "Epoch: 70/100...  Loss: 0.0052\n",
      "Epoch: 70/100...  Loss: 0.0054\n",
      "Epoch: 70/100...  Loss: 0.0053\n",
      "Epoch: 70/100...  Loss: 0.0052\n",
      "Epoch: 70/100...  Loss: 0.0055\n",
      "Epoch: 70/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0050\n",
      "Epoch: 71/100...  Loss: 0.0049\n",
      "Epoch: 71/100...  Loss: 0.0055\n",
      "Epoch: 71/100...  Loss: 0.0052\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0052\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0053\n",
      "Epoch: 71/100...  Loss: 0.0049\n",
      "Epoch: 72/100...  Loss: 0.0048\n",
      "Epoch: 72/100...  Loss: 0.0047\n",
      "Epoch: 72/100...  Loss: 0.0054\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0049\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0049\n",
      "Epoch: 72/100...  Loss: 0.0052\n",
      "Epoch: 72/100...  Loss: 0.0047\n",
      "Epoch: 73/100...  Loss: 0.0047\n",
      "Epoch: 73/100...  Loss: 0.0046\n",
      "Epoch: 73/100...  Loss: 0.0052\n",
      "Epoch: 73/100...  Loss: 0.0049\n",
      "Epoch: 73/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0049\n",
      "Epoch: 73/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0050\n",
      "Epoch: 73/100...  Loss: 0.0046\n",
      "Epoch: 74/100...  Loss: 0.0046\n",
      "Epoch: 74/100...  Loss: 0.0044\n",
      "Epoch: 74/100...  Loss: 0.0051\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0046\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0046\n",
      "Epoch: 74/100...  Loss: 0.0049\n",
      "Epoch: 74/100...  Loss: 0.0045\n",
      "Epoch: 75/100...  Loss: 0.0044\n",
      "Epoch: 75/100...  Loss: 0.0043\n",
      "Epoch: 75/100...  Loss: 0.0049\n",
      "Epoch: 75/100...  Loss: 0.0046\n",
      "Epoch: 75/100...  Loss: 0.0045\n",
      "Epoch: 75/100...  Loss: 0.0046\n",
      "Epoch: 75/100...  Loss: 0.0045\n",
      "Epoch: 75/100...  Loss: 0.0045\n",
      "Epoch: 75/100...  Loss: 0.0047\n",
      "Epoch: 75/100...  Loss: 0.0043\n",
      "Epoch: 76/100...  Loss: 0.0043\n",
      "Epoch: 76/100...  Loss: 0.0042\n",
      "Epoch: 76/100...  Loss: 0.0048\n",
      "Epoch: 76/100...  Loss: 0.0045\n",
      "Epoch: 76/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0046\n",
      "Epoch: 76/100...  Loss: 0.0042\n",
      "Epoch: 77/100...  Loss: 0.0042\n",
      "Epoch: 77/100...  Loss: 0.0041\n",
      "Epoch: 77/100...  Loss: 0.0047\n",
      "Epoch: 77/100...  Loss: 0.0044\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0045\n",
      "Epoch: 77/100...  Loss: 0.0041\n",
      "Epoch: 78/100...  Loss: 0.0041\n",
      "Epoch: 78/100...  Loss: 0.0040\n",
      "Epoch: 78/100...  Loss: 0.0046\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0044\n",
      "Epoch: 78/100...  Loss: 0.0040\n",
      "Epoch: 79/100...  Loss: 0.0040\n",
      "Epoch: 79/100...  Loss: 0.0039\n",
      "Epoch: 79/100...  Loss: 0.0044\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0042\n",
      "Epoch: 79/100...  Loss: 0.0039\n",
      "Epoch: 80/100...  Loss: 0.0039\n",
      "Epoch: 80/100...  Loss: 0.0038\n",
      "Epoch: 80/100...  Loss: 0.0044\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0041\n",
      "Epoch: 80/100...  Loss: 0.0038\n",
      "Epoch: 81/100...  Loss: 0.0038\n",
      "Epoch: 81/100...  Loss: 0.0037\n",
      "Epoch: 81/100...  Loss: 0.0043\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0040\n",
      "Epoch: 81/100...  Loss: 0.0037\n",
      "Epoch: 82/100...  Loss: 0.0037\n",
      "Epoch: 82/100...  Loss: 0.0036\n",
      "Epoch: 82/100...  Loss: 0.0042\n",
      "Epoch: 82/100...  Loss: 0.0039\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0039\n",
      "Epoch: 82/100...  Loss: 0.0036\n",
      "Epoch: 83/100...  Loss: 0.0036\n",
      "Epoch: 83/100...  Loss: 0.0035\n",
      "Epoch: 83/100...  Loss: 0.0043\n",
      "Epoch: 83/100...  Loss: 0.0038\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0038\n",
      "Epoch: 83/100...  Loss: 0.0035\n",
      "Epoch: 84/100...  Loss: 0.0035\n",
      "Epoch: 84/100...  Loss: 0.0035\n",
      "Epoch: 84/100...  Loss: 0.0040\n",
      "Epoch: 84/100...  Loss: 0.0037\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0037\n",
      "Epoch: 84/100...  Loss: 0.0034\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 85/100...  Loss: 0.0038\n",
      "Epoch: 85/100...  Loss: 0.0036\n",
      "Epoch: 85/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0037\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0033\n",
      "Epoch: 86/100...  Loss: 0.0033\n",
      "Epoch: 86/100...  Loss: 0.0037\n",
      "Epoch: 86/100...  Loss: 0.0035\n",
      "Epoch: 86/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0036\n",
      "Epoch: 86/100...  Loss: 0.0033\n",
      "Epoch: 87/100...  Loss: 0.0033\n",
      "Epoch: 87/100...  Loss: 0.0032\n",
      "Epoch: 87/100...  Loss: 0.0036\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0033\n",
      "Epoch: 87/100...  Loss: 0.0033\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0035\n",
      "Epoch: 87/100...  Loss: 0.0032\n",
      "Epoch: 88/100...  Loss: 0.0032\n",
      "Epoch: 88/100...  Loss: 0.0032\n",
      "Epoch: 88/100...  Loss: 0.0036\n",
      "Epoch: 88/100...  Loss: 0.0034\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0034\n",
      "Epoch: 88/100...  Loss: 0.0031\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 89/100...  Loss: 0.0035\n",
      "Epoch: 89/100...  Loss: 0.0033\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0033\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0030\n",
      "Epoch: 90/100...  Loss: 0.0030\n",
      "Epoch: 90/100...  Loss: 0.0034\n",
      "Epoch: 90/100...  Loss: 0.0032\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0032\n",
      "Epoch: 90/100...  Loss: 0.0032\n",
      "Epoch: 90/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0033\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0032\n",
      "Epoch: 91/100...  Loss: 0.0029\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 92/100...  Loss: 0.0033\n",
      "Epoch: 92/100...  Loss: 0.0031\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0031\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0032\n",
      "Epoch: 93/100...  Loss: 0.0030\n",
      "Epoch: 93/100...  Loss: 0.0030\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0030\n",
      "Epoch: 93/100...  Loss: 0.0030\n",
      "Epoch: 93/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0031\n",
      "Epoch: 94/100...  Loss: 0.0030\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0030\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0027\n",
      "Epoch: 95/100...  Loss: 0.0027\n",
      "Epoch: 95/100...  Loss: 0.0031\n",
      "Epoch: 95/100...  Loss: 0.0029\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0029\n",
      "Epoch: 95/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0030\n",
      "Epoch: 96/100...  Loss: 0.0029\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0029\n",
      "Epoch: 96/100...  Loss: 0.0026\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 97/100...  Loss: 0.0030\n",
      "Epoch: 97/100...  Loss: 0.0028\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0028\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0029\n",
      "Epoch: 98/100...  Loss: 0.0028\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 99/100...  Loss: 0.0028\n",
      "Epoch: 99/100...  Loss: 0.0027\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0027\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0028\n",
      "Epoch: 100/100...  Loss: 0.0027\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0025\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def load_model(architecture = 'nn', filepath = 'checkpoint.pth'):\n",
    "    \"\"\"\n",
    "    Function loads the model from checkpoint.\n",
    "\n",
    "    INPUT:\n",
    "        architecture - model architecture ('nn' - for fully connected neural network, 'conv' - for convolutional neural\n",
    "        network)\n",
    "        filepath - path for the saved model\n",
    "\n",
    "    OUTPUT:\n",
    "        model - loaded pytorch model\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading model from {} \\n\".format(filepath))\n",
    "\n",
    "    if architecture == 'nn':\n",
    "        checkpoint = torch.load(filepath)\n",
    "        input_size = checkpoint['input_size']\n",
    "        output_size = checkpoint['output_size']\n",
    "        hidden_sizes = checkpoint['hidden_layers']\n",
    "        dropout = checkpoint['dropout']\n",
    "        model = nn.Sequential(OrderedDict([\n",
    "                              ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                              ('relu1', nn.ReLU()),\n",
    "                              ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                              ('bn2', nn.BatchNorm1d(num_features=hidden_sizes[1])),\n",
    "                              ('relu2', nn.ReLU()),\n",
    "                              ('dropout', nn.Dropout(dropout)),\n",
    "                              ('fc3', nn.Linear(hidden_sizes[1], hidden_sizes[2])),\n",
    "                              ('bn3', nn.BatchNorm1d(num_features=hidden_sizes[2])),\n",
    "                              ('relu3', nn.ReLU()),\n",
    "                              ('logits', nn.Linear(hidden_sizes[2], output_size))]))\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    else:\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model = SimpleCNN()\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "model = load_model(architecture = 'nn', filepath = 'data/checkpoint.pth')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading model from data/checkpoint.pth \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "def get_preds(model, input, architecture = 'nn'):\n",
    "    \"\"\"\n",
    "    Function to get predicted probabilities from the model for each class.\n",
    "\n",
    "    INPUT:\n",
    "        model - pytorch model\n",
    "        input - (tensor) input vector\n",
    "\n",
    "    OUTPUT:\n",
    "        ps - (tensor) vector of predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # Turn off gradients to speed up this part\n",
    "    with torch.no_grad():\n",
    "        if architecture == 'nn':\n",
    "            logits = model.forward(input)\n",
    "        else:\n",
    "            image = input.numpy()\n",
    "            image = image.reshape(image.shape[0], 1, 28, 28)\n",
    "            logits = model.forward(torch.from_numpy(image).float())\n",
    "\n",
    "    ps = F.softmax(logits, dim=1)\n",
    "    return ps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "def evaluate_model(model, train, y_train, test, y_test, architecture = 'nn'):\n",
    "    \"\"\"\n",
    "    Function to print out train and test accuracy of the model.\n",
    "\n",
    "    INPUT:\n",
    "        model - pytorch model\n",
    "        train - (tensor) train dataset\n",
    "        y_train - (numpy) labels for train dataset\n",
    "        test - (tensor) test dataset\n",
    "        y_test - (numpy) labels for test dataset\n",
    "\n",
    "    OUTPUT:\n",
    "        accuracy_train - accuracy on train dataset\n",
    "        accuracy_test - accuracy on test dataset\n",
    "    \"\"\"\n",
    "    train_pred = get_preds(model, train, architecture)\n",
    "    train_pred_labels = get_labels(train_pred)\n",
    "\n",
    "    test_pred = get_preds(model, test, architecture)\n",
    "    test_pred_labels = get_labels(test_pred)\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, train_pred_labels)\n",
    "    accuracy_test = accuracy_score(y_test, test_pred_labels)\n",
    "\n",
    "    print(\"Accuracy score for train set is {} \\n\".format(accuracy_train))\n",
    "    print(\"Accuracy score for test set is {} \\n\".format(accuracy_test))\n",
    "\n",
    "    return accuracy_train, accuracy_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "def get_labels(pred):\n",
    "    \"\"\"\n",
    "        Function to get the vector of predicted labels for the images in\n",
    "        the dataset.\n",
    "\n",
    "        INPUT:\n",
    "            pred - (tensor) vector of predictions (probabilities for each class)\n",
    "        OUTPUT:\n",
    "            pred_labels - (numpy) array of predicted classes for each vector\n",
    "    \"\"\"\n",
    "\n",
    "    pred_np = pred.numpy()\n",
    "    pred_values = np.amax(pred_np, axis=1, keepdims=True)\n",
    "    pred_labels = np.array([np.where(pred_np[i, :] == pred_values[i, :])[0] for i in range(pred_np.shape[0])])\n",
    "    pred_labels = pred_labels.reshape(len(pred_np), 1)\n",
    "\n",
    "    return pred_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_test = y_test.reshape((X_test.shape[0], 1))\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "evaluate_model(model, X_train_shuffled, y_train_shuffled, X_test, y_test, architecture='nn')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dim'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-6435978138ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_shuffled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_shuffled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-4a14aba8493a>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, train, y_train, test, y_test, architecture)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_pred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtest_pred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-8e29e537b0e1>\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(model, input, architecture)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0marchitecture\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dl-env/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dl-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dl-env/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dl-env/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dim'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "evaluate_model"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('dl-env': conda)"
  },
  "interpreter": {
   "hash": "2aac1d2b359798efc30b59d804167ffd538e3092f84ffa86abb6645ab952d97e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}