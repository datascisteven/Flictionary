{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# import linear algebra and data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import matplotlib for plotting\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import helper libraries\n",
    "import requests\n",
    "from io import BytesIO # Use When expecting bytes-like objects\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from os import path\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "# import PIL for image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"src/\")\n",
    "import image_utils\n",
    "from image_utils import add_flipped_and_rotated_images\n",
    "from simple_conv_nn import SimpleCNN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting model with epochs = 100, learning rate = 0.003\n",
      "\n",
      "Epoch: 1/100...  Loss: 2.2537\n",
      "Epoch: 1/100...  Loss: 2.0904\n",
      "Epoch: 1/100...  Loss: 1.9694\n",
      "Epoch: 1/100...  Loss: 1.8898\n",
      "Epoch: 1/100...  Loss: 1.8384\n",
      "Epoch: 1/100...  Loss: 1.7660\n",
      "Epoch: 1/100...  Loss: 1.7301\n",
      "Epoch: 1/100...  Loss: 1.7217\n",
      "Epoch: 1/100...  Loss: 1.6917\n",
      "Epoch: 1/100...  Loss: 1.6503\n",
      "Epoch: 2/100...  Loss: 1.6063\n",
      "Epoch: 2/100...  Loss: 1.6096\n",
      "Epoch: 2/100...  Loss: 1.5696\n",
      "Epoch: 2/100...  Loss: 1.5251\n",
      "Epoch: 2/100...  Loss: 1.5405\n",
      "Epoch: 2/100...  Loss: 1.4765\n",
      "Epoch: 2/100...  Loss: 1.4631\n",
      "Epoch: 2/100...  Loss: 1.4929\n",
      "Epoch: 2/100...  Loss: 1.4936\n",
      "Epoch: 2/100...  Loss: 1.4398\n",
      "Epoch: 3/100...  Loss: 1.4254\n",
      "Epoch: 3/100...  Loss: 1.4438\n",
      "Epoch: 3/100...  Loss: 1.4103\n",
      "Epoch: 3/100...  Loss: 1.3643\n",
      "Epoch: 3/100...  Loss: 1.3967\n",
      "Epoch: 3/100...  Loss: 1.3275\n",
      "Epoch: 3/100...  Loss: 1.3228\n",
      "Epoch: 3/100...  Loss: 1.3605\n",
      "Epoch: 3/100...  Loss: 1.3801\n",
      "Epoch: 3/100...  Loss: 1.3096\n",
      "Epoch: 4/100...  Loss: 1.3054\n",
      "Epoch: 4/100...  Loss: 1.3340\n",
      "Epoch: 4/100...  Loss: 1.2989\n",
      "Epoch: 4/100...  Loss: 1.2570\n",
      "Epoch: 4/100...  Loss: 1.2980\n",
      "Epoch: 4/100...  Loss: 1.2281\n",
      "Epoch: 4/100...  Loss: 1.2252\n",
      "Epoch: 4/100...  Loss: 1.2625\n",
      "Epoch: 4/100...  Loss: 1.2901\n",
      "Epoch: 4/100...  Loss: 1.2116\n",
      "Epoch: 5/100...  Loss: 1.2140\n",
      "Epoch: 5/100...  Loss: 1.2461\n",
      "Epoch: 5/100...  Loss: 1.2093\n",
      "Epoch: 5/100...  Loss: 1.1679\n",
      "Epoch: 5/100...  Loss: 1.2159\n",
      "Epoch: 5/100...  Loss: 1.1459\n",
      "Epoch: 5/100...  Loss: 1.1421\n",
      "Epoch: 5/100...  Loss: 1.1827\n",
      "Epoch: 5/100...  Loss: 1.2113\n",
      "Epoch: 5/100...  Loss: 1.1277\n",
      "Epoch: 6/100...  Loss: 1.1347\n",
      "Epoch: 6/100...  Loss: 1.1663\n",
      "Epoch: 6/100...  Loss: 1.1290\n",
      "Epoch: 6/100...  Loss: 1.0927\n",
      "Epoch: 6/100...  Loss: 1.1416\n",
      "Epoch: 6/100...  Loss: 1.0716\n",
      "Epoch: 6/100...  Loss: 1.0679\n",
      "Epoch: 6/100...  Loss: 1.1083\n",
      "Epoch: 6/100...  Loss: 1.1352\n",
      "Epoch: 6/100...  Loss: 1.0530\n",
      "Epoch: 7/100...  Loss: 1.0643\n",
      "Epoch: 7/100...  Loss: 1.0924\n",
      "Epoch: 7/100...  Loss: 1.0578\n",
      "Epoch: 7/100...  Loss: 1.0240\n",
      "Epoch: 7/100...  Loss: 1.0683\n",
      "Epoch: 7/100...  Loss: 1.0048\n",
      "Epoch: 7/100...  Loss: 0.9972\n",
      "Epoch: 7/100...  Loss: 1.0373\n",
      "Epoch: 7/100...  Loss: 1.0660\n",
      "Epoch: 7/100...  Loss: 0.9827\n",
      "Epoch: 8/100...  Loss: 0.9987\n",
      "Epoch: 8/100...  Loss: 1.0197\n",
      "Epoch: 8/100...  Loss: 0.9902\n",
      "Epoch: 8/100...  Loss: 0.9588\n",
      "Epoch: 8/100...  Loss: 0.9992\n",
      "Epoch: 8/100...  Loss: 0.9409\n",
      "Epoch: 8/100...  Loss: 0.9314\n",
      "Epoch: 8/100...  Loss: 0.9715\n",
      "Epoch: 8/100...  Loss: 0.9988\n",
      "Epoch: 8/100...  Loss: 0.9176\n",
      "Epoch: 9/100...  Loss: 0.9369\n",
      "Epoch: 9/100...  Loss: 0.9531\n",
      "Epoch: 9/100...  Loss: 0.9266\n",
      "Epoch: 9/100...  Loss: 0.8969\n",
      "Epoch: 9/100...  Loss: 0.9339\n",
      "Epoch: 9/100...  Loss: 0.8817\n",
      "Epoch: 9/100...  Loss: 0.8687\n",
      "Epoch: 9/100...  Loss: 0.9068\n",
      "Epoch: 9/100...  Loss: 0.9318\n",
      "Epoch: 9/100...  Loss: 0.8572\n",
      "Epoch: 10/100...  Loss: 0.8737\n",
      "Epoch: 10/100...  Loss: 0.8903\n",
      "Epoch: 10/100...  Loss: 0.8648\n",
      "Epoch: 10/100...  Loss: 0.8386\n",
      "Epoch: 10/100...  Loss: 0.8761\n",
      "Epoch: 10/100...  Loss: 0.8214\n",
      "Epoch: 10/100...  Loss: 0.8089\n",
      "Epoch: 10/100...  Loss: 0.8485\n",
      "Epoch: 10/100...  Loss: 0.8701\n",
      "Epoch: 10/100...  Loss: 0.8010\n",
      "Epoch: 11/100...  Loss: 0.8131\n",
      "Epoch: 11/100...  Loss: 0.8318\n",
      "Epoch: 11/100...  Loss: 0.8008\n",
      "Epoch: 11/100...  Loss: 0.7831\n",
      "Epoch: 11/100...  Loss: 0.8190\n",
      "Epoch: 11/100...  Loss: 0.7669\n",
      "Epoch: 11/100...  Loss: 0.7543\n",
      "Epoch: 11/100...  Loss: 0.7917\n",
      "Epoch: 11/100...  Loss: 0.8069\n",
      "Epoch: 11/100...  Loss: 0.7425\n",
      "Epoch: 12/100...  Loss: 0.7573\n",
      "Epoch: 12/100...  Loss: 0.7775\n",
      "Epoch: 12/100...  Loss: 0.7418\n",
      "Epoch: 12/100...  Loss: 0.7315\n",
      "Epoch: 12/100...  Loss: 0.7626\n",
      "Epoch: 12/100...  Loss: 0.7115\n",
      "Epoch: 12/100...  Loss: 0.7022\n",
      "Epoch: 12/100...  Loss: 0.7356\n",
      "Epoch: 12/100...  Loss: 0.7451\n",
      "Epoch: 12/100...  Loss: 0.6849\n",
      "Epoch: 13/100...  Loss: 0.7008\n",
      "Epoch: 13/100...  Loss: 0.7225\n",
      "Epoch: 13/100...  Loss: 0.6870\n",
      "Epoch: 13/100...  Loss: 0.6795\n",
      "Epoch: 13/100...  Loss: 0.7037\n",
      "Epoch: 13/100...  Loss: 0.6581\n",
      "Epoch: 13/100...  Loss: 0.6478\n",
      "Epoch: 13/100...  Loss: 0.6776\n",
      "Epoch: 13/100...  Loss: 0.6883\n",
      "Epoch: 13/100...  Loss: 0.6376\n",
      "Epoch: 14/100...  Loss: 0.6494\n",
      "Epoch: 14/100...  Loss: 0.6659\n",
      "Epoch: 14/100...  Loss: 0.6375\n",
      "Epoch: 14/100...  Loss: 0.6326\n",
      "Epoch: 14/100...  Loss: 0.6490\n",
      "Epoch: 14/100...  Loss: 0.6100\n",
      "Epoch: 14/100...  Loss: 0.6008\n",
      "Epoch: 14/100...  Loss: 0.6281\n",
      "Epoch: 14/100...  Loss: 0.6383\n",
      "Epoch: 14/100...  Loss: 0.5880\n",
      "Epoch: 15/100...  Loss: 0.5964\n",
      "Epoch: 15/100...  Loss: 0.6177\n",
      "Epoch: 15/100...  Loss: 0.5891\n",
      "Epoch: 15/100...  Loss: 0.5851\n",
      "Epoch: 15/100...  Loss: 0.5974\n",
      "Epoch: 15/100...  Loss: 0.5621\n",
      "Epoch: 15/100...  Loss: 0.5514\n",
      "Epoch: 15/100...  Loss: 0.5773\n",
      "Epoch: 15/100...  Loss: 0.5856\n",
      "Epoch: 15/100...  Loss: 0.5414\n",
      "Epoch: 16/100...  Loss: 0.5531\n",
      "Epoch: 16/100...  Loss: 0.5675\n",
      "Epoch: 16/100...  Loss: 0.5421\n",
      "Epoch: 16/100...  Loss: 0.5428\n",
      "Epoch: 16/100...  Loss: 0.5501\n",
      "Epoch: 16/100...  Loss: 0.5170\n",
      "Epoch: 16/100...  Loss: 0.5035\n",
      "Epoch: 16/100...  Loss: 0.5270\n",
      "Epoch: 16/100...  Loss: 0.5393\n",
      "Epoch: 16/100...  Loss: 0.4968\n",
      "Epoch: 17/100...  Loss: 0.5108\n",
      "Epoch: 17/100...  Loss: 0.5211\n",
      "Epoch: 17/100...  Loss: 0.4987\n",
      "Epoch: 17/100...  Loss: 0.4977\n",
      "Epoch: 17/100...  Loss: 0.5000\n",
      "Epoch: 17/100...  Loss: 0.4749\n",
      "Epoch: 17/100...  Loss: 0.4631\n",
      "Epoch: 17/100...  Loss: 0.4806\n",
      "Epoch: 17/100...  Loss: 0.4910\n",
      "Epoch: 17/100...  Loss: 0.4583\n",
      "Epoch: 18/100...  Loss: 0.4692\n",
      "Epoch: 18/100...  Loss: 0.4773\n",
      "Epoch: 18/100...  Loss: 0.4555\n",
      "Epoch: 18/100...  Loss: 0.4592\n",
      "Epoch: 18/100...  Loss: 0.4556\n",
      "Epoch: 18/100...  Loss: 0.4318\n",
      "Epoch: 18/100...  Loss: 0.4187\n",
      "Epoch: 18/100...  Loss: 0.4383\n",
      "Epoch: 18/100...  Loss: 0.4447\n",
      "Epoch: 18/100...  Loss: 0.4118\n",
      "Epoch: 19/100...  Loss: 0.4280\n",
      "Epoch: 19/100...  Loss: 0.4365\n",
      "Epoch: 19/100...  Loss: 0.4160\n",
      "Epoch: 19/100...  Loss: 0.4196\n",
      "Epoch: 19/100...  Loss: 0.4170\n",
      "Epoch: 19/100...  Loss: 0.3935\n",
      "Epoch: 19/100...  Loss: 0.3817\n",
      "Epoch: 19/100...  Loss: 0.3904\n",
      "Epoch: 19/100...  Loss: 0.4067\n",
      "Epoch: 19/100...  Loss: 0.3764\n",
      "Epoch: 20/100...  Loss: 0.3905\n",
      "Epoch: 20/100...  Loss: 0.3979\n",
      "Epoch: 20/100...  Loss: 0.3712\n",
      "Epoch: 20/100...  Loss: 0.3838\n",
      "Epoch: 20/100...  Loss: 0.3786\n",
      "Epoch: 20/100...  Loss: 0.3529\n",
      "Epoch: 20/100...  Loss: 0.3453\n",
      "Epoch: 20/100...  Loss: 0.3535\n",
      "Epoch: 20/100...  Loss: 0.3659\n",
      "Epoch: 20/100...  Loss: 0.3438\n",
      "Epoch: 21/100...  Loss: 0.3517\n",
      "Epoch: 21/100...  Loss: 0.3624\n",
      "Epoch: 21/100...  Loss: 0.3370\n",
      "Epoch: 21/100...  Loss: 0.3527\n",
      "Epoch: 21/100...  Loss: 0.3357\n",
      "Epoch: 21/100...  Loss: 0.3180\n",
      "Epoch: 21/100...  Loss: 0.3085\n",
      "Epoch: 21/100...  Loss: 0.3191\n",
      "Epoch: 21/100...  Loss: 0.3290\n",
      "Epoch: 21/100...  Loss: 0.3094\n",
      "Epoch: 22/100...  Loss: 0.3151\n",
      "Epoch: 22/100...  Loss: 0.3250\n",
      "Epoch: 22/100...  Loss: 0.2963\n",
      "Epoch: 22/100...  Loss: 0.3202\n",
      "Epoch: 22/100...  Loss: 0.2959\n",
      "Epoch: 22/100...  Loss: 0.2892\n",
      "Epoch: 22/100...  Loss: 0.2750\n",
      "Epoch: 22/100...  Loss: 0.2881\n",
      "Epoch: 22/100...  Loss: 0.2935\n",
      "Epoch: 22/100...  Loss: 0.2738\n",
      "Epoch: 23/100...  Loss: 0.2838\n",
      "Epoch: 23/100...  Loss: 0.2961\n",
      "Epoch: 23/100...  Loss: 0.2674\n",
      "Epoch: 23/100...  Loss: 0.2901\n",
      "Epoch: 23/100...  Loss: 0.2698\n",
      "Epoch: 23/100...  Loss: 0.2592\n",
      "Epoch: 23/100...  Loss: 0.2417\n",
      "Epoch: 23/100...  Loss: 0.2607\n",
      "Epoch: 23/100...  Loss: 0.2577\n",
      "Epoch: 23/100...  Loss: 0.2443\n",
      "Epoch: 24/100...  Loss: 0.2554\n",
      "Epoch: 24/100...  Loss: 0.2653\n",
      "Epoch: 24/100...  Loss: 0.2404\n",
      "Epoch: 24/100...  Loss: 0.2579\n",
      "Epoch: 24/100...  Loss: 0.2343\n",
      "Epoch: 24/100...  Loss: 0.2339\n",
      "Epoch: 24/100...  Loss: 0.2153\n",
      "Epoch: 24/100...  Loss: 0.2310\n",
      "Epoch: 24/100...  Loss: 0.2357\n",
      "Epoch: 24/100...  Loss: 0.2193\n",
      "Epoch: 25/100...  Loss: 0.2245\n",
      "Epoch: 25/100...  Loss: 0.2291\n",
      "Epoch: 25/100...  Loss: 0.2062\n",
      "Epoch: 25/100...  Loss: 0.2269\n",
      "Epoch: 25/100...  Loss: 0.2165\n",
      "Epoch: 25/100...  Loss: 0.2099\n",
      "Epoch: 25/100...  Loss: 0.1855\n",
      "Epoch: 25/100...  Loss: 0.1982\n",
      "Epoch: 25/100...  Loss: 0.2100\n",
      "Epoch: 25/100...  Loss: 0.1930\n",
      "Epoch: 26/100...  Loss: 0.1955\n",
      "Epoch: 26/100...  Loss: 0.2059\n",
      "Epoch: 26/100...  Loss: 0.1810\n",
      "Epoch: 26/100...  Loss: 0.2005\n",
      "Epoch: 26/100...  Loss: 0.1926\n",
      "Epoch: 26/100...  Loss: 0.1922\n",
      "Epoch: 26/100...  Loss: 0.1633\n",
      "Epoch: 26/100...  Loss: 0.1770\n",
      "Epoch: 26/100...  Loss: 0.1804\n",
      "Epoch: 26/100...  Loss: 0.1697\n",
      "Epoch: 27/100...  Loss: 0.1713\n",
      "Epoch: 27/100...  Loss: 0.1763\n",
      "Epoch: 27/100...  Loss: 0.1627\n",
      "Epoch: 27/100...  Loss: 0.1828\n",
      "Epoch: 27/100...  Loss: 0.1660\n",
      "Epoch: 27/100...  Loss: 0.1628\n",
      "Epoch: 27/100...  Loss: 0.1430\n",
      "Epoch: 27/100...  Loss: 0.1565\n",
      "Epoch: 27/100...  Loss: 0.1546\n",
      "Epoch: 27/100...  Loss: 0.1439\n",
      "Epoch: 28/100...  Loss: 0.1484\n",
      "Epoch: 28/100...  Loss: 0.1548\n",
      "Epoch: 28/100...  Loss: 0.1397\n",
      "Epoch: 28/100...  Loss: 0.1561\n",
      "Epoch: 28/100...  Loss: 0.1414\n",
      "Epoch: 28/100...  Loss: 0.1465\n",
      "Epoch: 28/100...  Loss: 0.1224\n",
      "Epoch: 28/100...  Loss: 0.1318\n",
      "Epoch: 28/100...  Loss: 0.1319\n",
      "Epoch: 28/100...  Loss: 0.1256\n",
      "Epoch: 29/100...  Loss: 0.1228\n",
      "Epoch: 29/100...  Loss: 0.1338\n",
      "Epoch: 29/100...  Loss: 0.1209\n",
      "Epoch: 29/100...  Loss: 0.1305\n",
      "Epoch: 29/100...  Loss: 0.1278\n",
      "Epoch: 29/100...  Loss: 0.1235\n",
      "Epoch: 29/100...  Loss: 0.1086\n",
      "Epoch: 29/100...  Loss: 0.1147\n",
      "Epoch: 29/100...  Loss: 0.1146\n",
      "Epoch: 29/100...  Loss: 0.1082\n",
      "Epoch: 30/100...  Loss: 0.1071\n",
      "Epoch: 30/100...  Loss: 0.1128\n",
      "Epoch: 30/100...  Loss: 0.1015\n",
      "Epoch: 30/100...  Loss: 0.1145\n",
      "Epoch: 30/100...  Loss: 0.1079\n",
      "Epoch: 30/100...  Loss: 0.1048\n",
      "Epoch: 30/100...  Loss: 0.0909\n",
      "Epoch: 30/100...  Loss: 0.1029\n",
      "Epoch: 30/100...  Loss: 0.0986\n",
      "Epoch: 30/100...  Loss: 0.0950\n",
      "Epoch: 31/100...  Loss: 0.0931\n",
      "Epoch: 31/100...  Loss: 0.0963\n",
      "Epoch: 31/100...  Loss: 0.0895\n",
      "Epoch: 31/100...  Loss: 0.0977\n",
      "Epoch: 31/100...  Loss: 0.0925\n",
      "Epoch: 31/100...  Loss: 0.0932\n",
      "Epoch: 31/100...  Loss: 0.0798\n",
      "Epoch: 31/100...  Loss: 0.0869\n",
      "Epoch: 31/100...  Loss: 0.0849\n",
      "Epoch: 31/100...  Loss: 0.0796\n",
      "Epoch: 32/100...  Loss: 0.0764\n",
      "Epoch: 32/100...  Loss: 0.0814\n",
      "Epoch: 32/100...  Loss: 0.0761\n",
      "Epoch: 32/100...  Loss: 0.0820\n",
      "Epoch: 32/100...  Loss: 0.0801\n",
      "Epoch: 32/100...  Loss: 0.0780\n",
      "Epoch: 32/100...  Loss: 0.0686\n",
      "Epoch: 32/100...  Loss: 0.0744\n",
      "Epoch: 32/100...  Loss: 0.0716\n",
      "Epoch: 32/100...  Loss: 0.0666\n",
      "Epoch: 33/100...  Loss: 0.0658\n",
      "Epoch: 33/100...  Loss: 0.0685\n",
      "Epoch: 33/100...  Loss: 0.0647\n",
      "Epoch: 33/100...  Loss: 0.0706\n",
      "Epoch: 33/100...  Loss: 0.0687\n",
      "Epoch: 33/100...  Loss: 0.0662\n",
      "Epoch: 33/100...  Loss: 0.0578\n",
      "Epoch: 33/100...  Loss: 0.0660\n",
      "Epoch: 33/100...  Loss: 0.0606\n",
      "Epoch: 33/100...  Loss: 0.0575\n",
      "Epoch: 34/100...  Loss: 0.0562\n",
      "Epoch: 34/100...  Loss: 0.0584\n",
      "Epoch: 34/100...  Loss: 0.0544\n",
      "Epoch: 34/100...  Loss: 0.0611\n",
      "Epoch: 34/100...  Loss: 0.0581\n",
      "Epoch: 34/100...  Loss: 0.0567\n",
      "Epoch: 34/100...  Loss: 0.0488\n",
      "Epoch: 34/100...  Loss: 0.0559\n",
      "Epoch: 34/100...  Loss: 0.0524\n",
      "Epoch: 34/100...  Loss: 0.0498\n",
      "Epoch: 35/100...  Loss: 0.0480\n",
      "Epoch: 35/100...  Loss: 0.0497\n",
      "Epoch: 35/100...  Loss: 0.0469\n",
      "Epoch: 35/100...  Loss: 0.0519\n",
      "Epoch: 35/100...  Loss: 0.0502\n",
      "Epoch: 35/100...  Loss: 0.0497\n",
      "Epoch: 35/100...  Loss: 0.0424\n",
      "Epoch: 35/100...  Loss: 0.0489\n",
      "Epoch: 35/100...  Loss: 0.0453\n",
      "Epoch: 35/100...  Loss: 0.0432\n",
      "Epoch: 36/100...  Loss: 0.0424\n",
      "Epoch: 36/100...  Loss: 0.0435\n",
      "Epoch: 36/100...  Loss: 0.0402\n",
      "Epoch: 36/100...  Loss: 0.0459\n",
      "Epoch: 36/100...  Loss: 0.0434\n",
      "Epoch: 36/100...  Loss: 0.0438\n",
      "Epoch: 36/100...  Loss: 0.0368\n",
      "Epoch: 36/100...  Loss: 0.0429\n",
      "Epoch: 36/100...  Loss: 0.0397\n",
      "Epoch: 36/100...  Loss: 0.0388\n",
      "Epoch: 37/100...  Loss: 0.0370\n",
      "Epoch: 37/100...  Loss: 0.0385\n",
      "Epoch: 37/100...  Loss: 0.0356\n",
      "Epoch: 37/100...  Loss: 0.0406\n",
      "Epoch: 37/100...  Loss: 0.0391\n",
      "Epoch: 37/100...  Loss: 0.0387\n",
      "Epoch: 37/100...  Loss: 0.0326\n",
      "Epoch: 37/100...  Loss: 0.0378\n",
      "Epoch: 37/100...  Loss: 0.0349\n",
      "Epoch: 37/100...  Loss: 0.0342\n",
      "Epoch: 38/100...  Loss: 0.0331\n",
      "Epoch: 38/100...  Loss: 0.0340\n",
      "Epoch: 38/100...  Loss: 0.0315\n",
      "Epoch: 38/100...  Loss: 0.0362\n",
      "Epoch: 38/100...  Loss: 0.0348\n",
      "Epoch: 38/100...  Loss: 0.0341\n",
      "Epoch: 38/100...  Loss: 0.0289\n",
      "Epoch: 38/100...  Loss: 0.0322\n",
      "Epoch: 38/100...  Loss: 0.0312\n",
      "Epoch: 38/100...  Loss: 0.0306\n",
      "Epoch: 39/100...  Loss: 0.0296\n",
      "Epoch: 39/100...  Loss: 0.0302\n",
      "Epoch: 39/100...  Loss: 0.0286\n",
      "Epoch: 39/100...  Loss: 0.0321\n",
      "Epoch: 39/100...  Loss: 0.0314\n",
      "Epoch: 39/100...  Loss: 0.0309\n",
      "Epoch: 39/100...  Loss: 0.0261\n",
      "Epoch: 39/100...  Loss: 0.0289\n",
      "Epoch: 39/100...  Loss: 0.0282\n",
      "Epoch: 39/100...  Loss: 0.0274\n",
      "Epoch: 40/100...  Loss: 0.0268\n",
      "Epoch: 40/100...  Loss: 0.0273\n",
      "Epoch: 40/100...  Loss: 0.0258\n",
      "Epoch: 40/100...  Loss: 0.0294\n",
      "Epoch: 40/100...  Loss: 0.0283\n",
      "Epoch: 40/100...  Loss: 0.0282\n",
      "Epoch: 40/100...  Loss: 0.0237\n",
      "Epoch: 40/100...  Loss: 0.0264\n",
      "Epoch: 40/100...  Loss: 0.0256\n",
      "Epoch: 40/100...  Loss: 0.0248\n",
      "Epoch: 41/100...  Loss: 0.0243\n",
      "Epoch: 41/100...  Loss: 0.0242\n",
      "Epoch: 41/100...  Loss: 0.0238\n",
      "Epoch: 41/100...  Loss: 0.0266\n",
      "Epoch: 41/100...  Loss: 0.0253\n",
      "Epoch: 41/100...  Loss: 0.0257\n",
      "Epoch: 41/100...  Loss: 0.0216\n",
      "Epoch: 41/100...  Loss: 0.0239\n",
      "Epoch: 41/100...  Loss: 0.0234\n",
      "Epoch: 41/100...  Loss: 0.0224\n",
      "Epoch: 42/100...  Loss: 0.0223\n",
      "Epoch: 42/100...  Loss: 0.0219\n",
      "Epoch: 42/100...  Loss: 0.0219\n",
      "Epoch: 42/100...  Loss: 0.0244\n",
      "Epoch: 42/100...  Loss: 0.0228\n",
      "Epoch: 42/100...  Loss: 0.0238\n",
      "Epoch: 42/100...  Loss: 0.0199\n",
      "Epoch: 42/100...  Loss: 0.0220\n",
      "Epoch: 42/100...  Loss: 0.0214\n",
      "Epoch: 42/100...  Loss: 0.0205\n",
      "Epoch: 43/100...  Loss: 0.0202\n",
      "Epoch: 43/100...  Loss: 0.0203\n",
      "Epoch: 43/100...  Loss: 0.0200\n",
      "Epoch: 43/100...  Loss: 0.0225\n",
      "Epoch: 43/100...  Loss: 0.0208\n",
      "Epoch: 43/100...  Loss: 0.0218\n",
      "Epoch: 43/100...  Loss: 0.0185\n",
      "Epoch: 43/100...  Loss: 0.0203\n",
      "Epoch: 43/100...  Loss: 0.0196\n",
      "Epoch: 43/100...  Loss: 0.0187\n",
      "Epoch: 44/100...  Loss: 0.0187\n",
      "Epoch: 44/100...  Loss: 0.0186\n",
      "Epoch: 44/100...  Loss: 0.0186\n",
      "Epoch: 44/100...  Loss: 0.0208\n",
      "Epoch: 44/100...  Loss: 0.0192\n",
      "Epoch: 44/100...  Loss: 0.0199\n",
      "Epoch: 44/100...  Loss: 0.0172\n",
      "Epoch: 44/100...  Loss: 0.0188\n",
      "Epoch: 44/100...  Loss: 0.0181\n",
      "Epoch: 44/100...  Loss: 0.0171\n",
      "Epoch: 45/100...  Loss: 0.0174\n",
      "Epoch: 45/100...  Loss: 0.0172\n",
      "Epoch: 45/100...  Loss: 0.0171\n",
      "Epoch: 45/100...  Loss: 0.0193\n",
      "Epoch: 45/100...  Loss: 0.0176\n",
      "Epoch: 45/100...  Loss: 0.0184\n",
      "Epoch: 45/100...  Loss: 0.0160\n",
      "Epoch: 45/100...  Loss: 0.0174\n",
      "Epoch: 45/100...  Loss: 0.0167\n",
      "Epoch: 45/100...  Loss: 0.0159\n",
      "Epoch: 46/100...  Loss: 0.0161\n",
      "Epoch: 46/100...  Loss: 0.0160\n",
      "Epoch: 46/100...  Loss: 0.0159\n",
      "Epoch: 46/100...  Loss: 0.0180\n",
      "Epoch: 46/100...  Loss: 0.0163\n",
      "Epoch: 46/100...  Loss: 0.0172\n",
      "Epoch: 46/100...  Loss: 0.0150\n",
      "Epoch: 46/100...  Loss: 0.0163\n",
      "Epoch: 46/100...  Loss: 0.0156\n",
      "Epoch: 46/100...  Loss: 0.0147\n",
      "Epoch: 47/100...  Loss: 0.0150\n",
      "Epoch: 47/100...  Loss: 0.0149\n",
      "Epoch: 47/100...  Loss: 0.0148\n",
      "Epoch: 47/100...  Loss: 0.0167\n",
      "Epoch: 47/100...  Loss: 0.0151\n",
      "Epoch: 47/100...  Loss: 0.0161\n",
      "Epoch: 47/100...  Loss: 0.0141\n",
      "Epoch: 47/100...  Loss: 0.0152\n",
      "Epoch: 47/100...  Loss: 0.0145\n",
      "Epoch: 47/100...  Loss: 0.0137\n",
      "Epoch: 48/100...  Loss: 0.0141\n",
      "Epoch: 48/100...  Loss: 0.0139\n",
      "Epoch: 48/100...  Loss: 0.0141\n",
      "Epoch: 48/100...  Loss: 0.0157\n",
      "Epoch: 48/100...  Loss: 0.0140\n",
      "Epoch: 48/100...  Loss: 0.0150\n",
      "Epoch: 48/100...  Loss: 0.0132\n",
      "Epoch: 48/100...  Loss: 0.0142\n",
      "Epoch: 48/100...  Loss: 0.0136\n",
      "Epoch: 48/100...  Loss: 0.0129\n",
      "Epoch: 49/100...  Loss: 0.0132\n",
      "Epoch: 49/100...  Loss: 0.0132\n",
      "Epoch: 49/100...  Loss: 0.0132\n",
      "Epoch: 49/100...  Loss: 0.0148\n",
      "Epoch: 49/100...  Loss: 0.0131\n",
      "Epoch: 49/100...  Loss: 0.0140\n",
      "Epoch: 49/100...  Loss: 0.0125\n",
      "Epoch: 49/100...  Loss: 0.0134\n",
      "Epoch: 49/100...  Loss: 0.0128\n",
      "Epoch: 49/100...  Loss: 0.0121\n",
      "Epoch: 50/100...  Loss: 0.0124\n",
      "Epoch: 50/100...  Loss: 0.0124\n",
      "Epoch: 50/100...  Loss: 0.0124\n",
      "Epoch: 50/100...  Loss: 0.0139\n",
      "Epoch: 50/100...  Loss: 0.0123\n",
      "Epoch: 50/100...  Loss: 0.0131\n",
      "Epoch: 50/100...  Loss: 0.0118\n",
      "Epoch: 50/100...  Loss: 0.0126\n",
      "Epoch: 50/100...  Loss: 0.0121\n",
      "Epoch: 50/100...  Loss: 0.0114\n",
      "Epoch: 51/100...  Loss: 0.0118\n",
      "Epoch: 51/100...  Loss: 0.0116\n",
      "Epoch: 51/100...  Loss: 0.0117\n",
      "Epoch: 51/100...  Loss: 0.0132\n",
      "Epoch: 51/100...  Loss: 0.0116\n",
      "Epoch: 51/100...  Loss: 0.0124\n",
      "Epoch: 51/100...  Loss: 0.0111\n",
      "Epoch: 51/100...  Loss: 0.0119\n",
      "Epoch: 51/100...  Loss: 0.0114\n",
      "Epoch: 51/100...  Loss: 0.0108\n",
      "Epoch: 52/100...  Loss: 0.0111\n",
      "Epoch: 52/100...  Loss: 0.0110\n",
      "Epoch: 52/100...  Loss: 0.0111\n",
      "Epoch: 52/100...  Loss: 0.0125\n",
      "Epoch: 52/100...  Loss: 0.0110\n",
      "Epoch: 52/100...  Loss: 0.0117\n",
      "Epoch: 52/100...  Loss: 0.0105\n",
      "Epoch: 52/100...  Loss: 0.0113\n",
      "Epoch: 52/100...  Loss: 0.0108\n",
      "Epoch: 52/100...  Loss: 0.0102\n",
      "Epoch: 53/100...  Loss: 0.0105\n",
      "Epoch: 53/100...  Loss: 0.0105\n",
      "Epoch: 53/100...  Loss: 0.0106\n",
      "Epoch: 53/100...  Loss: 0.0119\n",
      "Epoch: 53/100...  Loss: 0.0104\n",
      "Epoch: 53/100...  Loss: 0.0111\n",
      "Epoch: 53/100...  Loss: 0.0100\n",
      "Epoch: 53/100...  Loss: 0.0106\n",
      "Epoch: 53/100...  Loss: 0.0103\n",
      "Epoch: 53/100...  Loss: 0.0097\n",
      "Epoch: 54/100...  Loss: 0.0100\n",
      "Epoch: 54/100...  Loss: 0.0099\n",
      "Epoch: 54/100...  Loss: 0.0100\n",
      "Epoch: 54/100...  Loss: 0.0114\n",
      "Epoch: 54/100...  Loss: 0.0099\n",
      "Epoch: 54/100...  Loss: 0.0104\n",
      "Epoch: 54/100...  Loss: 0.0095\n",
      "Epoch: 54/100...  Loss: 0.0102\n",
      "Epoch: 54/100...  Loss: 0.0097\n",
      "Epoch: 54/100...  Loss: 0.0093\n",
      "Epoch: 55/100...  Loss: 0.0094\n",
      "Epoch: 55/100...  Loss: 0.0095\n",
      "Epoch: 55/100...  Loss: 0.0096\n",
      "Epoch: 55/100...  Loss: 0.0109\n",
      "Epoch: 55/100...  Loss: 0.0095\n",
      "Epoch: 55/100...  Loss: 0.0099\n",
      "Epoch: 55/100...  Loss: 0.0091\n",
      "Epoch: 55/100...  Loss: 0.0097\n",
      "Epoch: 55/100...  Loss: 0.0093\n",
      "Epoch: 55/100...  Loss: 0.0088\n",
      "Epoch: 56/100...  Loss: 0.0090\n",
      "Epoch: 56/100...  Loss: 0.0090\n",
      "Epoch: 56/100...  Loss: 0.0091\n",
      "Epoch: 56/100...  Loss: 0.0103\n",
      "Epoch: 56/100...  Loss: 0.0090\n",
      "Epoch: 56/100...  Loss: 0.0095\n",
      "Epoch: 56/100...  Loss: 0.0087\n",
      "Epoch: 56/100...  Loss: 0.0092\n",
      "Epoch: 56/100...  Loss: 0.0089\n",
      "Epoch: 56/100...  Loss: 0.0084\n",
      "Epoch: 57/100...  Loss: 0.0086\n",
      "Epoch: 57/100...  Loss: 0.0086\n",
      "Epoch: 57/100...  Loss: 0.0087\n",
      "Epoch: 57/100...  Loss: 0.0098\n",
      "Epoch: 57/100...  Loss: 0.0087\n",
      "Epoch: 57/100...  Loss: 0.0090\n",
      "Epoch: 57/100...  Loss: 0.0083\n",
      "Epoch: 57/100...  Loss: 0.0089\n",
      "Epoch: 57/100...  Loss: 0.0085\n",
      "Epoch: 57/100...  Loss: 0.0081\n",
      "Epoch: 58/100...  Loss: 0.0082\n",
      "Epoch: 58/100...  Loss: 0.0082\n",
      "Epoch: 58/100...  Loss: 0.0083\n",
      "Epoch: 58/100...  Loss: 0.0094\n",
      "Epoch: 58/100...  Loss: 0.0083\n",
      "Epoch: 58/100...  Loss: 0.0087\n",
      "Epoch: 58/100...  Loss: 0.0079\n",
      "Epoch: 58/100...  Loss: 0.0085\n",
      "Epoch: 58/100...  Loss: 0.0082\n",
      "Epoch: 58/100...  Loss: 0.0077\n",
      "Epoch: 59/100...  Loss: 0.0079\n",
      "Epoch: 59/100...  Loss: 0.0079\n",
      "Epoch: 59/100...  Loss: 0.0079\n",
      "Epoch: 59/100...  Loss: 0.0090\n",
      "Epoch: 59/100...  Loss: 0.0080\n",
      "Epoch: 59/100...  Loss: 0.0083\n",
      "Epoch: 59/100...  Loss: 0.0076\n",
      "Epoch: 59/100...  Loss: 0.0081\n",
      "Epoch: 59/100...  Loss: 0.0078\n",
      "Epoch: 59/100...  Loss: 0.0074\n",
      "Epoch: 60/100...  Loss: 0.0075\n",
      "Epoch: 60/100...  Loss: 0.0076\n",
      "Epoch: 60/100...  Loss: 0.0076\n",
      "Epoch: 60/100...  Loss: 0.0086\n",
      "Epoch: 60/100...  Loss: 0.0076\n",
      "Epoch: 60/100...  Loss: 0.0080\n",
      "Epoch: 60/100...  Loss: 0.0073\n",
      "Epoch: 60/100...  Loss: 0.0078\n",
      "Epoch: 60/100...  Loss: 0.0075\n",
      "Epoch: 60/100...  Loss: 0.0071\n",
      "Epoch: 61/100...  Loss: 0.0072\n",
      "Epoch: 61/100...  Loss: 0.0073\n",
      "Epoch: 61/100...  Loss: 0.0073\n",
      "Epoch: 61/100...  Loss: 0.0083\n",
      "Epoch: 61/100...  Loss: 0.0074\n",
      "Epoch: 61/100...  Loss: 0.0076\n",
      "Epoch: 61/100...  Loss: 0.0070\n",
      "Epoch: 61/100...  Loss: 0.0075\n",
      "Epoch: 61/100...  Loss: 0.0072\n",
      "Epoch: 61/100...  Loss: 0.0069\n",
      "Epoch: 62/100...  Loss: 0.0070\n",
      "Epoch: 62/100...  Loss: 0.0070\n",
      "Epoch: 62/100...  Loss: 0.0070\n",
      "Epoch: 62/100...  Loss: 0.0079\n",
      "Epoch: 62/100...  Loss: 0.0071\n",
      "Epoch: 62/100...  Loss: 0.0073\n",
      "Epoch: 62/100...  Loss: 0.0067\n",
      "Epoch: 62/100...  Loss: 0.0073\n",
      "Epoch: 62/100...  Loss: 0.0069\n",
      "Epoch: 62/100...  Loss: 0.0066\n",
      "Epoch: 63/100...  Loss: 0.0067\n",
      "Epoch: 63/100...  Loss: 0.0067\n",
      "Epoch: 63/100...  Loss: 0.0067\n",
      "Epoch: 63/100...  Loss: 0.0075\n",
      "Epoch: 63/100...  Loss: 0.0069\n",
      "Epoch: 63/100...  Loss: 0.0071\n",
      "Epoch: 63/100...  Loss: 0.0064\n",
      "Epoch: 63/100...  Loss: 0.0070\n",
      "Epoch: 63/100...  Loss: 0.0067\n",
      "Epoch: 63/100...  Loss: 0.0064\n",
      "Epoch: 64/100...  Loss: 0.0065\n",
      "Epoch: 64/100...  Loss: 0.0065\n",
      "Epoch: 64/100...  Loss: 0.0065\n",
      "Epoch: 64/100...  Loss: 0.0071\n",
      "Epoch: 64/100...  Loss: 0.0066\n",
      "Epoch: 64/100...  Loss: 0.0068\n",
      "Epoch: 64/100...  Loss: 0.0062\n",
      "Epoch: 64/100...  Loss: 0.0068\n",
      "Epoch: 64/100...  Loss: 0.0064\n",
      "Epoch: 64/100...  Loss: 0.0062\n",
      "Epoch: 65/100...  Loss: 0.0062\n",
      "Epoch: 65/100...  Loss: 0.0062\n",
      "Epoch: 65/100...  Loss: 0.0063\n",
      "Epoch: 65/100...  Loss: 0.0068\n",
      "Epoch: 65/100...  Loss: 0.0064\n",
      "Epoch: 65/100...  Loss: 0.0066\n",
      "Epoch: 65/100...  Loss: 0.0060\n",
      "Epoch: 65/100...  Loss: 0.0065\n",
      "Epoch: 65/100...  Loss: 0.0062\n",
      "Epoch: 65/100...  Loss: 0.0059\n",
      "Epoch: 66/100...  Loss: 0.0060\n",
      "Epoch: 66/100...  Loss: 0.0060\n",
      "Epoch: 66/100...  Loss: 0.0061\n",
      "Epoch: 66/100...  Loss: 0.0065\n",
      "Epoch: 66/100...  Loss: 0.0062\n",
      "Epoch: 66/100...  Loss: 0.0064\n",
      "Epoch: 66/100...  Loss: 0.0058\n",
      "Epoch: 66/100...  Loss: 0.0063\n",
      "Epoch: 66/100...  Loss: 0.0060\n",
      "Epoch: 66/100...  Loss: 0.0057\n",
      "Epoch: 67/100...  Loss: 0.0058\n",
      "Epoch: 67/100...  Loss: 0.0058\n",
      "Epoch: 67/100...  Loss: 0.0059\n",
      "Epoch: 67/100...  Loss: 0.0063\n",
      "Epoch: 67/100...  Loss: 0.0060\n",
      "Epoch: 67/100...  Loss: 0.0062\n",
      "Epoch: 67/100...  Loss: 0.0056\n",
      "Epoch: 67/100...  Loss: 0.0061\n",
      "Epoch: 67/100...  Loss: 0.0058\n",
      "Epoch: 67/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0057\n",
      "Epoch: 68/100...  Loss: 0.0060\n",
      "Epoch: 68/100...  Loss: 0.0058\n",
      "Epoch: 68/100...  Loss: 0.0060\n",
      "Epoch: 68/100...  Loss: 0.0054\n",
      "Epoch: 68/100...  Loss: 0.0059\n",
      "Epoch: 68/100...  Loss: 0.0057\n",
      "Epoch: 68/100...  Loss: 0.0054\n",
      "Epoch: 69/100...  Loss: 0.0055\n",
      "Epoch: 69/100...  Loss: 0.0055\n",
      "Epoch: 69/100...  Loss: 0.0055\n",
      "Epoch: 69/100...  Loss: 0.0058\n",
      "Epoch: 69/100...  Loss: 0.0056\n",
      "Epoch: 69/100...  Loss: 0.0058\n",
      "Epoch: 69/100...  Loss: 0.0052\n",
      "Epoch: 69/100...  Loss: 0.0058\n",
      "Epoch: 69/100...  Loss: 0.0055\n",
      "Epoch: 69/100...  Loss: 0.0052\n",
      "Epoch: 70/100...  Loss: 0.0053\n",
      "Epoch: 70/100...  Loss: 0.0053\n",
      "Epoch: 70/100...  Loss: 0.0053\n",
      "Epoch: 70/100...  Loss: 0.0056\n",
      "Epoch: 70/100...  Loss: 0.0054\n",
      "Epoch: 70/100...  Loss: 0.0056\n",
      "Epoch: 70/100...  Loss: 0.0050\n",
      "Epoch: 70/100...  Loss: 0.0056\n",
      "Epoch: 70/100...  Loss: 0.0053\n",
      "Epoch: 70/100...  Loss: 0.0050\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0055\n",
      "Epoch: 71/100...  Loss: 0.0053\n",
      "Epoch: 71/100...  Loss: 0.0054\n",
      "Epoch: 71/100...  Loss: 0.0049\n",
      "Epoch: 71/100...  Loss: 0.0054\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0049\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0053\n",
      "Epoch: 72/100...  Loss: 0.0051\n",
      "Epoch: 72/100...  Loss: 0.0053\n",
      "Epoch: 72/100...  Loss: 0.0047\n",
      "Epoch: 72/100...  Loss: 0.0053\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0051\n",
      "Epoch: 73/100...  Loss: 0.0050\n",
      "Epoch: 73/100...  Loss: 0.0051\n",
      "Epoch: 73/100...  Loss: 0.0046\n",
      "Epoch: 73/100...  Loss: 0.0051\n",
      "Epoch: 73/100...  Loss: 0.0049\n",
      "Epoch: 73/100...  Loss: 0.0046\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0050\n",
      "Epoch: 74/100...  Loss: 0.0048\n",
      "Epoch: 74/100...  Loss: 0.0050\n",
      "Epoch: 74/100...  Loss: 0.0045\n",
      "Epoch: 74/100...  Loss: 0.0050\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0045\n",
      "Epoch: 75/100...  Loss: 0.0046\n",
      "Epoch: 75/100...  Loss: 0.0045\n",
      "Epoch: 75/100...  Loss: 0.0046\n",
      "Epoch: 75/100...  Loss: 0.0048\n",
      "Epoch: 75/100...  Loss: 0.0047\n",
      "Epoch: 75/100...  Loss: 0.0048\n",
      "Epoch: 75/100...  Loss: 0.0043\n",
      "Epoch: 75/100...  Loss: 0.0048\n",
      "Epoch: 75/100...  Loss: 0.0046\n",
      "Epoch: 75/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0045\n",
      "Epoch: 76/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0047\n",
      "Epoch: 76/100...  Loss: 0.0046\n",
      "Epoch: 76/100...  Loss: 0.0047\n",
      "Epoch: 76/100...  Loss: 0.0042\n",
      "Epoch: 76/100...  Loss: 0.0047\n",
      "Epoch: 76/100...  Loss: 0.0045\n",
      "Epoch: 76/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0045\n",
      "Epoch: 77/100...  Loss: 0.0045\n",
      "Epoch: 77/100...  Loss: 0.0046\n",
      "Epoch: 77/100...  Loss: 0.0041\n",
      "Epoch: 77/100...  Loss: 0.0046\n",
      "Epoch: 77/100...  Loss: 0.0044\n",
      "Epoch: 77/100...  Loss: 0.0041\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0044\n",
      "Epoch: 78/100...  Loss: 0.0044\n",
      "Epoch: 78/100...  Loss: 0.0045\n",
      "Epoch: 78/100...  Loss: 0.0040\n",
      "Epoch: 78/100...  Loss: 0.0044\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0040\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0043\n",
      "Epoch: 79/100...  Loss: 0.0042\n",
      "Epoch: 79/100...  Loss: 0.0043\n",
      "Epoch: 79/100...  Loss: 0.0039\n",
      "Epoch: 79/100...  Loss: 0.0043\n",
      "Epoch: 79/100...  Loss: 0.0042\n",
      "Epoch: 79/100...  Loss: 0.0039\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0042\n",
      "Epoch: 80/100...  Loss: 0.0041\n",
      "Epoch: 80/100...  Loss: 0.0042\n",
      "Epoch: 80/100...  Loss: 0.0038\n",
      "Epoch: 80/100...  Loss: 0.0042\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0038\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0041\n",
      "Epoch: 81/100...  Loss: 0.0040\n",
      "Epoch: 81/100...  Loss: 0.0041\n",
      "Epoch: 81/100...  Loss: 0.0037\n",
      "Epoch: 81/100...  Loss: 0.0041\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0037\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0040\n",
      "Epoch: 82/100...  Loss: 0.0039\n",
      "Epoch: 82/100...  Loss: 0.0040\n",
      "Epoch: 82/100...  Loss: 0.0036\n",
      "Epoch: 82/100...  Loss: 0.0040\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0036\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0039\n",
      "Epoch: 83/100...  Loss: 0.0039\n",
      "Epoch: 83/100...  Loss: 0.0039\n",
      "Epoch: 83/100...  Loss: 0.0035\n",
      "Epoch: 83/100...  Loss: 0.0039\n",
      "Epoch: 83/100...  Loss: 0.0038\n",
      "Epoch: 83/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0038\n",
      "Epoch: 84/100...  Loss: 0.0038\n",
      "Epoch: 84/100...  Loss: 0.0039\n",
      "Epoch: 84/100...  Loss: 0.0034\n",
      "Epoch: 84/100...  Loss: 0.0038\n",
      "Epoch: 84/100...  Loss: 0.0037\n",
      "Epoch: 84/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0036\n",
      "Epoch: 85/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0037\n",
      "Epoch: 85/100...  Loss: 0.0037\n",
      "Epoch: 85/100...  Loss: 0.0038\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 85/100...  Loss: 0.0037\n",
      "Epoch: 85/100...  Loss: 0.0036\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0035\n",
      "Epoch: 86/100...  Loss: 0.0035\n",
      "Epoch: 86/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0036\n",
      "Epoch: 86/100...  Loss: 0.0036\n",
      "Epoch: 86/100...  Loss: 0.0037\n",
      "Epoch: 86/100...  Loss: 0.0033\n",
      "Epoch: 86/100...  Loss: 0.0037\n",
      "Epoch: 86/100...  Loss: 0.0035\n",
      "Epoch: 86/100...  Loss: 0.0033\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0035\n",
      "Epoch: 87/100...  Loss: 0.0035\n",
      "Epoch: 87/100...  Loss: 0.0036\n",
      "Epoch: 87/100...  Loss: 0.0032\n",
      "Epoch: 87/100...  Loss: 0.0036\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0032\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0034\n",
      "Epoch: 88/100...  Loss: 0.0034\n",
      "Epoch: 88/100...  Loss: 0.0035\n",
      "Epoch: 88/100...  Loss: 0.0031\n",
      "Epoch: 88/100...  Loss: 0.0035\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0034\n",
      "Epoch: 89/100...  Loss: 0.0034\n",
      "Epoch: 89/100...  Loss: 0.0035\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 89/100...  Loss: 0.0034\n",
      "Epoch: 89/100...  Loss: 0.0033\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0032\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0033\n",
      "Epoch: 90/100...  Loss: 0.0033\n",
      "Epoch: 90/100...  Loss: 0.0034\n",
      "Epoch: 90/100...  Loss: 0.0030\n",
      "Epoch: 90/100...  Loss: 0.0033\n",
      "Epoch: 90/100...  Loss: 0.0032\n",
      "Epoch: 90/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0032\n",
      "Epoch: 91/100...  Loss: 0.0032\n",
      "Epoch: 91/100...  Loss: 0.0033\n",
      "Epoch: 91/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0033\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0031\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0031\n",
      "Epoch: 92/100...  Loss: 0.0032\n",
      "Epoch: 92/100...  Loss: 0.0032\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 92/100...  Loss: 0.0032\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0030\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0031\n",
      "Epoch: 93/100...  Loss: 0.0031\n",
      "Epoch: 93/100...  Loss: 0.0032\n",
      "Epoch: 93/100...  Loss: 0.0028\n",
      "Epoch: 93/100...  Loss: 0.0031\n",
      "Epoch: 93/100...  Loss: 0.0030\n",
      "Epoch: 93/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0030\n",
      "Epoch: 94/100...  Loss: 0.0030\n",
      "Epoch: 94/100...  Loss: 0.0031\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0031\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0029\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0029\n",
      "Epoch: 95/100...  Loss: 0.0030\n",
      "Epoch: 95/100...  Loss: 0.0031\n",
      "Epoch: 95/100...  Loss: 0.0027\n",
      "Epoch: 95/100...  Loss: 0.0030\n",
      "Epoch: 95/100...  Loss: 0.0029\n",
      "Epoch: 95/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0029\n",
      "Epoch: 96/100...  Loss: 0.0029\n",
      "Epoch: 96/100...  Loss: 0.0030\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0030\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0028\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0028\n",
      "Epoch: 97/100...  Loss: 0.0029\n",
      "Epoch: 97/100...  Loss: 0.0029\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 97/100...  Loss: 0.0029\n",
      "Epoch: 97/100...  Loss: 0.0028\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0028\n",
      "Epoch: 98/100...  Loss: 0.0028\n",
      "Epoch: 98/100...  Loss: 0.0029\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0028\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0027\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0027\n",
      "Epoch: 99/100...  Loss: 0.0028\n",
      "Epoch: 99/100...  Loss: 0.0028\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 99/100...  Loss: 0.0028\n",
      "Epoch: 99/100...  Loss: 0.0027\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0027\n",
      "Epoch: 100/100...  Loss: 0.0027\n",
      "Epoch: 100/100...  Loss: 0.0028\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0027\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0025\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def load_data():\n",
    "    print(\"Loading data \\n\")\n",
    "\n",
    "    # Check for already loaded datasets\n",
    "    if not(path.exists('xtrain_doodle.pickle')):\n",
    "        # Load from web\n",
    "        print(\"Loading data from the web \\n\")\n",
    "\n",
    "        # Classes we will load\n",
    "        categories = ['bee', 'cat', 'cow', 'dog', 'duck', 'horse', 'pig', 'rabbit', 'snake', 'whale']\n",
    "\n",
    "        # Dictionary for URL and class labels\n",
    "        URL_DATA = {}\n",
    "        for category in categories:\n",
    "            URL_DATA[category] = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/' + category +'.npy'\n",
    "\n",
    "        # Load data for classes in dictionary\n",
    "        classes_dict = {}\n",
    "        for key, value in URL_DATA.items():\n",
    "            response = requests.get(value)\n",
    "            classes_dict[key] = np.load(BytesIO(response.content))\n",
    "\n",
    "        # Generate labels and add labels to loaded data\n",
    "        for i, (key, value) in enumerate(classes_dict.items()):\n",
    "            value = value.astype('float32')/255.\n",
    "            if i == 0:\n",
    "                classes_dict[key] = np.c_[value, np.zeros(len(value))]\n",
    "            else:\n",
    "                classes_dict[key] = np.c_[value,i*np.ones(len(value))]\n",
    "\n",
    "        # Create a dict with label codes\n",
    "        label_dict = {0:'bee', 1:'cat', 2:'cow', 3:'dog', 4:'duck',\n",
    "                      5:'horse', 6:'pig', 7:'rabbit', 8:'snake', 9:'whale'}\n",
    "\n",
    "        lst = []\n",
    "        for key, value in classes_dict.items():\n",
    "            lst.append(value[:3000])\n",
    "        doodles = np.concatenate(lst)\n",
    "\n",
    "        # Split the data into features and class labels (X & y respectively)\n",
    "        y = doodles[:,-1].astype('float32')\n",
    "        X = doodles[:,:784]\n",
    "\n",
    "        # Split each dataset into train/test splits\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "    else:\n",
    "        # Load data from pickle files\n",
    "        print(\"Loading data from pickle files \\n\")\n",
    "\n",
    "        file = open(\"xtrain_doodle.pickle\",'rb')\n",
    "        X_train = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"xtest_doodle.pickle\",'rb')\n",
    "        X_test = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"ytrain_doodle.pickle\",'rb')\n",
    "        y_train = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"ytest_doodle.pickle\",'rb')\n",
    "        y_test = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, classes_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "categories = ['bee', 'cat', 'cow', 'dog', 'duck', 'horse', 'pig', 'rabbit', 'snake', 'whale']\n",
    "\n",
    "URL_DATA = {}\n",
    "for category in categories:\n",
    "    URL_DATA[category] = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/' + category +'.npy'\n",
    "\n",
    "URL_DATA"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bee': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bee.npy',\n",
       " 'cat': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cat.npy',\n",
       " 'cow': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cow.npy',\n",
       " 'dog': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy',\n",
       " 'duck': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/duck.npy',\n",
       " 'horse': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/horse.npy',\n",
       " 'pig': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pig.npy',\n",
       " 'rabbit': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rabbit.npy',\n",
       " 'snake': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/snake.npy',\n",
       " 'whale': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/whale.npy'}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "classes_dict = {}\n",
    "for key, value in URL_DATA.items():\n",
    "    response = requests.get(value)\n",
    "    classes_dict[key] = np.load(BytesIO(response.content))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "classes_dict"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bee': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'cat': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'cow': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'dog': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'duck': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'horse': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'pig': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'rabbit': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'snake': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'whale': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "len(classes_dict['dog'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "152159"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "len(classes_dict['cat'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "123202"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Generate labels and add labels to loaded data\n",
    "for i, (key, value) in enumerate(classes_dict.items()):\n",
    "    value = value.astype('float32')/255.\n",
    "    if i == 0:\n",
    "        classes_dict[key] = np.c_[value, np.zeros(len(value))]\n",
    "    else:\n",
    "        classes_dict[key] = np.c_[value,i*np.ones(len(value))]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "classes_dict['bee']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "classes_dict['bee'][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.33725491,\n",
       "       0.40000001, 0.40000001, 0.21960784, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00784314,\n",
       "       0.5411765 , 0.99607843, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.79215688, 0.23529412, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.34509805, 1.        , 0.5529412 ,\n",
       "       0.13333334, 0.09019608, 0.07843138, 0.38039216, 0.84313726,\n",
       "       0.96078432, 0.16078432, 0.        , 0.12156863, 0.23529412,\n",
       "       0.34117648, 0.1882353 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.72156864, 0.8509804 , 0.00392157, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.92941177, 0.66274512,\n",
       "       0.7019608 , 1.        , 1.        , 1.        , 0.97647059,\n",
       "       0.30980393, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10588235, 0.98823529, 0.48235294,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00392157, 0.92156863, 0.99215686, 0.9254902 , 0.4509804 ,\n",
       "       0.24313726, 0.16470589, 0.75294119, 0.97647059, 0.21176471,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.46666667, 0.99215686, 0.11764706, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.36078432, 1.        ,\n",
       "       0.80784315, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.10980392, 1.        , 0.39215687, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.76862746, 0.75686276,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01176471, 0.90980393, 1.        , 0.17254902, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 1.        ,\n",
       "       0.39215687, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.9254902 , 0.56078434, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.41176471, 1.        ,\n",
       "       0.80392158, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00784314, 0.69411767, 0.97647059, 0.18039216, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08235294, 1.        ,\n",
       "       0.40392157, 0.        , 0.12156863, 0.33725491, 0.40000001,\n",
       "       0.56862748, 0.98039216, 1.        , 0.46666667, 0.        ,\n",
       "       0.        , 0.        , 0.00784314, 0.56078434, 1.        ,\n",
       "       0.3764706 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.24313726, 1.        , 0.51764709, 0.72156864,\n",
       "       0.97254902, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.98039216, 0.95294118, 0.70980394, 0.29411766,\n",
       "       0.78431374, 1.        , 0.52941179, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.39607844,\n",
       "       1.        , 0.96078432, 1.        , 0.86274511, 0.94509804,\n",
       "       0.7647059 , 1.        , 0.74117649, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99607843, 0.9254902 , 0.32941177,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32549021, 0.96470588, 1.        ,\n",
       "       0.627451  , 0.89019608, 0.64313728, 0.88627452, 1.        ,\n",
       "       0.44313726, 0.28235295, 0.90588236, 1.        , 0.92941177,\n",
       "       0.93333334, 1.        , 0.67450982, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.61960787, 0.96862745, 1.        , 0.34901962, 1.        ,\n",
       "       0.32941177, 0.74901962, 1.        , 0.29019609, 0.12156863,\n",
       "       0.98431373, 1.        , 0.43921569, 0.00392157, 0.56470591,\n",
       "       1.        , 0.04705882, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.9137255 , 0.8509804 ,\n",
       "       0.90980393, 0.49803922, 0.97254902, 0.09803922, 0.98039216,\n",
       "       1.        , 0.07843138, 0.54509807, 0.97254902, 1.        ,\n",
       "       0.47843137, 0.6901961 , 0.67058825, 0.96078432, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.64313728, 0.49019608, 0.36470589, 0.16862746, 0.00784314,\n",
       "       0.05490196, 1.        , 0.93333334, 0.63529414, 0.70588237,\n",
       "       0.79215688, 0.36078432, 1.        , 0.87450981, 0.03921569,\n",
       "       0.92941177, 0.66274512, 1.        , 0.85882354, 0.96862745,\n",
       "       0.88235295, 0.77254903, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.18039216, 0.80392158, 0.9137255 ,\n",
       "       1.        , 1.        , 0.94509804, 0.65882355, 1.        ,\n",
       "       1.        , 0.36862746, 0.89019608, 0.60392159, 0.69411767,\n",
       "       0.98431373, 0.61960787, 0.41960785, 1.        , 0.37254903,\n",
       "       1.        , 0.83529413, 0.76862746, 0.99607843, 0.44705883,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 0.33725491,\n",
       "       0.55686277, 0.59607846, 1.        , 1.        , 0.27058825,\n",
       "       1.        , 0.4509804 , 0.96862745, 1.        , 0.44705883,\n",
       "       0.84313726, 0.74901962, 0.3137255 , 1.        , 0.26666668,\n",
       "       0.52156866, 0.99607843, 0.12156863, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.18431373,\n",
       "       1.        , 1.        , 0.36862746, 1.        , 0.50980395,\n",
       "       1.        , 1.        , 0.52156866, 1.        , 0.3137255 ,\n",
       "       0.41568628, 1.        , 0.07058824, 0.76862746, 0.78431374,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10980392, 0.98039216, 1.        ,\n",
       "       0.49019608, 0.99607843, 0.65098041, 0.9137255 , 1.        ,\n",
       "       0.76078433, 0.87058824, 0.01176471, 0.51372552, 0.96862745,\n",
       "       0.13333334, 0.99215686, 0.4509804 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.65490198, 1.        , 0.8509804 , 0.87058824,\n",
       "       0.85490197, 0.79215688, 1.        , 0.96470588, 0.53333336,\n",
       "       0.        , 0.6156863 , 0.86666667, 0.74509805, 0.93725491,\n",
       "       0.09411765, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.52941179,\n",
       "       0.97254902, 1.        , 0.94117647, 0.99607843, 0.73333335,\n",
       "       0.96862745, 1.        , 0.32549021, 0.        , 0.72941178,\n",
       "       0.90588236, 1.        , 0.34117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.60000002, 0.88235295, 0.98431373,\n",
       "       0.96862745, 1.        , 0.98823529, 0.98431373, 1.        ,\n",
       "       0.89803922, 0.81960785, 0.98431373, 0.99607843, 0.50980395,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.6156863 , 0.78823531, 0.77254903, 0.60784316, 0.97647059,\n",
       "       0.52549022, 0.66274512, 1.        , 0.66274512, 0.66666669,\n",
       "       0.93725491, 0.61176473, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.05882353, 0.09411765,\n",
       "       0.        , 0.10588235, 0.26666668, 0.        , 0.01568628,\n",
       "       0.24705882, 0.        , 0.        , 0.05882353, 0.01568628,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "label_dict = {0:'bee', 1:'cat', 2:'cow', 3:'dog', 4:'duck',\n",
    "              5:'horse', 6:'pig', 7:'rabbit', 8:'snake', 9:'whale'}\n",
    "\n",
    "lst = []\n",
    "for key, value in classes_dict.items():\n",
    "    lst.append(value[:3000])\n",
    "doodles = np.concatenate(lst)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "len(doodles)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "doodles[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.33725491,\n",
       "       0.40000001, 0.40000001, 0.21960784, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00784314,\n",
       "       0.5411765 , 0.99607843, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.79215688, 0.23529412, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.34509805, 1.        , 0.5529412 ,\n",
       "       0.13333334, 0.09019608, 0.07843138, 0.38039216, 0.84313726,\n",
       "       0.96078432, 0.16078432, 0.        , 0.12156863, 0.23529412,\n",
       "       0.34117648, 0.1882353 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.72156864, 0.8509804 , 0.00392157, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.92941177, 0.66274512,\n",
       "       0.7019608 , 1.        , 1.        , 1.        , 0.97647059,\n",
       "       0.30980393, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10588235, 0.98823529, 0.48235294,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00392157, 0.92156863, 0.99215686, 0.9254902 , 0.4509804 ,\n",
       "       0.24313726, 0.16470589, 0.75294119, 0.97647059, 0.21176471,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.46666667, 0.99215686, 0.11764706, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.36078432, 1.        ,\n",
       "       0.80784315, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.10980392, 1.        , 0.39215687, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.76862746, 0.75686276,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01176471, 0.90980393, 1.        , 0.17254902, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 1.        ,\n",
       "       0.39215687, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.9254902 , 0.56078434, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.41176471, 1.        ,\n",
       "       0.80392158, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00784314, 0.69411767, 0.97647059, 0.18039216, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08235294, 1.        ,\n",
       "       0.40392157, 0.        , 0.12156863, 0.33725491, 0.40000001,\n",
       "       0.56862748, 0.98039216, 1.        , 0.46666667, 0.        ,\n",
       "       0.        , 0.        , 0.00784314, 0.56078434, 1.        ,\n",
       "       0.3764706 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.24313726, 1.        , 0.51764709, 0.72156864,\n",
       "       0.97254902, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.98039216, 0.95294118, 0.70980394, 0.29411766,\n",
       "       0.78431374, 1.        , 0.52941179, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.39607844,\n",
       "       1.        , 0.96078432, 1.        , 0.86274511, 0.94509804,\n",
       "       0.7647059 , 1.        , 0.74117649, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99607843, 0.9254902 , 0.32941177,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32549021, 0.96470588, 1.        ,\n",
       "       0.627451  , 0.89019608, 0.64313728, 0.88627452, 1.        ,\n",
       "       0.44313726, 0.28235295, 0.90588236, 1.        , 0.92941177,\n",
       "       0.93333334, 1.        , 0.67450982, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.61960787, 0.96862745, 1.        , 0.34901962, 1.        ,\n",
       "       0.32941177, 0.74901962, 1.        , 0.29019609, 0.12156863,\n",
       "       0.98431373, 1.        , 0.43921569, 0.00392157, 0.56470591,\n",
       "       1.        , 0.04705882, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.9137255 , 0.8509804 ,\n",
       "       0.90980393, 0.49803922, 0.97254902, 0.09803922, 0.98039216,\n",
       "       1.        , 0.07843138, 0.54509807, 0.97254902, 1.        ,\n",
       "       0.47843137, 0.6901961 , 0.67058825, 0.96078432, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.64313728, 0.49019608, 0.36470589, 0.16862746, 0.00784314,\n",
       "       0.05490196, 1.        , 0.93333334, 0.63529414, 0.70588237,\n",
       "       0.79215688, 0.36078432, 1.        , 0.87450981, 0.03921569,\n",
       "       0.92941177, 0.66274512, 1.        , 0.85882354, 0.96862745,\n",
       "       0.88235295, 0.77254903, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.18039216, 0.80392158, 0.9137255 ,\n",
       "       1.        , 1.        , 0.94509804, 0.65882355, 1.        ,\n",
       "       1.        , 0.36862746, 0.89019608, 0.60392159, 0.69411767,\n",
       "       0.98431373, 0.61960787, 0.41960785, 1.        , 0.37254903,\n",
       "       1.        , 0.83529413, 0.76862746, 0.99607843, 0.44705883,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 0.33725491,\n",
       "       0.55686277, 0.59607846, 1.        , 1.        , 0.27058825,\n",
       "       1.        , 0.4509804 , 0.96862745, 1.        , 0.44705883,\n",
       "       0.84313726, 0.74901962, 0.3137255 , 1.        , 0.26666668,\n",
       "       0.52156866, 0.99607843, 0.12156863, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.18431373,\n",
       "       1.        , 1.        , 0.36862746, 1.        , 0.50980395,\n",
       "       1.        , 1.        , 0.52156866, 1.        , 0.3137255 ,\n",
       "       0.41568628, 1.        , 0.07058824, 0.76862746, 0.78431374,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10980392, 0.98039216, 1.        ,\n",
       "       0.49019608, 0.99607843, 0.65098041, 0.9137255 , 1.        ,\n",
       "       0.76078433, 0.87058824, 0.01176471, 0.51372552, 0.96862745,\n",
       "       0.13333334, 0.99215686, 0.4509804 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.65490198, 1.        , 0.8509804 , 0.87058824,\n",
       "       0.85490197, 0.79215688, 1.        , 0.96470588, 0.53333336,\n",
       "       0.        , 0.6156863 , 0.86666667, 0.74509805, 0.93725491,\n",
       "       0.09411765, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.52941179,\n",
       "       0.97254902, 1.        , 0.94117647, 0.99607843, 0.73333335,\n",
       "       0.96862745, 1.        , 0.32549021, 0.        , 0.72941178,\n",
       "       0.90588236, 1.        , 0.34117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.60000002, 0.88235295, 0.98431373,\n",
       "       0.96862745, 1.        , 0.98823529, 0.98431373, 1.        ,\n",
       "       0.89803922, 0.81960785, 0.98431373, 0.99607843, 0.50980395,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.6156863 , 0.78823531, 0.77254903, 0.60784316, 0.97647059,\n",
       "       0.52549022, 0.66274512, 1.        , 0.66274512, 0.66666669,\n",
       "       0.93725491, 0.61176473, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.05882353, 0.09411765,\n",
       "       0.        , 0.10588235, 0.26666668, 0.        , 0.01568628,\n",
       "       0.24705882, 0.        , 0.        , 0.05882353, 0.01568628,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "y = doodles[:,-1].astype('float32')\n",
    "X = doodles[:,:784]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 9., 9., 9.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "X_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "len(X_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21000"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "X_train[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.03921569, 0.33333334, 0.21568628,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.06666667, 0.48627451,\n",
       "       0.91764706, 1.        , 1.        , 0.23921569, 0.        ,\n",
       "       0.        , 0.07450981, 0.44705883, 0.3764706 , 0.06666667,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01568628, 0.20784314,\n",
       "       0.54901963, 0.94901961, 0.99215686, 0.67450982, 0.28235295,\n",
       "       1.        , 0.43921569, 0.        , 0.        , 0.36470589,\n",
       "       1.        , 1.        , 0.9137255 , 0.36862746, 0.00392157,\n",
       "       0.        , 0.        , 0.        , 0.01568628, 0.28235295,\n",
       "       0.59607846, 0.66666669, 0.66666669, 0.66666669, 0.66666669,\n",
       "       0.60000002, 0.95686275, 1.        , 0.97647059, 0.61176473,\n",
       "       0.15294118, 0.        , 0.22745098, 1.        , 0.3137255 ,\n",
       "       0.        , 0.        , 0.33725491, 1.        , 0.30980393,\n",
       "       0.74901962, 1.        , 0.75294119, 0.16078432, 0.        ,\n",
       "       0.36078432, 0.90980393, 1.        , 0.91764706, 0.80000001,\n",
       "       0.80000001, 0.80000001, 0.81176472, 0.98039216, 1.        ,\n",
       "       0.9254902 , 0.51764709, 0.05490196, 0.        , 0.        ,\n",
       "       0.68235296, 0.91764706, 0.04313726, 0.        , 0.        ,\n",
       "       0.28627452, 1.        , 0.21176471, 0.00392157, 0.36470589,\n",
       "       0.9137255 , 0.97647059, 0.73725492, 1.        , 0.74509805,\n",
       "       0.27843139, 0.01568628, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.0627451 , 0.34117648, 0.67843139, 0.99215686,\n",
       "       0.9137255 , 0.17254902, 0.16470589, 0.99215686, 0.47450981,\n",
       "       0.        , 0.        , 0.        , 0.07843138, 0.99215686,\n",
       "       0.45882353, 0.        , 0.        , 0.06666667, 0.72549021,\n",
       "       1.        , 0.4627451 , 0.00392157, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.17254902, 0.87450981, 0.89803922,\n",
       "       0.76862746, 0.93333334, 0.05490196, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.79607844, 0.72941178, 0.        ,\n",
       "       0.        , 0.23921569, 0.98431373, 0.62352943, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14509805, 0.95294118, 1.        , 0.35294119,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.52156866, 0.96862745, 0.11764706, 0.05098039, 0.89411765,\n",
       "       0.82745099, 0.03921569, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.44705883, 1.        , 0.25098041, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.09411765, 0.94117647,\n",
       "       0.68627453, 0.34117648, 1.        , 0.21960784, 0.        ,\n",
       "       0.        , 0.10588235, 0.01568628, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.02352941, 0.8509804 ,\n",
       "       0.81176472, 0.00784314, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.43137255, 1.        , 0.73725492,\n",
       "       0.89803922, 0.00392157, 0.        , 0.37254903, 0.98823529,\n",
       "       0.39215687, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32941177, 1.        , 0.25098041,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01176471, 0.60392159, 0.94117647, 0.60392159, 0.        ,\n",
       "       0.        , 0.68235296, 0.93333334, 0.07058824, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.02745098, 0.94509804, 0.56470591, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15686275,\n",
       "       1.        , 0.33725491, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.79607844,\n",
       "       0.67450982, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.21568628, 1.        , 0.25882354,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.76078433, 0.70980394, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.22352941, 1.        , 0.26274511, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.81960785, 0.6901961 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.99607843,\n",
       "       0.43921569, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.24705882, 0.99607843, 0.42745098,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.86666667, 0.68627453, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.20392157,\n",
       "       0.92941177, 0.81176472, 0.04313726, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.48235294, 0.99607843, 0.31764707, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.19215687, 0.02745098, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.23137255, 0.94117647, 0.84705883, 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01960784, 0.77254903,\n",
       "       0.98823529, 0.52549022, 0.14901961, 0.01960784, 0.07843138,\n",
       "       0.14117648, 0.42352942, 0.83529413, 1.        , 0.36078432,\n",
       "       0.        , 0.        , 0.        , 0.36078432, 0.95294118,\n",
       "       0.82352942, 0.09019608, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.03137255, 0.60784316, 0.99215686,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.75294119, 0.34117648, 0.01176471, 0.        , 0.10980392,\n",
       "       0.70588237, 1.        , 0.74117649, 0.07450981, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.09019608, 0.38431373, 0.45490196,\n",
       "       0.39607844, 0.33333334, 0.16078432, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.88627452, 0.94509804, 0.40000001,\n",
       "       0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.27450982, 0.10980392, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "len(X_train[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "X_train[0][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "X_train[0][1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "len(y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21000"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "def save_data(X_train, y_train, X_test, y_test, force = False):\n",
    "    \n",
    "    # Check for already saved files\n",
    "    if not(path.exists('data/xtrain.pickle')) or force:\n",
    "        # Save X_train dataset as a pickle file\n",
    "        with open('data/xtrain.pickle', 'wb') as f:\n",
    "            pickle.dump(X_train, f)\n",
    "\n",
    "        # Save X_test dataset as a pickle file\n",
    "        with open('data/xtest.pickle', 'wb') as f:\n",
    "            pickle.dump(X_test, f)\n",
    "\n",
    "        # Save y_train dataset as a pickle file\n",
    "        with open('data/ytrain.pickle', 'wb') as f:\n",
    "            pickle.dump(y_train, f)\n",
    "\n",
    "        # Save y_test dataset as a pickle file\n",
    "        with open('data/ytest.pickle', 'wb') as f:\n",
    "            pickle.dump(y_test, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "save_data(X_train, y_train, X_test, y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving data \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "type(X_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "type(X_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "type(y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "len(X_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21000"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "len(X_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def build_model(input_size, output_size, architecture = 'nn', dropout = 0.0):\n",
    "    if (architecture == 'nn'):\n",
    "        # Build a feed-forward network\n",
    "        model = nn.Sequential(OrderedDict([\n",
    "                              ('fc1', nn.Linear(input_size, 128)),\n",
    "                              ('relu1', nn.ReLU()),\n",
    "                              ('fc2', nn.Linear(128, 100)),\n",
    "                              ('bn2', nn.BatchNorm1d(num_features=100)),\n",
    "                              ('relu2', nn.ReLU()),\n",
    "                              ('dropout', nn.Dropout(dropout)),\n",
    "                              ('fc3', nn.Linear(100, 64)),\n",
    "                              ('bn3', nn.BatchNorm1d(num_features=64)),\n",
    "                              ('relu3', nn.ReLU()),\n",
    "                              ('logits', nn.Linear(64, output_size))]))\n",
    "    else:\n",
    "        if (architecture == 'conv'):\n",
    "            # Build a simple convolutional network\n",
    "            model = SimpleCNN(64, 10)\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "rnn_model = build_model(input_size=784, output_size=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "rnn_model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=100, bias=True)\n",
       "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (fc3): Linear(in_features=100, out_features=64, bias=True)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (logits): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "cnn_model = build_model(input_size = 784, output_size=10, architecture='conv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "cnn_model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(1, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3528, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "X_train = pickle.load(open('data/xtrain.pickle', 'rb'))\n",
    "X_test = pickle.load(open('data/xtest.pickle', 'rb'))\n",
    "y_train = pickle.load(open('data/ytrain.pickle', 'rb'))\n",
    "y_test = pickle.load(open('data/ytest.pickle', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "def shuffle(X_train, y_train):\n",
    "\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train, y_train, random_state=42)\n",
    "    y_train_shuffled = y_train_shuffled.reshape((X_train.shape[0], 1))\n",
    "\n",
    "    X_train_shuffled = torch.from_numpy(X_train_shuffled).float()\n",
    "    y_train_shuffled = torch.from_numpy(y_train_shuffled).long()\n",
    "\n",
    "    return X_train_shuffled, y_train_shuffled"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "X_train_shuffled[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1255, 0.6980, 0.6863, 0.6118,\n",
       "        0.5373, 0.3294, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0431, 0.8510, 0.9255, 0.7882,\n",
       "        0.8667, 0.9490, 0.9922, 0.1529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4118, 1.0000, 0.2353,\n",
       "        0.0000, 0.0000, 0.3333, 1.0000, 0.6824, 0.5882, 0.6824, 0.6784, 0.5373,\n",
       "        0.1020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0118, 0.3529, 0.5333, 0.4706, 0.4667, 0.8745, 0.7608,\n",
       "        0.0000, 0.0000, 0.0000, 0.8863, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 0.3569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2118, 0.8549, 1.0000, 0.9608, 1.0000, 1.0000, 0.9020,\n",
       "        0.2588, 0.0000, 0.0275, 0.7098, 0.6588, 0.7804, 0.8353, 0.8706, 0.9490,\n",
       "        1.0000, 0.8863, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2353, 0.9412, 0.8431, 0.2824, 0.0000, 0.0000, 0.0039,\n",
       "        0.0000, 0.0000, 0.0000, 0.3098, 1.0000, 0.9804, 0.8980, 0.8235, 0.7529,\n",
       "        0.6471, 0.2941, 0.0078, 0.0000, 0.0000, 0.0000, 0.1961, 0.9137, 0.9137,\n",
       "        0.8667, 0.8196, 0.8157, 0.9961, 0.8353, 0.0902, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.3765, 1.0000, 0.1294, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3961, 1.0000,\n",
       "        1.0000, 0.9255, 0.7059, 0.6745, 0.4706, 0.0471, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0902, 0.3725, 0.8235, 0.8980, 0.0039, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157,\n",
       "        0.5098, 0.8824, 1.0000, 0.9529, 0.3843, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0196, 0.2471, 0.7137, 0.8588, 0.9961, 1.0000, 0.8784, 0.2510, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0039, 0.2078, 0.7098, 1.0000, 0.6980, 0.4824, 0.6157,\n",
       "        0.8039, 0.9765, 1.0000, 1.0000, 1.0000, 0.8431, 0.7725, 0.8863, 0.8706,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.4392, 0.9098, 1.0000,\n",
       "        0.8784, 0.6902, 0.5020, 0.5373, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        0.8745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0314, 0.5961, 0.6667, 0.7216, 0.7333,\n",
       "        0.6627, 0.2549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000])"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "X_train_shuffled, y_train_shuffled = shuffle(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "type(X_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "type(y_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "y_train_numpy = y_train_shuffled.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "type(y_train_numpy)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "def fit_model(model, X_train, y_train, epochs = 100, n_chunks = 1000, learning_rate = 0.003, weight_decay = 0, optimizer = 'SGD'):\n",
    "\n",
    "    print(\"Fitting model with epochs = {epochs}, learning rate = {lr}\\n\"\\\n",
    "    .format(epochs = epochs, lr = learning_rate))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if (optimizer == 'SGD'):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "    print_every = 100\n",
    "    steps = 0\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        images = torch.chunk(X_train, n_chunks)\n",
    "        labels = torch.chunk(y_train, n_chunks)\n",
    "        for i in range(n_chunks):\n",
    "            steps += 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward passes\n",
    "            output = model.forward(images[i])\n",
    "            loss = criterion(output, labels[i].squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if steps % print_every == 0:\n",
    "                print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                      \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "                running_loss = 0\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "fit_model(rnn_model, X_train_shuffled, y_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting model with epochs = 100, learning rate = 0.003\n",
      "\n",
      "Epoch: 1/100...  Loss: 2.2037\n",
      "Epoch: 1/100...  Loss: 2.0504\n",
      "Epoch: 1/100...  Loss: 1.9417\n",
      "Epoch: 1/100...  Loss: 1.8689\n",
      "Epoch: 1/100...  Loss: 1.8064\n",
      "Epoch: 1/100...  Loss: 1.7327\n",
      "Epoch: 1/100...  Loss: 1.6904\n",
      "Epoch: 1/100...  Loss: 1.7019\n",
      "Epoch: 1/100...  Loss: 1.6900\n",
      "Epoch: 1/100...  Loss: 1.6214\n",
      "Epoch: 2/100...  Loss: 1.5917\n",
      "Epoch: 2/100...  Loss: 1.5950\n",
      "Epoch: 2/100...  Loss: 1.5655\n",
      "Epoch: 2/100...  Loss: 1.5290\n",
      "Epoch: 2/100...  Loss: 1.5380\n",
      "Epoch: 2/100...  Loss: 1.4726\n",
      "Epoch: 2/100...  Loss: 1.4581\n",
      "Epoch: 2/100...  Loss: 1.5071\n",
      "Epoch: 2/100...  Loss: 1.5234\n",
      "Epoch: 2/100...  Loss: 1.4467\n",
      "Epoch: 3/100...  Loss: 1.4421\n",
      "Epoch: 3/100...  Loss: 1.4452\n",
      "Epoch: 3/100...  Loss: 1.4261\n",
      "Epoch: 3/100...  Loss: 1.3853\n",
      "Epoch: 3/100...  Loss: 1.4136\n",
      "Epoch: 3/100...  Loss: 1.3427\n",
      "Epoch: 3/100...  Loss: 1.3311\n",
      "Epoch: 3/100...  Loss: 1.3868\n",
      "Epoch: 3/100...  Loss: 1.4151\n",
      "Epoch: 3/100...  Loss: 1.3340\n",
      "Epoch: 4/100...  Loss: 1.3405\n",
      "Epoch: 4/100...  Loss: 1.3395\n",
      "Epoch: 4/100...  Loss: 1.3229\n",
      "Epoch: 4/100...  Loss: 1.2806\n",
      "Epoch: 4/100...  Loss: 1.3188\n",
      "Epoch: 4/100...  Loss: 1.2470\n",
      "Epoch: 4/100...  Loss: 1.2386\n",
      "Epoch: 4/100...  Loss: 1.2892\n",
      "Epoch: 4/100...  Loss: 1.3260\n",
      "Epoch: 4/100...  Loss: 1.2440\n",
      "Epoch: 5/100...  Loss: 1.2563\n",
      "Epoch: 5/100...  Loss: 1.2529\n",
      "Epoch: 5/100...  Loss: 1.2386\n",
      "Epoch: 5/100...  Loss: 1.1970\n",
      "Epoch: 5/100...  Loss: 1.2418\n",
      "Epoch: 5/100...  Loss: 1.1658\n",
      "Epoch: 5/100...  Loss: 1.1628\n",
      "Epoch: 5/100...  Loss: 1.2064\n",
      "Epoch: 5/100...  Loss: 1.2498\n",
      "Epoch: 5/100...  Loss: 1.1667\n",
      "Epoch: 6/100...  Loss: 1.1825\n",
      "Epoch: 6/100...  Loss: 1.1749\n",
      "Epoch: 6/100...  Loss: 1.1647\n",
      "Epoch: 6/100...  Loss: 1.1225\n",
      "Epoch: 6/100...  Loss: 1.1712\n",
      "Epoch: 6/100...  Loss: 1.0955\n",
      "Epoch: 6/100...  Loss: 1.0921\n",
      "Epoch: 6/100...  Loss: 1.1319\n",
      "Epoch: 6/100...  Loss: 1.1789\n",
      "Epoch: 6/100...  Loss: 1.0950\n",
      "Epoch: 7/100...  Loss: 1.1110\n",
      "Epoch: 7/100...  Loss: 1.1074\n",
      "Epoch: 7/100...  Loss: 1.0973\n",
      "Epoch: 7/100...  Loss: 1.0536\n",
      "Epoch: 7/100...  Loss: 1.1049\n",
      "Epoch: 7/100...  Loss: 1.0311\n",
      "Epoch: 7/100...  Loss: 1.0284\n",
      "Epoch: 7/100...  Loss: 1.0627\n",
      "Epoch: 7/100...  Loss: 1.1086\n",
      "Epoch: 7/100...  Loss: 1.0265\n",
      "Epoch: 8/100...  Loss: 1.0443\n",
      "Epoch: 8/100...  Loss: 1.0426\n",
      "Epoch: 8/100...  Loss: 1.0316\n",
      "Epoch: 8/100...  Loss: 0.9901\n",
      "Epoch: 8/100...  Loss: 1.0374\n",
      "Epoch: 8/100...  Loss: 0.9746\n",
      "Epoch: 8/100...  Loss: 0.9653\n",
      "Epoch: 8/100...  Loss: 1.0006\n",
      "Epoch: 8/100...  Loss: 1.0436\n",
      "Epoch: 8/100...  Loss: 0.9637\n",
      "Epoch: 9/100...  Loss: 0.9794\n",
      "Epoch: 9/100...  Loss: 0.9807\n",
      "Epoch: 9/100...  Loss: 0.9724\n",
      "Epoch: 9/100...  Loss: 0.9240\n",
      "Epoch: 9/100...  Loss: 0.9774\n",
      "Epoch: 9/100...  Loss: 0.9222\n",
      "Epoch: 9/100...  Loss: 0.9061\n",
      "Epoch: 9/100...  Loss: 0.9422\n",
      "Epoch: 9/100...  Loss: 0.9766\n",
      "Epoch: 9/100...  Loss: 0.9070\n",
      "Epoch: 10/100...  Loss: 0.9142\n",
      "Epoch: 10/100...  Loss: 0.9178\n",
      "Epoch: 10/100...  Loss: 0.9135\n",
      "Epoch: 10/100...  Loss: 0.8622\n",
      "Epoch: 10/100...  Loss: 0.9140\n",
      "Epoch: 10/100...  Loss: 0.8659\n",
      "Epoch: 10/100...  Loss: 0.8498\n",
      "Epoch: 10/100...  Loss: 0.8867\n",
      "Epoch: 10/100...  Loss: 0.9131\n",
      "Epoch: 10/100...  Loss: 0.8448\n",
      "Epoch: 11/100...  Loss: 0.8528\n",
      "Epoch: 11/100...  Loss: 0.8584\n",
      "Epoch: 11/100...  Loss: 0.8543\n",
      "Epoch: 11/100...  Loss: 0.8054\n",
      "Epoch: 11/100...  Loss: 0.8539\n",
      "Epoch: 11/100...  Loss: 0.8116\n",
      "Epoch: 11/100...  Loss: 0.7970\n",
      "Epoch: 11/100...  Loss: 0.8312\n",
      "Epoch: 11/100...  Loss: 0.8514\n",
      "Epoch: 11/100...  Loss: 0.7906\n",
      "Epoch: 12/100...  Loss: 0.7922\n",
      "Epoch: 12/100...  Loss: 0.7968\n",
      "Epoch: 12/100...  Loss: 0.8002\n",
      "Epoch: 12/100...  Loss: 0.7492\n",
      "Epoch: 12/100...  Loss: 0.7960\n",
      "Epoch: 12/100...  Loss: 0.7593\n",
      "Epoch: 12/100...  Loss: 0.7409\n",
      "Epoch: 12/100...  Loss: 0.7765\n",
      "Epoch: 12/100...  Loss: 0.7937\n",
      "Epoch: 12/100...  Loss: 0.7399\n",
      "Epoch: 13/100...  Loss: 0.7317\n",
      "Epoch: 13/100...  Loss: 0.7392\n",
      "Epoch: 13/100...  Loss: 0.7480\n",
      "Epoch: 13/100...  Loss: 0.7009\n",
      "Epoch: 13/100...  Loss: 0.7377\n",
      "Epoch: 13/100...  Loss: 0.7062\n",
      "Epoch: 13/100...  Loss: 0.6879\n",
      "Epoch: 13/100...  Loss: 0.7198\n",
      "Epoch: 13/100...  Loss: 0.7361\n",
      "Epoch: 13/100...  Loss: 0.6875\n",
      "Epoch: 14/100...  Loss: 0.6748\n",
      "Epoch: 14/100...  Loss: 0.6822\n",
      "Epoch: 14/100...  Loss: 0.6958\n",
      "Epoch: 14/100...  Loss: 0.6558\n",
      "Epoch: 14/100...  Loss: 0.6833\n",
      "Epoch: 14/100...  Loss: 0.6545\n",
      "Epoch: 14/100...  Loss: 0.6326\n",
      "Epoch: 14/100...  Loss: 0.6701\n",
      "Epoch: 14/100...  Loss: 0.6841\n",
      "Epoch: 14/100...  Loss: 0.6346\n",
      "Epoch: 15/100...  Loss: 0.6256\n",
      "Epoch: 15/100...  Loss: 0.6295\n",
      "Epoch: 15/100...  Loss: 0.6423\n",
      "Epoch: 15/100...  Loss: 0.6114\n",
      "Epoch: 15/100...  Loss: 0.6311\n",
      "Epoch: 15/100...  Loss: 0.6044\n",
      "Epoch: 15/100...  Loss: 0.5828\n",
      "Epoch: 15/100...  Loss: 0.6183\n",
      "Epoch: 15/100...  Loss: 0.6390\n",
      "Epoch: 15/100...  Loss: 0.5861\n",
      "Epoch: 16/100...  Loss: 0.5765\n",
      "Epoch: 16/100...  Loss: 0.5761\n",
      "Epoch: 16/100...  Loss: 0.5949\n",
      "Epoch: 16/100...  Loss: 0.5682\n",
      "Epoch: 16/100...  Loss: 0.5802\n",
      "Epoch: 16/100...  Loss: 0.5564\n",
      "Epoch: 16/100...  Loss: 0.5389\n",
      "Epoch: 16/100...  Loss: 0.5703\n",
      "Epoch: 16/100...  Loss: 0.5888\n",
      "Epoch: 16/100...  Loss: 0.5404\n",
      "Epoch: 17/100...  Loss: 0.5290\n",
      "Epoch: 17/100...  Loss: 0.5287\n",
      "Epoch: 17/100...  Loss: 0.5443\n",
      "Epoch: 17/100...  Loss: 0.5252\n",
      "Epoch: 17/100...  Loss: 0.5346\n",
      "Epoch: 17/100...  Loss: 0.5115\n",
      "Epoch: 17/100...  Loss: 0.4978\n",
      "Epoch: 17/100...  Loss: 0.5210\n",
      "Epoch: 17/100...  Loss: 0.5378\n",
      "Epoch: 17/100...  Loss: 0.4965\n",
      "Epoch: 18/100...  Loss: 0.4833\n",
      "Epoch: 18/100...  Loss: 0.4825\n",
      "Epoch: 18/100...  Loss: 0.4992\n",
      "Epoch: 18/100...  Loss: 0.4894\n",
      "Epoch: 18/100...  Loss: 0.4913\n",
      "Epoch: 18/100...  Loss: 0.4667\n",
      "Epoch: 18/100...  Loss: 0.4594\n",
      "Epoch: 18/100...  Loss: 0.4758\n",
      "Epoch: 18/100...  Loss: 0.4912\n",
      "Epoch: 18/100...  Loss: 0.4516\n",
      "Epoch: 19/100...  Loss: 0.4391\n",
      "Epoch: 19/100...  Loss: 0.4386\n",
      "Epoch: 19/100...  Loss: 0.4539\n",
      "Epoch: 19/100...  Loss: 0.4542\n",
      "Epoch: 19/100...  Loss: 0.4455\n",
      "Epoch: 19/100...  Loss: 0.4195\n",
      "Epoch: 19/100...  Loss: 0.4241\n",
      "Epoch: 19/100...  Loss: 0.4335\n",
      "Epoch: 19/100...  Loss: 0.4463\n",
      "Epoch: 19/100...  Loss: 0.4130\n",
      "Epoch: 20/100...  Loss: 0.3980\n",
      "Epoch: 20/100...  Loss: 0.3969\n",
      "Epoch: 20/100...  Loss: 0.4090\n",
      "Epoch: 20/100...  Loss: 0.4185\n",
      "Epoch: 20/100...  Loss: 0.4032\n",
      "Epoch: 20/100...  Loss: 0.3785\n",
      "Epoch: 20/100...  Loss: 0.3883\n",
      "Epoch: 20/100...  Loss: 0.3902\n",
      "Epoch: 20/100...  Loss: 0.4061\n",
      "Epoch: 20/100...  Loss: 0.3706\n",
      "Epoch: 21/100...  Loss: 0.3637\n",
      "Epoch: 21/100...  Loss: 0.3531\n",
      "Epoch: 21/100...  Loss: 0.3708\n",
      "Epoch: 21/100...  Loss: 0.3775\n",
      "Epoch: 21/100...  Loss: 0.3634\n",
      "Epoch: 21/100...  Loss: 0.3460\n",
      "Epoch: 21/100...  Loss: 0.3513\n",
      "Epoch: 21/100...  Loss: 0.3542\n",
      "Epoch: 21/100...  Loss: 0.3716\n",
      "Epoch: 21/100...  Loss: 0.3340\n",
      "Epoch: 22/100...  Loss: 0.3293\n",
      "Epoch: 22/100...  Loss: 0.3168\n",
      "Epoch: 22/100...  Loss: 0.3395\n",
      "Epoch: 22/100...  Loss: 0.3399\n",
      "Epoch: 22/100...  Loss: 0.3284\n",
      "Epoch: 22/100...  Loss: 0.3126\n",
      "Epoch: 22/100...  Loss: 0.3141\n",
      "Epoch: 22/100...  Loss: 0.3255\n",
      "Epoch: 22/100...  Loss: 0.3328\n",
      "Epoch: 22/100...  Loss: 0.3037\n",
      "Epoch: 23/100...  Loss: 0.2913\n",
      "Epoch: 23/100...  Loss: 0.2838\n",
      "Epoch: 23/100...  Loss: 0.3039\n",
      "Epoch: 23/100...  Loss: 0.3045\n",
      "Epoch: 23/100...  Loss: 0.2939\n",
      "Epoch: 23/100...  Loss: 0.2731\n",
      "Epoch: 23/100...  Loss: 0.2883\n",
      "Epoch: 23/100...  Loss: 0.2938\n",
      "Epoch: 23/100...  Loss: 0.3054\n",
      "Epoch: 23/100...  Loss: 0.2721\n",
      "Epoch: 24/100...  Loss: 0.2606\n",
      "Epoch: 24/100...  Loss: 0.2529\n",
      "Epoch: 24/100...  Loss: 0.2722\n",
      "Epoch: 24/100...  Loss: 0.2747\n",
      "Epoch: 24/100...  Loss: 0.2615\n",
      "Epoch: 24/100...  Loss: 0.2406\n",
      "Epoch: 24/100...  Loss: 0.2568\n",
      "Epoch: 24/100...  Loss: 0.2586\n",
      "Epoch: 24/100...  Loss: 0.2678\n",
      "Epoch: 24/100...  Loss: 0.2431\n",
      "Epoch: 25/100...  Loss: 0.2332\n",
      "Epoch: 25/100...  Loss: 0.2246\n",
      "Epoch: 25/100...  Loss: 0.2407\n",
      "Epoch: 25/100...  Loss: 0.2453\n",
      "Epoch: 25/100...  Loss: 0.2304\n",
      "Epoch: 25/100...  Loss: 0.2168\n",
      "Epoch: 25/100...  Loss: 0.2273\n",
      "Epoch: 25/100...  Loss: 0.2291\n",
      "Epoch: 25/100...  Loss: 0.2383\n",
      "Epoch: 25/100...  Loss: 0.2167\n",
      "Epoch: 26/100...  Loss: 0.2036\n",
      "Epoch: 26/100...  Loss: 0.1945\n",
      "Epoch: 26/100...  Loss: 0.2112\n",
      "Epoch: 26/100...  Loss: 0.2140\n",
      "Epoch: 26/100...  Loss: 0.2079\n",
      "Epoch: 26/100...  Loss: 0.1892\n",
      "Epoch: 26/100...  Loss: 0.2014\n",
      "Epoch: 26/100...  Loss: 0.2006\n",
      "Epoch: 26/100...  Loss: 0.2116\n",
      "Epoch: 26/100...  Loss: 0.1839\n",
      "Epoch: 27/100...  Loss: 0.1761\n",
      "Epoch: 27/100...  Loss: 0.1714\n",
      "Epoch: 27/100...  Loss: 0.1884\n",
      "Epoch: 27/100...  Loss: 0.1923\n",
      "Epoch: 27/100...  Loss: 0.1829\n",
      "Epoch: 27/100...  Loss: 0.1716\n",
      "Epoch: 27/100...  Loss: 0.1760\n",
      "Epoch: 27/100...  Loss: 0.1769\n",
      "Epoch: 27/100...  Loss: 0.1907\n",
      "Epoch: 27/100...  Loss: 0.1655\n",
      "Epoch: 28/100...  Loss: 0.1506\n",
      "Epoch: 28/100...  Loss: 0.1475\n",
      "Epoch: 28/100...  Loss: 0.1601\n",
      "Epoch: 28/100...  Loss: 0.1657\n",
      "Epoch: 28/100...  Loss: 0.1655\n",
      "Epoch: 28/100...  Loss: 0.1496\n",
      "Epoch: 28/100...  Loss: 0.1565\n",
      "Epoch: 28/100...  Loss: 0.1533\n",
      "Epoch: 28/100...  Loss: 0.1659\n",
      "Epoch: 28/100...  Loss: 0.1473\n",
      "Epoch: 29/100...  Loss: 0.1296\n",
      "Epoch: 29/100...  Loss: 0.1296\n",
      "Epoch: 29/100...  Loss: 0.1433\n",
      "Epoch: 29/100...  Loss: 0.1465\n",
      "Epoch: 29/100...  Loss: 0.1398\n",
      "Epoch: 29/100...  Loss: 0.1333\n",
      "Epoch: 29/100...  Loss: 0.1383\n",
      "Epoch: 29/100...  Loss: 0.1354\n",
      "Epoch: 29/100...  Loss: 0.1453\n",
      "Epoch: 29/100...  Loss: 0.1250\n",
      "Epoch: 30/100...  Loss: 0.1138\n",
      "Epoch: 30/100...  Loss: 0.1122\n",
      "Epoch: 30/100...  Loss: 0.1226\n",
      "Epoch: 30/100...  Loss: 0.1217\n",
      "Epoch: 30/100...  Loss: 0.1222\n",
      "Epoch: 30/100...  Loss: 0.1125\n",
      "Epoch: 30/100...  Loss: 0.1187\n",
      "Epoch: 30/100...  Loss: 0.1149\n",
      "Epoch: 30/100...  Loss: 0.1248\n",
      "Epoch: 30/100...  Loss: 0.1065\n",
      "Epoch: 31/100...  Loss: 0.0980\n",
      "Epoch: 31/100...  Loss: 0.0973\n",
      "Epoch: 31/100...  Loss: 0.1032\n",
      "Epoch: 31/100...  Loss: 0.1045\n",
      "Epoch: 31/100...  Loss: 0.1007\n",
      "Epoch: 31/100...  Loss: 0.0976\n",
      "Epoch: 31/100...  Loss: 0.0976\n",
      "Epoch: 31/100...  Loss: 0.0954\n",
      "Epoch: 31/100...  Loss: 0.1081\n",
      "Epoch: 31/100...  Loss: 0.0913\n",
      "Epoch: 32/100...  Loss: 0.0830\n",
      "Epoch: 32/100...  Loss: 0.0823\n",
      "Epoch: 32/100...  Loss: 0.0890\n",
      "Epoch: 32/100...  Loss: 0.0885\n",
      "Epoch: 32/100...  Loss: 0.0878\n",
      "Epoch: 32/100...  Loss: 0.0802\n",
      "Epoch: 32/100...  Loss: 0.0847\n",
      "Epoch: 32/100...  Loss: 0.0814\n",
      "Epoch: 32/100...  Loss: 0.0910\n",
      "Epoch: 32/100...  Loss: 0.0766\n",
      "Epoch: 33/100...  Loss: 0.0705\n",
      "Epoch: 33/100...  Loss: 0.0687\n",
      "Epoch: 33/100...  Loss: 0.0739\n",
      "Epoch: 33/100...  Loss: 0.0730\n",
      "Epoch: 33/100...  Loss: 0.0719\n",
      "Epoch: 33/100...  Loss: 0.0684\n",
      "Epoch: 33/100...  Loss: 0.0712\n",
      "Epoch: 33/100...  Loss: 0.0689\n",
      "Epoch: 33/100...  Loss: 0.0751\n",
      "Epoch: 33/100...  Loss: 0.0648\n",
      "Epoch: 34/100...  Loss: 0.0582\n",
      "Epoch: 34/100...  Loss: 0.0585\n",
      "Epoch: 34/100...  Loss: 0.0636\n",
      "Epoch: 34/100...  Loss: 0.0626\n",
      "Epoch: 34/100...  Loss: 0.0611\n",
      "Epoch: 34/100...  Loss: 0.0577\n",
      "Epoch: 34/100...  Loss: 0.0602\n",
      "Epoch: 34/100...  Loss: 0.0562\n",
      "Epoch: 34/100...  Loss: 0.0636\n",
      "Epoch: 34/100...  Loss: 0.0544\n",
      "Epoch: 35/100...  Loss: 0.0491\n",
      "Epoch: 35/100...  Loss: 0.0487\n",
      "Epoch: 35/100...  Loss: 0.0526\n",
      "Epoch: 35/100...  Loss: 0.0526\n",
      "Epoch: 35/100...  Loss: 0.0510\n",
      "Epoch: 35/100...  Loss: 0.0492\n",
      "Epoch: 35/100...  Loss: 0.0517\n",
      "Epoch: 35/100...  Loss: 0.0485\n",
      "Epoch: 35/100...  Loss: 0.0544\n",
      "Epoch: 35/100...  Loss: 0.0461\n",
      "Epoch: 36/100...  Loss: 0.0425\n",
      "Epoch: 36/100...  Loss: 0.0412\n",
      "Epoch: 36/100...  Loss: 0.0459\n",
      "Epoch: 36/100...  Loss: 0.0450\n",
      "Epoch: 36/100...  Loss: 0.0434\n",
      "Epoch: 36/100...  Loss: 0.0420\n",
      "Epoch: 36/100...  Loss: 0.0444\n",
      "Epoch: 36/100...  Loss: 0.0421\n",
      "Epoch: 36/100...  Loss: 0.0470\n",
      "Epoch: 36/100...  Loss: 0.0398\n",
      "Epoch: 37/100...  Loss: 0.0371\n",
      "Epoch: 37/100...  Loss: 0.0353\n",
      "Epoch: 37/100...  Loss: 0.0399\n",
      "Epoch: 37/100...  Loss: 0.0380\n",
      "Epoch: 37/100...  Loss: 0.0378\n",
      "Epoch: 37/100...  Loss: 0.0358\n",
      "Epoch: 37/100...  Loss: 0.0386\n",
      "Epoch: 37/100...  Loss: 0.0368\n",
      "Epoch: 37/100...  Loss: 0.0415\n",
      "Epoch: 37/100...  Loss: 0.0351\n",
      "Epoch: 38/100...  Loss: 0.0322\n",
      "Epoch: 38/100...  Loss: 0.0313\n",
      "Epoch: 38/100...  Loss: 0.0351\n",
      "Epoch: 38/100...  Loss: 0.0334\n",
      "Epoch: 38/100...  Loss: 0.0336\n",
      "Epoch: 38/100...  Loss: 0.0321\n",
      "Epoch: 38/100...  Loss: 0.0345\n",
      "Epoch: 38/100...  Loss: 0.0327\n",
      "Epoch: 38/100...  Loss: 0.0365\n",
      "Epoch: 38/100...  Loss: 0.0310\n",
      "Epoch: 39/100...  Loss: 0.0285\n",
      "Epoch: 39/100...  Loss: 0.0271\n",
      "Epoch: 39/100...  Loss: 0.0311\n",
      "Epoch: 39/100...  Loss: 0.0291\n",
      "Epoch: 39/100...  Loss: 0.0295\n",
      "Epoch: 39/100...  Loss: 0.0286\n",
      "Epoch: 39/100...  Loss: 0.0303\n",
      "Epoch: 39/100...  Loss: 0.0290\n",
      "Epoch: 39/100...  Loss: 0.0323\n",
      "Epoch: 39/100...  Loss: 0.0278\n",
      "Epoch: 40/100...  Loss: 0.0258\n",
      "Epoch: 40/100...  Loss: 0.0243\n",
      "Epoch: 40/100...  Loss: 0.0281\n",
      "Epoch: 40/100...  Loss: 0.0262\n",
      "Epoch: 40/100...  Loss: 0.0263\n",
      "Epoch: 40/100...  Loss: 0.0260\n",
      "Epoch: 40/100...  Loss: 0.0272\n",
      "Epoch: 40/100...  Loss: 0.0261\n",
      "Epoch: 40/100...  Loss: 0.0291\n",
      "Epoch: 40/100...  Loss: 0.0251\n",
      "Epoch: 41/100...  Loss: 0.0231\n",
      "Epoch: 41/100...  Loss: 0.0219\n",
      "Epoch: 41/100...  Loss: 0.0252\n",
      "Epoch: 41/100...  Loss: 0.0238\n",
      "Epoch: 41/100...  Loss: 0.0236\n",
      "Epoch: 41/100...  Loss: 0.0238\n",
      "Epoch: 41/100...  Loss: 0.0245\n",
      "Epoch: 41/100...  Loss: 0.0237\n",
      "Epoch: 41/100...  Loss: 0.0264\n",
      "Epoch: 41/100...  Loss: 0.0228\n",
      "Epoch: 42/100...  Loss: 0.0211\n",
      "Epoch: 42/100...  Loss: 0.0199\n",
      "Epoch: 42/100...  Loss: 0.0229\n",
      "Epoch: 42/100...  Loss: 0.0216\n",
      "Epoch: 42/100...  Loss: 0.0215\n",
      "Epoch: 42/100...  Loss: 0.0217\n",
      "Epoch: 42/100...  Loss: 0.0222\n",
      "Epoch: 42/100...  Loss: 0.0213\n",
      "Epoch: 42/100...  Loss: 0.0240\n",
      "Epoch: 42/100...  Loss: 0.0210\n",
      "Epoch: 43/100...  Loss: 0.0194\n",
      "Epoch: 43/100...  Loss: 0.0182\n",
      "Epoch: 43/100...  Loss: 0.0210\n",
      "Epoch: 43/100...  Loss: 0.0198\n",
      "Epoch: 43/100...  Loss: 0.0197\n",
      "Epoch: 43/100...  Loss: 0.0200\n",
      "Epoch: 43/100...  Loss: 0.0204\n",
      "Epoch: 43/100...  Loss: 0.0197\n",
      "Epoch: 43/100...  Loss: 0.0220\n",
      "Epoch: 43/100...  Loss: 0.0193\n",
      "Epoch: 44/100...  Loss: 0.0180\n",
      "Epoch: 44/100...  Loss: 0.0168\n",
      "Epoch: 44/100...  Loss: 0.0194\n",
      "Epoch: 44/100...  Loss: 0.0183\n",
      "Epoch: 44/100...  Loss: 0.0181\n",
      "Epoch: 44/100...  Loss: 0.0184\n",
      "Epoch: 44/100...  Loss: 0.0189\n",
      "Epoch: 44/100...  Loss: 0.0182\n",
      "Epoch: 44/100...  Loss: 0.0203\n",
      "Epoch: 44/100...  Loss: 0.0179\n",
      "Epoch: 45/100...  Loss: 0.0166\n",
      "Epoch: 45/100...  Loss: 0.0156\n",
      "Epoch: 45/100...  Loss: 0.0179\n",
      "Epoch: 45/100...  Loss: 0.0170\n",
      "Epoch: 45/100...  Loss: 0.0168\n",
      "Epoch: 45/100...  Loss: 0.0171\n",
      "Epoch: 45/100...  Loss: 0.0172\n",
      "Epoch: 45/100...  Loss: 0.0169\n",
      "Epoch: 45/100...  Loss: 0.0187\n",
      "Epoch: 45/100...  Loss: 0.0168\n",
      "Epoch: 46/100...  Loss: 0.0154\n",
      "Epoch: 46/100...  Loss: 0.0145\n",
      "Epoch: 46/100...  Loss: 0.0166\n",
      "Epoch: 46/100...  Loss: 0.0159\n",
      "Epoch: 46/100...  Loss: 0.0157\n",
      "Epoch: 46/100...  Loss: 0.0160\n",
      "Epoch: 46/100...  Loss: 0.0159\n",
      "Epoch: 46/100...  Loss: 0.0158\n",
      "Epoch: 46/100...  Loss: 0.0173\n",
      "Epoch: 46/100...  Loss: 0.0155\n",
      "Epoch: 47/100...  Loss: 0.0144\n",
      "Epoch: 47/100...  Loss: 0.0136\n",
      "Epoch: 47/100...  Loss: 0.0154\n",
      "Epoch: 47/100...  Loss: 0.0147\n",
      "Epoch: 47/100...  Loss: 0.0146\n",
      "Epoch: 47/100...  Loss: 0.0149\n",
      "Epoch: 47/100...  Loss: 0.0148\n",
      "Epoch: 47/100...  Loss: 0.0148\n",
      "Epoch: 47/100...  Loss: 0.0161\n",
      "Epoch: 47/100...  Loss: 0.0144\n",
      "Epoch: 48/100...  Loss: 0.0135\n",
      "Epoch: 48/100...  Loss: 0.0127\n",
      "Epoch: 48/100...  Loss: 0.0144\n",
      "Epoch: 48/100...  Loss: 0.0138\n",
      "Epoch: 48/100...  Loss: 0.0137\n",
      "Epoch: 48/100...  Loss: 0.0140\n",
      "Epoch: 48/100...  Loss: 0.0138\n",
      "Epoch: 48/100...  Loss: 0.0138\n",
      "Epoch: 48/100...  Loss: 0.0149\n",
      "Epoch: 48/100...  Loss: 0.0134\n",
      "Epoch: 49/100...  Loss: 0.0127\n",
      "Epoch: 49/100...  Loss: 0.0119\n",
      "Epoch: 49/100...  Loss: 0.0135\n",
      "Epoch: 49/100...  Loss: 0.0130\n",
      "Epoch: 49/100...  Loss: 0.0129\n",
      "Epoch: 49/100...  Loss: 0.0132\n",
      "Epoch: 49/100...  Loss: 0.0129\n",
      "Epoch: 49/100...  Loss: 0.0129\n",
      "Epoch: 49/100...  Loss: 0.0139\n",
      "Epoch: 49/100...  Loss: 0.0126\n",
      "Epoch: 50/100...  Loss: 0.0119\n",
      "Epoch: 50/100...  Loss: 0.0113\n",
      "Epoch: 50/100...  Loss: 0.0128\n",
      "Epoch: 50/100...  Loss: 0.0123\n",
      "Epoch: 50/100...  Loss: 0.0121\n",
      "Epoch: 50/100...  Loss: 0.0125\n",
      "Epoch: 50/100...  Loss: 0.0121\n",
      "Epoch: 50/100...  Loss: 0.0122\n",
      "Epoch: 50/100...  Loss: 0.0130\n",
      "Epoch: 50/100...  Loss: 0.0118\n",
      "Epoch: 51/100...  Loss: 0.0113\n",
      "Epoch: 51/100...  Loss: 0.0107\n",
      "Epoch: 51/100...  Loss: 0.0122\n",
      "Epoch: 51/100...  Loss: 0.0117\n",
      "Epoch: 51/100...  Loss: 0.0115\n",
      "Epoch: 51/100...  Loss: 0.0118\n",
      "Epoch: 51/100...  Loss: 0.0114\n",
      "Epoch: 51/100...  Loss: 0.0114\n",
      "Epoch: 51/100...  Loss: 0.0122\n",
      "Epoch: 51/100...  Loss: 0.0112\n",
      "Epoch: 52/100...  Loss: 0.0107\n",
      "Epoch: 52/100...  Loss: 0.0102\n",
      "Epoch: 52/100...  Loss: 0.0115\n",
      "Epoch: 52/100...  Loss: 0.0110\n",
      "Epoch: 52/100...  Loss: 0.0108\n",
      "Epoch: 52/100...  Loss: 0.0112\n",
      "Epoch: 52/100...  Loss: 0.0108\n",
      "Epoch: 52/100...  Loss: 0.0108\n",
      "Epoch: 52/100...  Loss: 0.0115\n",
      "Epoch: 52/100...  Loss: 0.0106\n",
      "Epoch: 53/100...  Loss: 0.0101\n",
      "Epoch: 53/100...  Loss: 0.0097\n",
      "Epoch: 53/100...  Loss: 0.0109\n",
      "Epoch: 53/100...  Loss: 0.0105\n",
      "Epoch: 53/100...  Loss: 0.0103\n",
      "Epoch: 53/100...  Loss: 0.0107\n",
      "Epoch: 53/100...  Loss: 0.0103\n",
      "Epoch: 53/100...  Loss: 0.0102\n",
      "Epoch: 53/100...  Loss: 0.0110\n",
      "Epoch: 53/100...  Loss: 0.0100\n",
      "Epoch: 54/100...  Loss: 0.0096\n",
      "Epoch: 54/100...  Loss: 0.0092\n",
      "Epoch: 54/100...  Loss: 0.0103\n",
      "Epoch: 54/100...  Loss: 0.0100\n",
      "Epoch: 54/100...  Loss: 0.0098\n",
      "Epoch: 54/100...  Loss: 0.0101\n",
      "Epoch: 54/100...  Loss: 0.0098\n",
      "Epoch: 54/100...  Loss: 0.0097\n",
      "Epoch: 54/100...  Loss: 0.0104\n",
      "Epoch: 54/100...  Loss: 0.0095\n",
      "Epoch: 55/100...  Loss: 0.0091\n",
      "Epoch: 55/100...  Loss: 0.0088\n",
      "Epoch: 55/100...  Loss: 0.0098\n",
      "Epoch: 55/100...  Loss: 0.0095\n",
      "Epoch: 55/100...  Loss: 0.0093\n",
      "Epoch: 55/100...  Loss: 0.0096\n",
      "Epoch: 55/100...  Loss: 0.0093\n",
      "Epoch: 55/100...  Loss: 0.0092\n",
      "Epoch: 55/100...  Loss: 0.0099\n",
      "Epoch: 55/100...  Loss: 0.0091\n",
      "Epoch: 56/100...  Loss: 0.0087\n",
      "Epoch: 56/100...  Loss: 0.0084\n",
      "Epoch: 56/100...  Loss: 0.0094\n",
      "Epoch: 56/100...  Loss: 0.0090\n",
      "Epoch: 56/100...  Loss: 0.0089\n",
      "Epoch: 56/100...  Loss: 0.0092\n",
      "Epoch: 56/100...  Loss: 0.0088\n",
      "Epoch: 56/100...  Loss: 0.0088\n",
      "Epoch: 56/100...  Loss: 0.0094\n",
      "Epoch: 56/100...  Loss: 0.0087\n",
      "Epoch: 57/100...  Loss: 0.0083\n",
      "Epoch: 57/100...  Loss: 0.0080\n",
      "Epoch: 57/100...  Loss: 0.0090\n",
      "Epoch: 57/100...  Loss: 0.0087\n",
      "Epoch: 57/100...  Loss: 0.0085\n",
      "Epoch: 57/100...  Loss: 0.0088\n",
      "Epoch: 57/100...  Loss: 0.0084\n",
      "Epoch: 57/100...  Loss: 0.0084\n",
      "Epoch: 57/100...  Loss: 0.0090\n",
      "Epoch: 57/100...  Loss: 0.0083\n",
      "Epoch: 58/100...  Loss: 0.0080\n",
      "Epoch: 58/100...  Loss: 0.0076\n",
      "Epoch: 58/100...  Loss: 0.0086\n",
      "Epoch: 58/100...  Loss: 0.0083\n",
      "Epoch: 58/100...  Loss: 0.0081\n",
      "Epoch: 58/100...  Loss: 0.0084\n",
      "Epoch: 58/100...  Loss: 0.0081\n",
      "Epoch: 58/100...  Loss: 0.0081\n",
      "Epoch: 58/100...  Loss: 0.0086\n",
      "Epoch: 58/100...  Loss: 0.0079\n",
      "Epoch: 59/100...  Loss: 0.0076\n",
      "Epoch: 59/100...  Loss: 0.0073\n",
      "Epoch: 59/100...  Loss: 0.0083\n",
      "Epoch: 59/100...  Loss: 0.0080\n",
      "Epoch: 59/100...  Loss: 0.0078\n",
      "Epoch: 59/100...  Loss: 0.0081\n",
      "Epoch: 59/100...  Loss: 0.0077\n",
      "Epoch: 59/100...  Loss: 0.0077\n",
      "Epoch: 59/100...  Loss: 0.0082\n",
      "Epoch: 59/100...  Loss: 0.0076\n",
      "Epoch: 60/100...  Loss: 0.0073\n",
      "Epoch: 60/100...  Loss: 0.0070\n",
      "Epoch: 60/100...  Loss: 0.0079\n",
      "Epoch: 60/100...  Loss: 0.0076\n",
      "Epoch: 60/100...  Loss: 0.0074\n",
      "Epoch: 60/100...  Loss: 0.0077\n",
      "Epoch: 60/100...  Loss: 0.0074\n",
      "Epoch: 60/100...  Loss: 0.0074\n",
      "Epoch: 60/100...  Loss: 0.0079\n",
      "Epoch: 60/100...  Loss: 0.0072\n",
      "Epoch: 61/100...  Loss: 0.0070\n",
      "Epoch: 61/100...  Loss: 0.0068\n",
      "Epoch: 61/100...  Loss: 0.0076\n",
      "Epoch: 61/100...  Loss: 0.0073\n",
      "Epoch: 61/100...  Loss: 0.0072\n",
      "Epoch: 61/100...  Loss: 0.0074\n",
      "Epoch: 61/100...  Loss: 0.0072\n",
      "Epoch: 61/100...  Loss: 0.0071\n",
      "Epoch: 61/100...  Loss: 0.0076\n",
      "Epoch: 61/100...  Loss: 0.0070\n",
      "Epoch: 62/100...  Loss: 0.0067\n",
      "Epoch: 62/100...  Loss: 0.0065\n",
      "Epoch: 62/100...  Loss: 0.0074\n",
      "Epoch: 62/100...  Loss: 0.0070\n",
      "Epoch: 62/100...  Loss: 0.0069\n",
      "Epoch: 62/100...  Loss: 0.0071\n",
      "Epoch: 62/100...  Loss: 0.0069\n",
      "Epoch: 62/100...  Loss: 0.0068\n",
      "Epoch: 62/100...  Loss: 0.0073\n",
      "Epoch: 62/100...  Loss: 0.0067\n",
      "Epoch: 63/100...  Loss: 0.0065\n",
      "Epoch: 63/100...  Loss: 0.0063\n",
      "Epoch: 63/100...  Loss: 0.0071\n",
      "Epoch: 63/100...  Loss: 0.0067\n",
      "Epoch: 63/100...  Loss: 0.0066\n",
      "Epoch: 63/100...  Loss: 0.0069\n",
      "Epoch: 63/100...  Loss: 0.0066\n",
      "Epoch: 63/100...  Loss: 0.0066\n",
      "Epoch: 63/100...  Loss: 0.0070\n",
      "Epoch: 63/100...  Loss: 0.0065\n",
      "Epoch: 64/100...  Loss: 0.0063\n",
      "Epoch: 64/100...  Loss: 0.0061\n",
      "Epoch: 64/100...  Loss: 0.0069\n",
      "Epoch: 64/100...  Loss: 0.0065\n",
      "Epoch: 64/100...  Loss: 0.0064\n",
      "Epoch: 64/100...  Loss: 0.0066\n",
      "Epoch: 64/100...  Loss: 0.0064\n",
      "Epoch: 64/100...  Loss: 0.0064\n",
      "Epoch: 64/100...  Loss: 0.0067\n",
      "Epoch: 64/100...  Loss: 0.0062\n",
      "Epoch: 65/100...  Loss: 0.0060\n",
      "Epoch: 65/100...  Loss: 0.0058\n",
      "Epoch: 65/100...  Loss: 0.0066\n",
      "Epoch: 65/100...  Loss: 0.0063\n",
      "Epoch: 65/100...  Loss: 0.0061\n",
      "Epoch: 65/100...  Loss: 0.0064\n",
      "Epoch: 65/100...  Loss: 0.0062\n",
      "Epoch: 65/100...  Loss: 0.0061\n",
      "Epoch: 65/100...  Loss: 0.0065\n",
      "Epoch: 65/100...  Loss: 0.0060\n",
      "Epoch: 66/100...  Loss: 0.0058\n",
      "Epoch: 66/100...  Loss: 0.0057\n",
      "Epoch: 66/100...  Loss: 0.0064\n",
      "Epoch: 66/100...  Loss: 0.0061\n",
      "Epoch: 66/100...  Loss: 0.0059\n",
      "Epoch: 66/100...  Loss: 0.0061\n",
      "Epoch: 66/100...  Loss: 0.0060\n",
      "Epoch: 66/100...  Loss: 0.0059\n",
      "Epoch: 66/100...  Loss: 0.0063\n",
      "Epoch: 66/100...  Loss: 0.0058\n",
      "Epoch: 67/100...  Loss: 0.0056\n",
      "Epoch: 67/100...  Loss: 0.0055\n",
      "Epoch: 67/100...  Loss: 0.0062\n",
      "Epoch: 67/100...  Loss: 0.0059\n",
      "Epoch: 67/100...  Loss: 0.0057\n",
      "Epoch: 67/100...  Loss: 0.0059\n",
      "Epoch: 67/100...  Loss: 0.0058\n",
      "Epoch: 67/100...  Loss: 0.0058\n",
      "Epoch: 67/100...  Loss: 0.0061\n",
      "Epoch: 67/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0055\n",
      "Epoch: 68/100...  Loss: 0.0053\n",
      "Epoch: 68/100...  Loss: 0.0060\n",
      "Epoch: 68/100...  Loss: 0.0057\n",
      "Epoch: 68/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0057\n",
      "Epoch: 68/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0059\n",
      "Epoch: 68/100...  Loss: 0.0054\n",
      "Epoch: 69/100...  Loss: 0.0053\n",
      "Epoch: 69/100...  Loss: 0.0052\n",
      "Epoch: 69/100...  Loss: 0.0058\n",
      "Epoch: 69/100...  Loss: 0.0055\n",
      "Epoch: 69/100...  Loss: 0.0054\n",
      "Epoch: 69/100...  Loss: 0.0055\n",
      "Epoch: 69/100...  Loss: 0.0054\n",
      "Epoch: 69/100...  Loss: 0.0054\n",
      "Epoch: 69/100...  Loss: 0.0057\n",
      "Epoch: 69/100...  Loss: 0.0052\n",
      "Epoch: 70/100...  Loss: 0.0051\n",
      "Epoch: 70/100...  Loss: 0.0050\n",
      "Epoch: 70/100...  Loss: 0.0057\n",
      "Epoch: 70/100...  Loss: 0.0053\n",
      "Epoch: 70/100...  Loss: 0.0052\n",
      "Epoch: 70/100...  Loss: 0.0054\n",
      "Epoch: 70/100...  Loss: 0.0053\n",
      "Epoch: 70/100...  Loss: 0.0052\n",
      "Epoch: 70/100...  Loss: 0.0055\n",
      "Epoch: 70/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0050\n",
      "Epoch: 71/100...  Loss: 0.0049\n",
      "Epoch: 71/100...  Loss: 0.0055\n",
      "Epoch: 71/100...  Loss: 0.0052\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0052\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0053\n",
      "Epoch: 71/100...  Loss: 0.0049\n",
      "Epoch: 72/100...  Loss: 0.0048\n",
      "Epoch: 72/100...  Loss: 0.0047\n",
      "Epoch: 72/100...  Loss: 0.0054\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0049\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0049\n",
      "Epoch: 72/100...  Loss: 0.0052\n",
      "Epoch: 72/100...  Loss: 0.0047\n",
      "Epoch: 73/100...  Loss: 0.0047\n",
      "Epoch: 73/100...  Loss: 0.0046\n",
      "Epoch: 73/100...  Loss: 0.0052\n",
      "Epoch: 73/100...  Loss: 0.0049\n",
      "Epoch: 73/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0049\n",
      "Epoch: 73/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0050\n",
      "Epoch: 73/100...  Loss: 0.0046\n",
      "Epoch: 74/100...  Loss: 0.0046\n",
      "Epoch: 74/100...  Loss: 0.0044\n",
      "Epoch: 74/100...  Loss: 0.0051\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0046\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0046\n",
      "Epoch: 74/100...  Loss: 0.0049\n",
      "Epoch: 74/100...  Loss: 0.0045\n",
      "Epoch: 75/100...  Loss: 0.0044\n",
      "Epoch: 75/100...  Loss: 0.0043\n",
      "Epoch: 75/100...  Loss: 0.0049\n",
      "Epoch: 75/100...  Loss: 0.0046\n",
      "Epoch: 75/100...  Loss: 0.0045\n",
      "Epoch: 75/100...  Loss: 0.0046\n",
      "Epoch: 75/100...  Loss: 0.0045\n",
      "Epoch: 75/100...  Loss: 0.0045\n",
      "Epoch: 75/100...  Loss: 0.0047\n",
      "Epoch: 75/100...  Loss: 0.0043\n",
      "Epoch: 76/100...  Loss: 0.0043\n",
      "Epoch: 76/100...  Loss: 0.0042\n",
      "Epoch: 76/100...  Loss: 0.0048\n",
      "Epoch: 76/100...  Loss: 0.0045\n",
      "Epoch: 76/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0046\n",
      "Epoch: 76/100...  Loss: 0.0042\n",
      "Epoch: 77/100...  Loss: 0.0042\n",
      "Epoch: 77/100...  Loss: 0.0041\n",
      "Epoch: 77/100...  Loss: 0.0047\n",
      "Epoch: 77/100...  Loss: 0.0044\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0045\n",
      "Epoch: 77/100...  Loss: 0.0041\n",
      "Epoch: 78/100...  Loss: 0.0041\n",
      "Epoch: 78/100...  Loss: 0.0040\n",
      "Epoch: 78/100...  Loss: 0.0046\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0044\n",
      "Epoch: 78/100...  Loss: 0.0040\n",
      "Epoch: 79/100...  Loss: 0.0040\n",
      "Epoch: 79/100...  Loss: 0.0039\n",
      "Epoch: 79/100...  Loss: 0.0044\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0042\n",
      "Epoch: 79/100...  Loss: 0.0039\n",
      "Epoch: 80/100...  Loss: 0.0039\n",
      "Epoch: 80/100...  Loss: 0.0038\n",
      "Epoch: 80/100...  Loss: 0.0044\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0041\n",
      "Epoch: 80/100...  Loss: 0.0038\n",
      "Epoch: 81/100...  Loss: 0.0038\n",
      "Epoch: 81/100...  Loss: 0.0037\n",
      "Epoch: 81/100...  Loss: 0.0043\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0040\n",
      "Epoch: 81/100...  Loss: 0.0037\n",
      "Epoch: 82/100...  Loss: 0.0037\n",
      "Epoch: 82/100...  Loss: 0.0036\n",
      "Epoch: 82/100...  Loss: 0.0042\n",
      "Epoch: 82/100...  Loss: 0.0039\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0039\n",
      "Epoch: 82/100...  Loss: 0.0036\n",
      "Epoch: 83/100...  Loss: 0.0036\n",
      "Epoch: 83/100...  Loss: 0.0035\n",
      "Epoch: 83/100...  Loss: 0.0043\n",
      "Epoch: 83/100...  Loss: 0.0038\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0038\n",
      "Epoch: 83/100...  Loss: 0.0035\n",
      "Epoch: 84/100...  Loss: 0.0035\n",
      "Epoch: 84/100...  Loss: 0.0035\n",
      "Epoch: 84/100...  Loss: 0.0040\n",
      "Epoch: 84/100...  Loss: 0.0037\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0037\n",
      "Epoch: 84/100...  Loss: 0.0034\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 85/100...  Loss: 0.0038\n",
      "Epoch: 85/100...  Loss: 0.0036\n",
      "Epoch: 85/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0037\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0033\n",
      "Epoch: 86/100...  Loss: 0.0033\n",
      "Epoch: 86/100...  Loss: 0.0037\n",
      "Epoch: 86/100...  Loss: 0.0035\n",
      "Epoch: 86/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0036\n",
      "Epoch: 86/100...  Loss: 0.0033\n",
      "Epoch: 87/100...  Loss: 0.0033\n",
      "Epoch: 87/100...  Loss: 0.0032\n",
      "Epoch: 87/100...  Loss: 0.0036\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0033\n",
      "Epoch: 87/100...  Loss: 0.0033\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0035\n",
      "Epoch: 87/100...  Loss: 0.0032\n",
      "Epoch: 88/100...  Loss: 0.0032\n",
      "Epoch: 88/100...  Loss: 0.0032\n",
      "Epoch: 88/100...  Loss: 0.0036\n",
      "Epoch: 88/100...  Loss: 0.0034\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0034\n",
      "Epoch: 88/100...  Loss: 0.0031\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 89/100...  Loss: 0.0035\n",
      "Epoch: 89/100...  Loss: 0.0033\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0033\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0030\n",
      "Epoch: 90/100...  Loss: 0.0030\n",
      "Epoch: 90/100...  Loss: 0.0034\n",
      "Epoch: 90/100...  Loss: 0.0032\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0032\n",
      "Epoch: 90/100...  Loss: 0.0032\n",
      "Epoch: 90/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0033\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0032\n",
      "Epoch: 91/100...  Loss: 0.0029\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 92/100...  Loss: 0.0033\n",
      "Epoch: 92/100...  Loss: 0.0031\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0031\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0032\n",
      "Epoch: 93/100...  Loss: 0.0030\n",
      "Epoch: 93/100...  Loss: 0.0030\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0030\n",
      "Epoch: 93/100...  Loss: 0.0030\n",
      "Epoch: 93/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0031\n",
      "Epoch: 94/100...  Loss: 0.0030\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0030\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0027\n",
      "Epoch: 95/100...  Loss: 0.0027\n",
      "Epoch: 95/100...  Loss: 0.0031\n",
      "Epoch: 95/100...  Loss: 0.0029\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0029\n",
      "Epoch: 95/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0030\n",
      "Epoch: 96/100...  Loss: 0.0029\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0029\n",
      "Epoch: 96/100...  Loss: 0.0026\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 97/100...  Loss: 0.0030\n",
      "Epoch: 97/100...  Loss: 0.0028\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0028\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0029\n",
      "Epoch: 98/100...  Loss: 0.0028\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 99/100...  Loss: 0.0028\n",
      "Epoch: 99/100...  Loss: 0.0027\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0027\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0028\n",
      "Epoch: 100/100...  Loss: 0.0027\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0025\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def load_model(architecture = 'nn', filepath = 'checkpoint.pth'):\n",
    "    print(\"Loading model from {} \\n\".format(filepath))\n",
    "\n",
    "    if architecture == 'nn':\n",
    "        checkpoint = torch.load(filepath)\n",
    "        input_size = checkpoint['input_size']\n",
    "        output_size = checkpoint['output_size']\n",
    "        hidden_sizes = checkpoint['hidden_layers']\n",
    "        dropout = checkpoint['dropout']\n",
    "        model = nn.Sequential(OrderedDict([\n",
    "                              ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                              ('relu1', nn.ReLU()),\n",
    "                              ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                              ('bn2', nn.BatchNorm1d(num_features=hidden_sizes[1])),\n",
    "                              ('relu2', nn.ReLU()),\n",
    "                              ('dropout', nn.Dropout(dropout)),\n",
    "                              ('fc3', nn.Linear(hidden_sizes[1], hidden_sizes[2])),\n",
    "                              ('bn3', nn.BatchNorm1d(num_features=hidden_sizes[2])),\n",
    "                              ('relu3', nn.ReLU()),\n",
    "                              ('logits', nn.Linear(hidden_sizes[2], output_size))]))\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    else:\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model = SimpleCNN()\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "model = load_model(architecture = 'nn', filepath = 'data/checkpoint.pth')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading model from data/checkpoint.pth \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "def get_preds(model, input, architecture = 'nn'):\n",
    "    # Turn off gradients to speed up this part\n",
    "    with torch.no_grad():\n",
    "        if architecture == 'nn':\n",
    "            logits = model.forward(input)\n",
    "        else:\n",
    "            image = input.numpy()\n",
    "            image = image.reshape(image.shape[0], 1, 28, 28)\n",
    "            logits = model.forward(torch.from_numpy(image).float())\n",
    "\n",
    "    ps = F.softmax(logits, dim=1)\n",
    "    return ps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "def evaluate_model(model, train, y_train, test, y_test, architecture = 'nn'):\n",
    "    train_pred = get_preds(model, train, architecture)\n",
    "    train_pred_labels = get_labels(train_pred)\n",
    "\n",
    "    test_pred = get_preds(model, test, architecture)\n",
    "    test_pred_labels = get_labels(test_pred)\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, train_pred_labels)\n",
    "    accuracy_test = accuracy_score(y_test, test_pred_labels)\n",
    "\n",
    "    print(\"Accuracy score for train set is {} \\n\".format(accuracy_train))\n",
    "    print(\"Accuracy score for test set is {} \\n\".format(accuracy_test))\n",
    "\n",
    "    return accuracy_train, accuracy_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "def get_labels(pred):\n",
    "    pred_np = pred.numpy()\n",
    "    pred_values = np.amax(pred_np, axis=1, keepdims=True)\n",
    "    pred_labels = np.array([np.where(pred_np[i, :] == pred_values[i, :])[0] for i in range(pred_np.shape[0])])\n",
    "    pred_labels = pred_labels.reshape(len(pred_np), 1)\n",
    "\n",
    "    return pred_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "y_test = y_test.reshape((X_test.shape[0], 1))\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "y_test_numpy = y_test.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "evaluate_model(model, X_train_shuffled, y_train_shuffled, X_test, y_test, architecture='nn')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy score for train set is 0.9331428571428572 \n",
      "\n",
      "Accuracy score for test set is 0.586 \n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.9331428571428572, 0.586)"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "evaluate_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "def plot_learning_curve(input_size, output_size, hidden_sizes, train, labels, y_train, test, y_test, learning_rate = 0.003, weight_decay = 0.0, dropout = 0.0, n_chunks = 1000, optimizer = 'SGD'):\n",
    "    \"\"\"\n",
    "    Function to plot learning curve depending on the number of epochs.\n",
    "\n",
    "    INPUT:\n",
    "        input_size, output_size, hidden_sizes - model parameters\n",
    "        train - (tensor) train dataset\n",
    "        labels - (tensor) labels for train dataset\n",
    "        y_train - (numpy) labels for train dataset\n",
    "        test - (tensor) test dataset\n",
    "        y_test - (numpy) labels for test dataset\n",
    "        learning_rate - learning rate hyperparameter\n",
    "        weight_decay - weight decay (regularization)\n",
    "        dropout - dropout for hidden layer\n",
    "        n_chunks - the number of minibatches to train the model\n",
    "        optimizer - optimizer to be used for training (SGD or Adam)\n",
    "\n",
    "    OUTPUT: None\n",
    "    \"\"\"\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "\n",
    "    for epochs in np.arange(10, 210, 10):\n",
    "        # create model\n",
    "        modle = build_model(input_size, output_size, hidden_sizes, dropout = dropout)\n",
    "\n",
    "        # fit model\n",
    "        fit_model(modle, train, labels, epochs = epochs, n_chunks = n_chunks, learning_rate = learning_rate, weight_decay = weight_decay, optimizer = optimizer)\n",
    "        # get accuracy\n",
    "        accuracy_train, accuracy_test = evaluate_model(modle, train, y_train, test, y_test)\n",
    "\n",
    "        train_acc.append(accuracy_train)\n",
    "        test_acc.append(accuracy_test)\n",
    "\n",
    "    # Plot curve\n",
    "    x = np.arange(10, 210, 10)\n",
    "    plt.plot(x, train_acc)\n",
    "    plt.plot(x, test_acc)\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.title('Accuracy, learning_rate = ' + str(learning_rate), fontsize=20)\n",
    "    plt.xlabel('Number of epochs', fontsize=14)\n",
    "    plt.ylabel('Accuracy', fontsize=14)\n",
    "\n",
    "    ts = time.time()\n",
    "    plt.savefig('learning_curve' + str(ts) + '.png')\n",
    "\n",
    "    df = pd.DataFrame.from_dict({'train' : train_acc, 'test' :test_acc})\n",
    "    df.to_csv('learning_curve_' + str(ts) + '.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "plot_learning_curve(input_size=784, output_size=10, hidden_sizes=[128, 100, 64], \n",
    "                    train=X_train_shuffled, labels=y_train_shuffled, \n",
    "                    y_train=y_train_numpy, \n",
    "                    test=X_test, y_test=y_test)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "UnboundLocalError",
     "evalue": "local variable 'model' referenced before assignment",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-4017d1845fc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_shuffled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train_shuffled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train_numpy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     test=X_test, y_test=y_test)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-9ebd4db13ec6>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[0;34m(input_size, output_size, hidden_sizes, train, labels, y_train, test, y_test, learning_rate, weight_decay, dropout, n_chunks, optimizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m210\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mmodle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-496da0b59cf6>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(input_size, output_size, architecture, dropout)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Build a simple convolutional network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'model' referenced before assignment"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "X_train[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "def view_image(img, filename = 'image'):\n",
    "    fig, ax = plt.subplots(figsize=(6,9))\n",
    "    ax.imshow(img.reshape(28, 28).squeeze())\n",
    "    ax.axis('off')\n",
    "    plt.savefig(filename + '.png')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "view_image(X_train[0], filename='train_0')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x648 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"349.2pt\" version=\"1.1\" viewBox=\"0 0 349.2 349.2\" width=\"349.2pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-12T14:53:15.618710</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 349.2 \nL 349.2 349.2 \nL 349.2 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g clip-path=\"url(#p371ed872f9)\">\n    <image height=\"335\" id=\"image75550fa2dc\" transform=\"scale(1 -1)translate(0 -335)\" width=\"335\" x=\"7.2\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAU8AAAFPCAYAAADNzUzyAAAK4UlEQVR4nO3df4zfdWHH8fv27tpSaqldoV0p8tNSAwxkgwAOqNpsyYQ5s1PZ4A/RTYbQDuePGMnMjFlcwA2HQtmGyJJNsiUwVrMobI4umwWrAoZu8rM/2FQ6EGmphf64u/2x+Oc2X6956R19PP5+3fe+6V2f9/7n/f0MVg/GJocAiMw62G8AYCYST4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQriCVAQT4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQriCVAQT4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQriCVAQT4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQriCVAQT4CCeAIURg72G4BXqhffeU60X/jQs9F+/PGnoj0/WU6eAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BhsHowNnmw38ShbOS410T7f//V5dF+76uzH+/sXYNs/0L2+osf3v3jjzc9Er32dLP2yUej/crR56L9W2/+cLQ/+rr7o/3QpDT8b5w8AQriCVAQT4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQriCVAQT4CCu+3/h1mHHx7tn/rcidH+X8//fLQfHQxH+5ns8qfPj/bfuv3UaH/UrV+P9pMHDkT74RXZ78LELS9H+y+v/Lto/+tb3xjtd77jsGh/4DvfjfYznZMnQEE8AQriCVAQT4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQriCVA45O62D+bMifYn/Uv2+n+4LPuC1919dbRfcVvw3POhoaGhzU9G8+EjF0f7icVHRPvtFy38sbdX/lp2d3vNq7dH+zM++b5ov+QzG6P9VNux5rxov/5D10X7v9p1erTfcNEp0f7Atqej/XTj5AlQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFA65u+1brjs32j966U3R/pzfvSraL7rt/mjP/2x81ZnRfnTTo9F+Ys+eaD/d7H3LWdH+T2/+dLTfsOe10f7uXz4n2o8/sSXaTzUnT4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQriCVAQT4CCeAIUxBOgcMjdbb/8sezZ3rdsvzDaz/mFbdEepqsDb/7ZaH/D57LPgVi/64xov/GCJdF+/IWd0T7l5AlQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFEYO9hv4/xpeclS0f/v8B6P9xzcui/avGdoW7WG6GvnKN6P91desjfb33XRLtD9tzZui/TGf2BjtU06eAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0Bhxt9t33PmsdF+eJD9vfipzePRHg5Vh929Kdp/6hMnR/slF3wn2k81J0+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToDDj77YPJian9htM8cvDoerJPUdF+8NG9kf7qf5UCidPgIJ4AhTEE6AgngAF8QQoiCdAQTwBCuIJUBBPgIJ4AhTEE6Aw4++2z3l+75S+/kuLs78vh0/R+4BXmiNnvxjt/+0HS6L9/Gidc/IEKIgnQEE8AQriCVAQT4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQrT7m77YCR7S1t+J+v/4/t/GO1fXLUn2i/+k2gee+me46P9upO/EO0/fO7bov2B7z0T7XnlmjVvXrR/z6J7o/0dD50d7VcMbYn2KSdPgIJ4AhTEE6AgngAF8QQoiCdAQTwBCuIJUBBPgIJ4AhTEE6Aw7e62b/9odn/18QtvjvaXbL042q87+y+i/XWDn4n2Q5OT0Xznl3462p90avYjfvqyE6L9suvdbee/PXvp6dH+xNGN0X75F4ej/VRz8gQoiCdAQTwBCuIJUBBPgIJ4AhTEE6AgngAF8QQoiCdAYbB6MJbdDwyNHH9stL9hQ/ao3Pc+dmm033VXdr3xwY+ti/ZvfPdvRvvZX/56tE8Nvza7bjmx7T+i/eT+fdGemWPW3LnRfuyhbdH+e/sXRvt/PuPwaD80MZ7tQ06eAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0Bhyh89/O0PLI32i8Kcz/vt0Wz/n49F+3s/mL3+xZ/6SrT/h43Lo/34rl3Z/okt0R5+ZOtHXx/t33PEA9H+/KveGu3nTXwt2k81J0+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToDDld9svecP90f63tmX3Xce//US0T/3++y+P9l9a99lof+tt50X7Y8Y2R3v4kcHrT4n2977r+vA7zI/Wu5cNR/t50XrqOXkCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAIXB6sHYZPIFI8uPjr7B3V9bH+1Pvf3qaH/ctdnd+dTI0iXRfs5fT0T7u076+2i/4vYro/3x12bP0h6ajH4dOIgmzzs92l9x+99E+4XDP4z2dz5/VrS/6sj7ov2Hfu6iaD/+3PejfcrJE6AgngAF8QQoiCdAQTwBCuIJUBBPgIJ4AhTEE6AgngAF8QQoxM9tf/6CY6L96CB7NvPR/7Q/2g8NBtF8x5pzo/0t13wm2r9u9r5of8nWt0T7x9+1LtpftmpVtN9y48pov+DOB6P95P7s32c6GV5yVLR/5m0nRvt5v7Ij2t932m3R/pt7o/nQxy57d7QfTGSfi7Dizq9G+yc+uCLan/CRqf3cCydPgIJ4AhTEE6AgngAF8QQoiCdAQTwBCuIJUBBPgIJ4AhTEE6AQ323f96rsLnlq97LRaL9y46ui/T3H3hztL396VbT/7trjov3Qpkei+Rlr3hftb3r/Z6P9G/5oQ7Tf9MnsswjW7zwz2k+lubOy937NovXRfv6sudH+4b3Z5fOT71ob7Vd+/MloP3juW9E+9ebNY9H+L995Y7T/vT9YHe3HX9gZ7Z08AQriCVAQT4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQriCVAQT4DCYPVgLHrY8o6150Xf4OGPZHfJUw+8PB7tr/jjNdF+6Y3hs58ns2dXT7XB6Oxov3Msu3v+7MUvR/vFC3dH++nk+y/Mj/b3/Xz2uQLLR7LX3znxUrQ/+6tXRPvjr5+I9pPf2Bzt9/7SWdF+w61/Fu3PuvbKaL/o89n/dSdPgIJ4AhTEE6AgngAF8QQoiCdAQTwBCuIJUBBPgIJ4AhTEE6AQP7f96Du3RfuVi7P7pXN+kD0XfvkdT0X7pc9sjPYz3eT+fdF+wR0PhPtoPqMdEe5/Y/RN0X7fhadF+63vyM4+//iLN0T75X97WLRf9cjbo/3Enw9H+9TuY7OWLApf38kToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCjEz20HZqaRpUui/WMfOCHaf2Hsxmh/9pzRaJ+68Ir3Rvu5X9wU7Z08AQriCVAQT4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQriCVAQT4CCu+3AT8TwggXRfsclp0T7Bdv3R/vZ93wj2qecPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgrvtAAUnT4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQriCVAQT4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQriCVAQT4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQriCVAQT4CCeAIUxBOgIJ4ABfEEKIgnQEE8AQriCVAQT4CCeAIUxBOg8F9T05UomErOVQAAAABJRU5ErkJggg==\" y=\"-7\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p371ed872f9\">\n   <rect height=\"334.8\" width=\"334.8\" x=\"7.2\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "def fit_conv(model, X_train, y_train, epochs = 100, n_chunks = 1000, learning_rate = 0.003, weight_decay = 0, optimizer = 'SGD'):\n",
    "\n",
    "    print(\"Fitting model with epochs = {epochs}, learning rate = {lr}\\n\"\\\n",
    "    .format(epochs = epochs, lr = learning_rate))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if (optimizer == 'SGD'):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "\n",
    "    print_every = 100\n",
    "    steps = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        images = torch.chunk(X_train, n_chunks)\n",
    "        labels = torch.chunk(y_train, n_chunks)\n",
    "\n",
    "        for i in range(n_chunks):\n",
    "            steps += 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward passes\n",
    "            np_images = images[i].numpy()\n",
    "            np_images = np_images.reshape(images[i].shape[0], 1, 28, 28)\n",
    "            img = torch.from_numpy(np_images).float()\n",
    "\n",
    "            output = model.forward(img)\n",
    "            loss = criterion(output, labels[i].squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                      \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "\n",
    "                running_loss = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "X_train_shuffled_numpy = X_train_shuffled.numpy()\n",
    "y_train_shuffled_numpy = y_train_shuffled.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "fit_conv(cnn_model, X_train_shuffled, y_train_shuffled, optimizer='Adam')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting model with epochs = 100, learning rate = 0.003\n",
      "\n",
      "Epoch: 1/100...  Loss: 1.0064\n",
      "Epoch: 1/100...  Loss: 0.9776\n",
      "Epoch: 1/100...  Loss: 0.9743\n",
      "Epoch: 1/100...  Loss: 0.9785\n",
      "Epoch: 1/100...  Loss: 0.9870\n",
      "Epoch: 1/100...  Loss: 0.9273\n",
      "Epoch: 1/100...  Loss: 0.8901\n",
      "Epoch: 1/100...  Loss: 0.9819\n",
      "Epoch: 1/100...  Loss: 0.9695\n",
      "Epoch: 1/100...  Loss: 0.8924\n",
      "Epoch: 2/100...  Loss: 0.8852\n",
      "Epoch: 2/100...  Loss: 0.8655\n",
      "Epoch: 2/100...  Loss: 0.8507\n",
      "Epoch: 2/100...  Loss: 0.8617\n",
      "Epoch: 2/100...  Loss: 0.8536\n",
      "Epoch: 2/100...  Loss: 0.8136\n",
      "Epoch: 2/100...  Loss: 0.7733\n",
      "Epoch: 2/100...  Loss: 0.8554\n",
      "Epoch: 2/100...  Loss: 0.8547\n",
      "Epoch: 2/100...  Loss: 0.7665\n",
      "Epoch: 3/100...  Loss: 0.7935\n",
      "Epoch: 3/100...  Loss: 0.7705\n",
      "Epoch: 3/100...  Loss: 0.7627\n",
      "Epoch: 3/100...  Loss: 0.7637\n",
      "Epoch: 3/100...  Loss: 0.7524\n",
      "Epoch: 3/100...  Loss: 0.7235\n",
      "Epoch: 3/100...  Loss: 0.6741\n",
      "Epoch: 3/100...  Loss: 0.7488\n",
      "Epoch: 3/100...  Loss: 0.7668\n",
      "Epoch: 3/100...  Loss: 0.6721\n",
      "Epoch: 4/100...  Loss: 0.7183\n",
      "Epoch: 4/100...  Loss: 0.6883\n",
      "Epoch: 4/100...  Loss: 0.6949\n",
      "Epoch: 4/100...  Loss: 0.6797\n",
      "Epoch: 4/100...  Loss: 0.6695\n",
      "Epoch: 4/100...  Loss: 0.6459\n",
      "Epoch: 4/100...  Loss: 0.5947\n",
      "Epoch: 4/100...  Loss: 0.6581\n",
      "Epoch: 4/100...  Loss: 0.6867\n",
      "Epoch: 4/100...  Loss: 0.5990\n",
      "Epoch: 5/100...  Loss: 0.6491\n",
      "Epoch: 5/100...  Loss: 0.6172\n",
      "Epoch: 5/100...  Loss: 0.6354\n",
      "Epoch: 5/100...  Loss: 0.5986\n",
      "Epoch: 5/100...  Loss: 0.5978\n",
      "Epoch: 5/100...  Loss: 0.5725\n",
      "Epoch: 5/100...  Loss: 0.5180\n",
      "Epoch: 5/100...  Loss: 0.5822\n",
      "Epoch: 5/100...  Loss: 0.6149\n",
      "Epoch: 5/100...  Loss: 0.5312\n",
      "Epoch: 6/100...  Loss: 0.5899\n",
      "Epoch: 6/100...  Loss: 0.5444\n",
      "Epoch: 6/100...  Loss: 0.5859\n",
      "Epoch: 6/100...  Loss: 0.5271\n",
      "Epoch: 6/100...  Loss: 0.5366\n",
      "Epoch: 6/100...  Loss: 0.5016\n",
      "Epoch: 6/100...  Loss: 0.4568\n",
      "Epoch: 6/100...  Loss: 0.5172\n",
      "Epoch: 6/100...  Loss: 0.5627\n",
      "Epoch: 6/100...  Loss: 0.4784\n",
      "Epoch: 7/100...  Loss: 0.5382\n",
      "Epoch: 7/100...  Loss: 0.4844\n",
      "Epoch: 7/100...  Loss: 0.5333\n",
      "Epoch: 7/100...  Loss: 0.4718\n",
      "Epoch: 7/100...  Loss: 0.4874\n",
      "Epoch: 7/100...  Loss: 0.4562\n",
      "Epoch: 7/100...  Loss: 0.4051\n",
      "Epoch: 7/100...  Loss: 0.4673\n",
      "Epoch: 7/100...  Loss: 0.5298\n",
      "Epoch: 7/100...  Loss: 0.4299\n",
      "Epoch: 8/100...  Loss: 0.4915\n",
      "Epoch: 8/100...  Loss: 0.4381\n",
      "Epoch: 8/100...  Loss: 0.4817\n",
      "Epoch: 8/100...  Loss: 0.4279\n",
      "Epoch: 8/100...  Loss: 0.4455\n",
      "Epoch: 8/100...  Loss: 0.4131\n",
      "Epoch: 8/100...  Loss: 0.3727\n",
      "Epoch: 8/100...  Loss: 0.4140\n",
      "Epoch: 8/100...  Loss: 0.4890\n",
      "Epoch: 8/100...  Loss: 0.4099\n",
      "Epoch: 9/100...  Loss: 0.4562\n",
      "Epoch: 9/100...  Loss: 0.4165\n",
      "Epoch: 9/100...  Loss: 0.4275\n",
      "Epoch: 9/100...  Loss: 0.3950\n",
      "Epoch: 9/100...  Loss: 0.4231\n",
      "Epoch: 9/100...  Loss: 0.3859\n",
      "Epoch: 9/100...  Loss: 0.3271\n",
      "Epoch: 9/100...  Loss: 0.3658\n",
      "Epoch: 9/100...  Loss: 0.4645\n",
      "Epoch: 9/100...  Loss: 0.3631\n",
      "Epoch: 10/100...  Loss: 0.4207\n",
      "Epoch: 10/100...  Loss: 0.3791\n",
      "Epoch: 10/100...  Loss: 0.3907\n",
      "Epoch: 10/100...  Loss: 0.3573\n",
      "Epoch: 10/100...  Loss: 0.3940\n",
      "Epoch: 10/100...  Loss: 0.3791\n",
      "Epoch: 10/100...  Loss: 0.2982\n",
      "Epoch: 10/100...  Loss: 0.3380\n",
      "Epoch: 10/100...  Loss: 0.4308\n",
      "Epoch: 10/100...  Loss: 0.3344\n",
      "Epoch: 11/100...  Loss: 0.4020\n",
      "Epoch: 11/100...  Loss: 0.3641\n",
      "Epoch: 11/100...  Loss: 0.3602\n",
      "Epoch: 11/100...  Loss: 0.3478\n",
      "Epoch: 11/100...  Loss: 0.3615\n",
      "Epoch: 11/100...  Loss: 0.3215\n",
      "Epoch: 11/100...  Loss: 0.2851\n",
      "Epoch: 11/100...  Loss: 0.3152\n",
      "Epoch: 11/100...  Loss: 0.3940\n",
      "Epoch: 11/100...  Loss: 0.3135\n",
      "Epoch: 12/100...  Loss: 0.3396\n",
      "Epoch: 12/100...  Loss: 0.3462\n",
      "Epoch: 12/100...  Loss: 0.3368\n",
      "Epoch: 12/100...  Loss: 0.3159\n",
      "Epoch: 12/100...  Loss: 0.3385\n",
      "Epoch: 12/100...  Loss: 0.2812\n",
      "Epoch: 12/100...  Loss: 0.2589\n",
      "Epoch: 12/100...  Loss: 0.3103\n",
      "Epoch: 12/100...  Loss: 0.3404\n",
      "Epoch: 12/100...  Loss: 0.2885\n",
      "Epoch: 13/100...  Loss: 0.3273\n",
      "Epoch: 13/100...  Loss: 0.3055\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('dl-env': conda)"
  },
  "interpreter": {
   "hash": "2aac1d2b359798efc30b59d804167ffd538e3092f84ffa86abb6645ab952d97e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}