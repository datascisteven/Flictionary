{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# import linear algebra and data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import matplotlib for plotting\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import helper libraries\n",
    "import requests\n",
    "from io import BytesIO # Use When expecting bytes-like objects\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from os import path\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "# import PIL for image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import image_utils\n",
    "from image_utils import add_flipped_and_rotated_images\n",
    "\n",
    "from simple_conv_nn import SimpleCNN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def main():\n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Argument parser')\n",
    "\n",
    "    parser.add_argument('--save_dir', action='store', default = ' ',\n",
    "                        help='Directory to save model checkpoint')\n",
    "\n",
    "    parser.add_argument('--learning_rate', type = float, action='store', default = 0.003,\n",
    "                        help='Model hyperparameters: learning rate')\n",
    "\n",
    "    parser.add_argument('--epochs', type = int, action='store', default = 30,\n",
    "                        help='Model hyperparameters: epochs')\n",
    "\n",
    "    parser.add_argument('--weight_decay', type = float, action='store', default = 0,\n",
    "                        help='Model hyperparameters: weight decay (regularization)')\n",
    "\n",
    "    parser.add_argument('--dropout', type = float, action='store', default = 0.0,\n",
    "                        help='Model hyperparameters: dropout')\n",
    "\n",
    "    parser.add_argument('--architecture', action='store', default = 'nn',\n",
    "                        help='Model architecture: nn - feed forward neural network with 1 hidden layer.',\n",
    "                        choices = ['nn', 'conv'])\n",
    "\n",
    "    parser.add_argument('--add_data', action='store_true',\n",
    "                        help='Add flipped and rotated images to the original training set.')\n",
    "\n",
    "    parser.add_argument('--mini_batches', type = int, action='store', default = 1000,\n",
    "                        help='Number of minibatches.')\n",
    "\n",
    "    parser.add_argument('--optimizer', action='store', default = 'SGD',\n",
    "    choices=['SGD', 'Adam'],\n",
    "    help='Optimizer for fitting the model.')\n",
    "\n",
    "    parser.add_argument('--gpu', action='store_true',\n",
    "                        help='Run training on GPU')\n",
    "    results = parser.parse_args()\n",
    "\n",
    "    learning_rate = results.learning_rate\n",
    "    epochs = results.epochs\n",
    "    weight_decay = results.weight_decay\n",
    "    dropout = results.dropout\n",
    "    architecture = results.architecture\n",
    "    n_chunks = results.mini_batches\n",
    "    optimizer = results.optimizer\n",
    "\n",
    "    if (results.gpu == True):\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "    if (results.save_dir == ' '):\n",
    "        save_path = 'checkpoint.pth'\n",
    "    else:\n",
    "        save_path = results.save_dir + '/' + 'checkpoint.pth'\n",
    "\n",
    "    # Load data\n",
    "    X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "    # Add flipped and rotated images to the dataset\n",
    "    if (results.add_data == True):\n",
    "        X_train, y_train = add_flipped_and_rotated_images(X_train, y_train)\n",
    "\n",
    "    # Save datasets to disk if required\n",
    "    save_data(X_train, y_train, X_test, y_test, force = results.add_data)\n",
    "\n",
    "    # Convert to tensors\n",
    "    train = torch.from_numpy(X_train).float()\n",
    "    labels = torch.from_numpy(y_train).long()\n",
    "    test = torch.from_numpy(X_test).float()\n",
    "    test_labels = torch.from_numpy(y_test).long()\n",
    "\n",
    "    # Hyperparameters for our network\n",
    "    input_size = 784\n",
    "    hidden_sizes = [128, 100, 64]\n",
    "    output_size = 10\n",
    "\n",
    "    # Build model\n",
    "    model = build_model(input_size, output_size, hidden_sizes, architecture = architecture, dropout = dropout)\n",
    "\n",
    "    # Fit model\n",
    "    if (architecture == 'nn'):\n",
    "        fit_model(model, train, labels, epochs = epochs, n_chunks = n_chunks, learning_rate = learning_rate, weight_decay = weight_decay, optimizer = optimizer)\n",
    "    else:\n",
    "        fit_conv(model, train, labels, epochs = epochs, n_chunks = n_chunks, learning_rate = learning_rate, weight_decay = weight_decay, optimizer = optimizer)\n",
    "\n",
    "    #plot_learning_curve(input_size, output_size, hidden_sizes, train, labels, y_train, test, y_test, learning_rate = learning_rate, dropout = dropout, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)\n",
    "    #plot_learning_curve_conv(input_size, output_size, hidden_sizes, train, labels, y_train, test, y_test, learning_rate = learning_rate, dropout = dropout, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)\n",
    "\n",
    "    # Evaluate model\n",
    "    #evaluate_model(model, train, y_train, test, y_test, architecture = architecture)\n",
    "    #test_model(model, test[0], architecture = architecture)\n",
    "\n",
    "    # Save the model\n",
    "    save_model(model,architecture, input_size, output_size, hidden_sizes, dropout, filepath = save_path)\n",
    "\n",
    "    #compare_hyperparameters(input_size, output_size, hidden_sizes, train, labels, y_train, test, y_test, learning_rate, n_chunks = n_chunks, optimizer = optimizer)\n",
    "\n",
    "    #loaded_model = load_model(architecture)\n",
    "    #loaded_model.eval()\n",
    "    #pred = test_model(loaded_model, test[0], architecture = architecture)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "usage: ipykernel_launcher [-h] [--save_dir SAVE_DIR]\n",
      "                          [--learning_rate LEARNING_RATE] [--epochs EPOCHS]\n",
      "                          [--weight_decay WEIGHT_DECAY] [--dropout DROPOUT]\n",
      "                          [--architecture {nn,conv}] [--add_data]\n",
      "                          [--mini_batches MINI_BATCHES]\n",
      "                          [--optimizer {SGD,Adam}] [--gpu]\n",
      "ipykernel_launcher: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"3e5043c8-c333-4242-bc6f-ae17ea62c731\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=/var/folders/0f/dj_hz1316w7c4qhdk33w3p3w0000gn/T/tmp-14831Fb7XbcN6TOkK.json\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "2",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/examsherpa/opt/anaconda3/envs/dl-env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Function loads quick draw dataset. If no data is loaded yet, the datasets\n",
    "    are loaded from the web. If there are already loaded datasets, then data\n",
    "    is loaded from the disk (pickle files).\n",
    "\n",
    "    INPUTS: None\n",
    "\n",
    "    OUTPUT:\n",
    "        X_train - train dataset\n",
    "        y_train - train dataset labels\n",
    "        X_test - test dataset\n",
    "        y_test - test dataset labels\n",
    "    \"\"\"\n",
    "    print(\"Loading data \\n\")\n",
    "\n",
    "    # Check for already loaded datasets\n",
    "    if not(path.exists('xtrain_doodle.pickle')):\n",
    "        # Load from web\n",
    "        print(\"Loading data from the web \\n\")\n",
    "\n",
    "        # Classes we will load\n",
    "        categories = ['bee', 'cat', 'cow', 'dog', 'duck', 'horse', 'pig', 'rabbit', 'snake', 'whale']\n",
    "\n",
    "        # Dictionary for URL and class labels\n",
    "        URL_DATA = {}\n",
    "        for category in categories:\n",
    "            URL_DATA[category] = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/' + category +'.npy'\n",
    "\n",
    "        # Load data for classes in dictionary\n",
    "        classes_dict = {}\n",
    "        for key, value in URL_DATA.items():\n",
    "            response = requests.get(value)\n",
    "            classes_dict[key] = np.load(BytesIO(response.content))\n",
    "\n",
    "        # Generate labels and add labels to loaded data\n",
    "        for i, (key, value) in enumerate(classes_dict.items()):\n",
    "            value = value.astype('float32')/255.\n",
    "            if i == 0:\n",
    "                classes_dict[key] = np.c_[value, np.zeros(len(value))]\n",
    "            else:\n",
    "                classes_dict[key] = np.c_[value,i*np.ones(len(value))]\n",
    "\n",
    "        # Create a dict with label codes\n",
    "        label_dict = {0:'bee', 1:'cat', 2:'cow', 3:'dog', 4:'duck',\n",
    "                      5:'horse', 6:'pig', 7:'rabbit', 8:'snake', 9:'whale'}\n",
    "\n",
    "        lst = []\n",
    "        for key, value in classes_dict.items():\n",
    "            lst.append(value[:3000])\n",
    "        doodles = np.concatenate(lst)\n",
    "\n",
    "        # Split the data into features and class labels (X & y respectively)\n",
    "        y = doodles[:,-1].astype('float32')\n",
    "        X = doodles[:,:784]\n",
    "\n",
    "        # Split each dataset into train/test splits\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "    else:\n",
    "        # Load data from pickle files\n",
    "        print(\"Loading data from pickle files \\n\")\n",
    "\n",
    "        file = open(\"xtrain_doodle.pickle\",'rb')\n",
    "        X_train = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"xtest_doodle.pickle\",'rb')\n",
    "        X_test = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"ytrain_doodle.pickle\",'rb')\n",
    "        y_train = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"ytest_doodle.pickle\",'rb')\n",
    "        y_test = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, classes_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "categories = ['bee', 'cat', 'cow', 'dog', 'duck', 'horse', 'pig', 'rabbit', 'snake', 'whale']\n",
    "\n",
    "URL_DATA = {}\n",
    "for category in categories:\n",
    "    URL_DATA[category] = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/' + category +'.npy'\n",
    "\n",
    "URL_DATA"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bee': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bee.npy',\n",
       " 'cat': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cat.npy',\n",
       " 'cow': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cow.npy',\n",
       " 'dog': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy',\n",
       " 'duck': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/duck.npy',\n",
       " 'horse': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/horse.npy',\n",
       " 'pig': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pig.npy',\n",
       " 'rabbit': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rabbit.npy',\n",
       " 'snake': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/snake.npy',\n",
       " 'whale': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/whale.npy'}"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "classes_dict = {}\n",
    "for key, value in URL_DATA.items():\n",
    "    response = requests.get(value)\n",
    "    classes_dict[key] = np.load(BytesIO(response.content))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "classes_dict"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bee': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'cat': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'cow': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'dog': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'duck': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'horse': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'pig': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'rabbit': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'snake': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'whale': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "len(classes_dict['dog'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "152159"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "len(classes_dict['cat'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "123202"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "# Generate labels and add labels to loaded data\n",
    "for i, (key, value) in enumerate(classes_dict.items()):\n",
    "    value = value.astype('float32')/255.\n",
    "    if i == 0:\n",
    "        classes_dict[key] = np.c_[value, np.zeros(len(value))]\n",
    "    else:\n",
    "        classes_dict[key] = np.c_[value,i*np.ones(len(value))]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "classes_dict['bee']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "classes_dict['bee'][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.33725491,\n",
       "       0.40000001, 0.40000001, 0.21960784, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00784314,\n",
       "       0.5411765 , 0.99607843, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.79215688, 0.23529412, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.34509805, 1.        , 0.5529412 ,\n",
       "       0.13333334, 0.09019608, 0.07843138, 0.38039216, 0.84313726,\n",
       "       0.96078432, 0.16078432, 0.        , 0.12156863, 0.23529412,\n",
       "       0.34117648, 0.1882353 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.72156864, 0.8509804 , 0.00392157, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.92941177, 0.66274512,\n",
       "       0.7019608 , 1.        , 1.        , 1.        , 0.97647059,\n",
       "       0.30980393, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10588235, 0.98823529, 0.48235294,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00392157, 0.92156863, 0.99215686, 0.9254902 , 0.4509804 ,\n",
       "       0.24313726, 0.16470589, 0.75294119, 0.97647059, 0.21176471,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.46666667, 0.99215686, 0.11764706, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.36078432, 1.        ,\n",
       "       0.80784315, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.10980392, 1.        , 0.39215687, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.76862746, 0.75686276,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01176471, 0.90980393, 1.        , 0.17254902, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 1.        ,\n",
       "       0.39215687, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.9254902 , 0.56078434, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.41176471, 1.        ,\n",
       "       0.80392158, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00784314, 0.69411767, 0.97647059, 0.18039216, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08235294, 1.        ,\n",
       "       0.40392157, 0.        , 0.12156863, 0.33725491, 0.40000001,\n",
       "       0.56862748, 0.98039216, 1.        , 0.46666667, 0.        ,\n",
       "       0.        , 0.        , 0.00784314, 0.56078434, 1.        ,\n",
       "       0.3764706 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.24313726, 1.        , 0.51764709, 0.72156864,\n",
       "       0.97254902, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.98039216, 0.95294118, 0.70980394, 0.29411766,\n",
       "       0.78431374, 1.        , 0.52941179, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.39607844,\n",
       "       1.        , 0.96078432, 1.        , 0.86274511, 0.94509804,\n",
       "       0.7647059 , 1.        , 0.74117649, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99607843, 0.9254902 , 0.32941177,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32549021, 0.96470588, 1.        ,\n",
       "       0.627451  , 0.89019608, 0.64313728, 0.88627452, 1.        ,\n",
       "       0.44313726, 0.28235295, 0.90588236, 1.        , 0.92941177,\n",
       "       0.93333334, 1.        , 0.67450982, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.61960787, 0.96862745, 1.        , 0.34901962, 1.        ,\n",
       "       0.32941177, 0.74901962, 1.        , 0.29019609, 0.12156863,\n",
       "       0.98431373, 1.        , 0.43921569, 0.00392157, 0.56470591,\n",
       "       1.        , 0.04705882, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.9137255 , 0.8509804 ,\n",
       "       0.90980393, 0.49803922, 0.97254902, 0.09803922, 0.98039216,\n",
       "       1.        , 0.07843138, 0.54509807, 0.97254902, 1.        ,\n",
       "       0.47843137, 0.6901961 , 0.67058825, 0.96078432, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.64313728, 0.49019608, 0.36470589, 0.16862746, 0.00784314,\n",
       "       0.05490196, 1.        , 0.93333334, 0.63529414, 0.70588237,\n",
       "       0.79215688, 0.36078432, 1.        , 0.87450981, 0.03921569,\n",
       "       0.92941177, 0.66274512, 1.        , 0.85882354, 0.96862745,\n",
       "       0.88235295, 0.77254903, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.18039216, 0.80392158, 0.9137255 ,\n",
       "       1.        , 1.        , 0.94509804, 0.65882355, 1.        ,\n",
       "       1.        , 0.36862746, 0.89019608, 0.60392159, 0.69411767,\n",
       "       0.98431373, 0.61960787, 0.41960785, 1.        , 0.37254903,\n",
       "       1.        , 0.83529413, 0.76862746, 0.99607843, 0.44705883,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 0.33725491,\n",
       "       0.55686277, 0.59607846, 1.        , 1.        , 0.27058825,\n",
       "       1.        , 0.4509804 , 0.96862745, 1.        , 0.44705883,\n",
       "       0.84313726, 0.74901962, 0.3137255 , 1.        , 0.26666668,\n",
       "       0.52156866, 0.99607843, 0.12156863, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.18431373,\n",
       "       1.        , 1.        , 0.36862746, 1.        , 0.50980395,\n",
       "       1.        , 1.        , 0.52156866, 1.        , 0.3137255 ,\n",
       "       0.41568628, 1.        , 0.07058824, 0.76862746, 0.78431374,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10980392, 0.98039216, 1.        ,\n",
       "       0.49019608, 0.99607843, 0.65098041, 0.9137255 , 1.        ,\n",
       "       0.76078433, 0.87058824, 0.01176471, 0.51372552, 0.96862745,\n",
       "       0.13333334, 0.99215686, 0.4509804 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.65490198, 1.        , 0.8509804 , 0.87058824,\n",
       "       0.85490197, 0.79215688, 1.        , 0.96470588, 0.53333336,\n",
       "       0.        , 0.6156863 , 0.86666667, 0.74509805, 0.93725491,\n",
       "       0.09411765, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.52941179,\n",
       "       0.97254902, 1.        , 0.94117647, 0.99607843, 0.73333335,\n",
       "       0.96862745, 1.        , 0.32549021, 0.        , 0.72941178,\n",
       "       0.90588236, 1.        , 0.34117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.60000002, 0.88235295, 0.98431373,\n",
       "       0.96862745, 1.        , 0.98823529, 0.98431373, 1.        ,\n",
       "       0.89803922, 0.81960785, 0.98431373, 0.99607843, 0.50980395,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.6156863 , 0.78823531, 0.77254903, 0.60784316, 0.97647059,\n",
       "       0.52549022, 0.66274512, 1.        , 0.66274512, 0.66666669,\n",
       "       0.93725491, 0.61176473, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.05882353, 0.09411765,\n",
       "       0.        , 0.10588235, 0.26666668, 0.        , 0.01568628,\n",
       "       0.24705882, 0.        , 0.        , 0.05882353, 0.01568628,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "label_dict = {0:'bee', 1:'cat', 2:'cow', 3:'dog', 4:'duck',\n",
    "              5:'horse', 6:'pig', 7:'rabbit', 8:'snake', 9:'whale'}\n",
    "\n",
    "lst = []\n",
    "for key, value in classes_dict.items():\n",
    "    lst.append(value[:3000])\n",
    "doodles = np.concatenate(lst)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "len(doodles)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "doodles[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.33725491,\n",
       "       0.40000001, 0.40000001, 0.21960784, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00784314,\n",
       "       0.5411765 , 0.99607843, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.79215688, 0.23529412, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.34509805, 1.        , 0.5529412 ,\n",
       "       0.13333334, 0.09019608, 0.07843138, 0.38039216, 0.84313726,\n",
       "       0.96078432, 0.16078432, 0.        , 0.12156863, 0.23529412,\n",
       "       0.34117648, 0.1882353 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.72156864, 0.8509804 , 0.00392157, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.92941177, 0.66274512,\n",
       "       0.7019608 , 1.        , 1.        , 1.        , 0.97647059,\n",
       "       0.30980393, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10588235, 0.98823529, 0.48235294,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00392157, 0.92156863, 0.99215686, 0.9254902 , 0.4509804 ,\n",
       "       0.24313726, 0.16470589, 0.75294119, 0.97647059, 0.21176471,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.46666667, 0.99215686, 0.11764706, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.36078432, 1.        ,\n",
       "       0.80784315, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.10980392, 1.        , 0.39215687, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.76862746, 0.75686276,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01176471, 0.90980393, 1.        , 0.17254902, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 1.        ,\n",
       "       0.39215687, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.9254902 , 0.56078434, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.41176471, 1.        ,\n",
       "       0.80392158, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00784314, 0.69411767, 0.97647059, 0.18039216, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08235294, 1.        ,\n",
       "       0.40392157, 0.        , 0.12156863, 0.33725491, 0.40000001,\n",
       "       0.56862748, 0.98039216, 1.        , 0.46666667, 0.        ,\n",
       "       0.        , 0.        , 0.00784314, 0.56078434, 1.        ,\n",
       "       0.3764706 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.24313726, 1.        , 0.51764709, 0.72156864,\n",
       "       0.97254902, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.98039216, 0.95294118, 0.70980394, 0.29411766,\n",
       "       0.78431374, 1.        , 0.52941179, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.39607844,\n",
       "       1.        , 0.96078432, 1.        , 0.86274511, 0.94509804,\n",
       "       0.7647059 , 1.        , 0.74117649, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99607843, 0.9254902 , 0.32941177,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32549021, 0.96470588, 1.        ,\n",
       "       0.627451  , 0.89019608, 0.64313728, 0.88627452, 1.        ,\n",
       "       0.44313726, 0.28235295, 0.90588236, 1.        , 0.92941177,\n",
       "       0.93333334, 1.        , 0.67450982, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.61960787, 0.96862745, 1.        , 0.34901962, 1.        ,\n",
       "       0.32941177, 0.74901962, 1.        , 0.29019609, 0.12156863,\n",
       "       0.98431373, 1.        , 0.43921569, 0.00392157, 0.56470591,\n",
       "       1.        , 0.04705882, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.9137255 , 0.8509804 ,\n",
       "       0.90980393, 0.49803922, 0.97254902, 0.09803922, 0.98039216,\n",
       "       1.        , 0.07843138, 0.54509807, 0.97254902, 1.        ,\n",
       "       0.47843137, 0.6901961 , 0.67058825, 0.96078432, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.64313728, 0.49019608, 0.36470589, 0.16862746, 0.00784314,\n",
       "       0.05490196, 1.        , 0.93333334, 0.63529414, 0.70588237,\n",
       "       0.79215688, 0.36078432, 1.        , 0.87450981, 0.03921569,\n",
       "       0.92941177, 0.66274512, 1.        , 0.85882354, 0.96862745,\n",
       "       0.88235295, 0.77254903, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.18039216, 0.80392158, 0.9137255 ,\n",
       "       1.        , 1.        , 0.94509804, 0.65882355, 1.        ,\n",
       "       1.        , 0.36862746, 0.89019608, 0.60392159, 0.69411767,\n",
       "       0.98431373, 0.61960787, 0.41960785, 1.        , 0.37254903,\n",
       "       1.        , 0.83529413, 0.76862746, 0.99607843, 0.44705883,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 0.33725491,\n",
       "       0.55686277, 0.59607846, 1.        , 1.        , 0.27058825,\n",
       "       1.        , 0.4509804 , 0.96862745, 1.        , 0.44705883,\n",
       "       0.84313726, 0.74901962, 0.3137255 , 1.        , 0.26666668,\n",
       "       0.52156866, 0.99607843, 0.12156863, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.18431373,\n",
       "       1.        , 1.        , 0.36862746, 1.        , 0.50980395,\n",
       "       1.        , 1.        , 0.52156866, 1.        , 0.3137255 ,\n",
       "       0.41568628, 1.        , 0.07058824, 0.76862746, 0.78431374,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10980392, 0.98039216, 1.        ,\n",
       "       0.49019608, 0.99607843, 0.65098041, 0.9137255 , 1.        ,\n",
       "       0.76078433, 0.87058824, 0.01176471, 0.51372552, 0.96862745,\n",
       "       0.13333334, 0.99215686, 0.4509804 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.65490198, 1.        , 0.8509804 , 0.87058824,\n",
       "       0.85490197, 0.79215688, 1.        , 0.96470588, 0.53333336,\n",
       "       0.        , 0.6156863 , 0.86666667, 0.74509805, 0.93725491,\n",
       "       0.09411765, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.52941179,\n",
       "       0.97254902, 1.        , 0.94117647, 0.99607843, 0.73333335,\n",
       "       0.96862745, 1.        , 0.32549021, 0.        , 0.72941178,\n",
       "       0.90588236, 1.        , 0.34117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.60000002, 0.88235295, 0.98431373,\n",
       "       0.96862745, 1.        , 0.98823529, 0.98431373, 1.        ,\n",
       "       0.89803922, 0.81960785, 0.98431373, 0.99607843, 0.50980395,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.6156863 , 0.78823531, 0.77254903, 0.60784316, 0.97647059,\n",
       "       0.52549022, 0.66274512, 1.        , 0.66274512, 0.66666669,\n",
       "       0.93725491, 0.61176473, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.05882353, 0.09411765,\n",
       "       0.        , 0.10588235, 0.26666668, 0.        , 0.01568628,\n",
       "       0.24705882, 0.        , 0.        , 0.05882353, 0.01568628,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "y = doodles[:,-1].astype('float32')\n",
    "X = doodles[:,:784]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 9., 9., 9.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "y_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 2., ..., 4., 0., 9.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "X_train[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.03921569, 0.33333334, 0.21568628,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.06666667, 0.48627451,\n",
       "       0.91764706, 1.        , 1.        , 0.23921569, 0.        ,\n",
       "       0.        , 0.07450981, 0.44705883, 0.3764706 , 0.06666667,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01568628, 0.20784314,\n",
       "       0.54901963, 0.94901961, 0.99215686, 0.67450982, 0.28235295,\n",
       "       1.        , 0.43921569, 0.        , 0.        , 0.36470589,\n",
       "       1.        , 1.        , 0.9137255 , 0.36862746, 0.00392157,\n",
       "       0.        , 0.        , 0.        , 0.01568628, 0.28235295,\n",
       "       0.59607846, 0.66666669, 0.66666669, 0.66666669, 0.66666669,\n",
       "       0.60000002, 0.95686275, 1.        , 0.97647059, 0.61176473,\n",
       "       0.15294118, 0.        , 0.22745098, 1.        , 0.3137255 ,\n",
       "       0.        , 0.        , 0.33725491, 1.        , 0.30980393,\n",
       "       0.74901962, 1.        , 0.75294119, 0.16078432, 0.        ,\n",
       "       0.36078432, 0.90980393, 1.        , 0.91764706, 0.80000001,\n",
       "       0.80000001, 0.80000001, 0.81176472, 0.98039216, 1.        ,\n",
       "       0.9254902 , 0.51764709, 0.05490196, 0.        , 0.        ,\n",
       "       0.68235296, 0.91764706, 0.04313726, 0.        , 0.        ,\n",
       "       0.28627452, 1.        , 0.21176471, 0.00392157, 0.36470589,\n",
       "       0.9137255 , 0.97647059, 0.73725492, 1.        , 0.74509805,\n",
       "       0.27843139, 0.01568628, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.0627451 , 0.34117648, 0.67843139, 0.99215686,\n",
       "       0.9137255 , 0.17254902, 0.16470589, 0.99215686, 0.47450981,\n",
       "       0.        , 0.        , 0.        , 0.07843138, 0.99215686,\n",
       "       0.45882353, 0.        , 0.        , 0.06666667, 0.72549021,\n",
       "       1.        , 0.4627451 , 0.00392157, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.17254902, 0.87450981, 0.89803922,\n",
       "       0.76862746, 0.93333334, 0.05490196, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.79607844, 0.72941178, 0.        ,\n",
       "       0.        , 0.23921569, 0.98431373, 0.62352943, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14509805, 0.95294118, 1.        , 0.35294119,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.52156866, 0.96862745, 0.11764706, 0.05098039, 0.89411765,\n",
       "       0.82745099, 0.03921569, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.44705883, 1.        , 0.25098041, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.09411765, 0.94117647,\n",
       "       0.68627453, 0.34117648, 1.        , 0.21960784, 0.        ,\n",
       "       0.        , 0.10588235, 0.01568628, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.02352941, 0.8509804 ,\n",
       "       0.81176472, 0.00784314, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.43137255, 1.        , 0.73725492,\n",
       "       0.89803922, 0.00392157, 0.        , 0.37254903, 0.98823529,\n",
       "       0.39215687, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32941177, 1.        , 0.25098041,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01176471, 0.60392159, 0.94117647, 0.60392159, 0.        ,\n",
       "       0.        , 0.68235296, 0.93333334, 0.07058824, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.02745098, 0.94509804, 0.56470591, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15686275,\n",
       "       1.        , 0.33725491, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.79607844,\n",
       "       0.67450982, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.21568628, 1.        , 0.25882354,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.76078433, 0.70980394, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.22352941, 1.        , 0.26274511, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.81960785, 0.6901961 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.99607843,\n",
       "       0.43921569, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.24705882, 0.99607843, 0.42745098,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.86666667, 0.68627453, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.20392157,\n",
       "       0.92941177, 0.81176472, 0.04313726, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.48235294, 0.99607843, 0.31764707, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.19215687, 0.02745098, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.23137255, 0.94117647, 0.84705883, 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01960784, 0.77254903,\n",
       "       0.98823529, 0.52549022, 0.14901961, 0.01960784, 0.07843138,\n",
       "       0.14117648, 0.42352942, 0.83529413, 1.        , 0.36078432,\n",
       "       0.        , 0.        , 0.        , 0.36078432, 0.95294118,\n",
       "       0.82352942, 0.09019608, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.03137255, 0.60784316, 0.99215686,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.75294119, 0.34117648, 0.01176471, 0.        , 0.10980392,\n",
       "       0.70588237, 1.        , 0.74117649, 0.07450981, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.09019608, 0.38431373, 0.45490196,\n",
       "       0.39607844, 0.33333334, 0.16078432, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.88627452, 0.94509804, 0.40000001,\n",
       "       0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.27450982, 0.10980392, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "X_train[1230]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.05098039, 0.3764706 , 0.35686275, 0.04313726, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.73725492, 1.        ,\n",
       "       1.        , 0.81568629, 0.10196079, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.10196079,\n",
       "       0.73333335, 0.70588237, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.7019608 , 0.80392158, 0.27058825, 0.93333334,\n",
       "       0.87843138, 0.16078432, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.23137255, 0.91764706, 1.        , 0.7764706 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.53725493,\n",
       "       0.95294118, 0.00392157, 0.16470589, 0.88235295, 0.93333334,\n",
       "       0.23921569, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.23137255, 0.94901961,\n",
       "       0.82352942, 0.86274511, 0.7019608 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.3764706 , 1.        , 0.11372549,\n",
       "       0.        , 0.09803922, 0.81176472, 0.97254902, 0.46666667,\n",
       "       0.60784316, 0.7647059 , 0.65098041, 0.36078432, 0.07450981,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.22745098, 0.94901961, 0.81568629, 0.07843138, 0.85490197,\n",
       "       0.61960787, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.20784314, 1.        , 0.32156864, 0.        , 0.        ,\n",
       "       0.05882353, 0.74509805, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.98823529, 0.74117649, 0.14117648,\n",
       "       0.        , 0.        , 0.22745098, 0.94901961, 0.81568629,\n",
       "       0.07843138, 0.        , 0.93333334, 0.54509807, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00784314, 0.87450981,\n",
       "       0.69803923, 0.        , 0.        , 0.        , 0.11372549,\n",
       "       0.94117647, 1.        , 0.98039216, 0.20784314, 0.18039216,\n",
       "       0.47450981, 0.90588236, 0.94509804, 0.31764707, 0.21960784,\n",
       "       0.94901961, 0.81568629, 0.07843138, 0.        , 0.01176471,\n",
       "       0.99215686, 0.47058824, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.48627451, 0.98823529, 0.10980392,\n",
       "       0.        , 0.        , 0.73333335, 0.94117647, 0.41960785,\n",
       "       0.22745098, 0.        , 0.        , 0.        , 0.09803922,\n",
       "       0.75294119, 0.99215686, 0.87058824, 0.8509804 , 0.07843138,\n",
       "       0.        , 0.        , 0.08235294, 1.        , 0.39607844,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09803922, 0.98431373, 0.49019608, 0.        , 0.22745098,\n",
       "       1.        , 0.41960785, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01960784, 0.71372551,\n",
       "       1.        , 0.56862748, 0.        , 0.        , 0.        ,\n",
       "       0.15686275, 1.        , 0.32156864, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.69411767,\n",
       "       0.88627452, 0.01176471, 0.71372551, 0.89803922, 0.03137255,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.03137255, 0.74509805, 0.97254902,\n",
       "       0.22745098, 0.        , 0.        , 0.23137255, 1.        ,\n",
       "       0.24313726, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.29019609, 1.        , 0.51764709,\n",
       "       1.        , 0.44313726, 0.        , 0.04313726, 0.35686275,\n",
       "       0.01176471, 0.        , 0.        , 0.        , 0.44313726,\n",
       "       0.78431374, 0.05490196, 0.83137256, 0.90980393, 0.11764706,\n",
       "       0.        , 0.35686275, 1.        , 0.13725491, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.7019608 , 0.99607843, 0.98431373, 0.0627451 ,\n",
       "       0.        , 0.48235294, 1.        , 0.64313728, 0.        ,\n",
       "       0.        , 0.        , 0.93725491, 1.        , 0.39607844,\n",
       "       0.13333334, 0.92156863, 0.81176472, 0.04313726, 0.54901963,\n",
       "       0.94509804, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.02352941,\n",
       "       0.93725491, 1.        , 0.71764708, 0.        , 0.43529412,\n",
       "       1.        , 0.78823531, 0.        , 0.        , 0.03921569,\n",
       "       0.98431373, 0.99215686, 0.25098041, 0.        , 0.24705882,\n",
       "       0.98039216, 0.67843139, 0.74117649, 0.75686276, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313726, 1.        , 0.61960787,\n",
       "       0.52941179, 0.        , 0.04313726, 0.68235296, 0.20392157,\n",
       "       0.        , 0.        , 0.13333334, 0.99215686, 0.33333334,\n",
       "       0.        , 0.        , 0.        , 0.41176471, 1.        ,\n",
       "       0.95294118, 0.56470591, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.16470589, 1.        , 0.31764707, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05098039,\n",
       "       0.40784314, 0.67843139, 0.01568628, 0.        , 0.        ,\n",
       "       0.        , 0.00392157, 0.7764706 , 1.        , 0.38431373,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.25490198, 1.        ,\n",
       "       0.21568628, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.3882353 , 1.        , 0.63529414,\n",
       "       0.01176471, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.32156864, 1.        , 0.1882353 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.29411766, 1.        , 0.17647059, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.42745098, 1.        , 0.04705882, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.30588236, 1.        ,\n",
       "       0.17647059, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.33333334,\n",
       "       1.        , 0.13725491, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.44705883, 1.        ,\n",
       "       0.03137255, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.47843137, 0.98431373, 0.05098039, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.36862746, 1.        , 0.10196079,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.46666667, 1.        , 0.01176471, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00784314, 0.78039217,\n",
       "       0.79215688, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.40784314, 1.        , 0.06666667, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.48235294,\n",
       "       0.99607843, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.45490196, 1.        , 0.34901962, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.27843139, 1.        ,\n",
       "       0.32549021, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.19607843, 0.87843138, 0.8509804 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.33725491, 0.97647059,\n",
       "       0.63921571, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00392157, 0.83529413, 0.79215688, 0.01568628,\n",
       "       0.        , 0.        , 0.05098039, 0.56470591, 0.98823529,\n",
       "       0.88235295, 0.16470589, 0.        , 0.        , 0.        ,\n",
       "       0.45882353, 0.99215686, 0.73333335, 0.04313726, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.32156864, 0.99607843, 0.63529414, 0.00784314, 0.3137255 ,\n",
       "       0.89411765, 0.98431373, 0.54509807, 0.04313726, 0.        ,\n",
       "       0.        , 0.10980392, 0.6156863 , 1.        , 0.61960787,\n",
       "       0.01960784, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.43529412,\n",
       "       1.        , 0.81960785, 0.98823529, 0.80000001, 0.20784314,\n",
       "       0.08235294, 0.22352941, 0.36862746, 0.66274512, 0.98039216,\n",
       "       0.98039216, 0.48627451, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.32549021, 0.80784315,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.90588236, 0.50980395, 0.09803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21960784, 0.36470589,\n",
       "       0.44313726, 0.40784314, 0.26666668, 0.12156863, 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "def save_data(X_train, y_train, X_test, y_test, force = False):\n",
    "    \"\"\"\n",
    "    The function saves datasets to disk as pickle files.\n",
    "\n",
    "    INPUT:\n",
    "        X_train - train dataset\n",
    "        y_train - train dataset labels\n",
    "        X_test - test dataset\n",
    "        y_test - test dataset labels\n",
    "        force - forced saving of the files\n",
    "\n",
    "    OUTPUT: None\n",
    "    \"\"\"\n",
    "    print(\"Saving data \\n\")\n",
    "\n",
    "    # Check for already saved files\n",
    "    if not(path.exists('xtrain_doodle.pickle')) or force:\n",
    "        # Save X_train dataset as a pickle file\n",
    "        with open('xtrain_doodle.pickle', 'wb') as f:\n",
    "            pickle.dump(X_train, f)\n",
    "\n",
    "        # Save X_test dataset as a pickle file\n",
    "        with open('xtest_doodle.pickle', 'wb') as f:\n",
    "            pickle.dump(X_test, f)\n",
    "\n",
    "        # Save y_train dataset as a pickle file\n",
    "        with open('ytrain_doodle.pickle', 'wb') as f:\n",
    "            pickle.dump(y_train, f)\n",
    "\n",
    "        # Save y_test dataset as a pickle file\n",
    "        with open('ytest_doodle.pickle', 'wb') as f:\n",
    "            pickle.dump(y_test, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "save_data(X_train, y_train, X_test, y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving data \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "type(X_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "type(X_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "type(y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "len(X_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21000"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "len(X_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "def build_model(input_size, output_size, architecture = 'nn', dropout = 0.0):\n",
    "    if (architecture == 'nn'):\n",
    "        # Build a feed-forward network\n",
    "        model = nn.Sequential(OrderedDict([\n",
    "                              ('fc1', nn.Linear(input_size, 128)),\n",
    "                              ('relu1', nn.ReLU()),\n",
    "                              ('fc2', nn.Linear(128, 100)),\n",
    "                              ('bn2', nn.BatchNorm1d(num_features=100)),\n",
    "                              ('relu2', nn.ReLU()),\n",
    "                              ('dropout', nn.Dropout(dropout)),\n",
    "                              ('fc3', nn.Linear(100, 64)),\n",
    "                              ('bn3', nn.BatchNorm1d(num_features=64)),\n",
    "                              ('relu3', nn.ReLU()),\n",
    "                              ('logits', nn.Linear(64, output_size))]))\n",
    "    else:\n",
    "        if (architecture == 'conv'):\n",
    "            # Build a simple convolutional network\n",
    "            model = SimpleCNN(64, 10)\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "rnn_model = build_model(input_size=784, output_size=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "rnn_model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=100, bias=True)\n",
       "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (fc3): Linear(in_features=100, out_features=64, bias=True)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (logits): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "cnn_model = build_model(input_size = 784, output_size=10, architecture='conv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "cnn_model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(1, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3528, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "X_train = pickle.load(open('xtrain.pickle', 'rb'))\n",
    "X_test = pickle.load(open('xtest.pickle', 'rb'))\n",
    "y_train = pickle.load(open('ytrain.pickle', 'rb'))\n",
    "y_test = pickle.load(open('ytest.pickle', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "def shuffle(X_train, y_train):\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "    y_train_shuffled = y_train_shuffled.reshape((X_train.shape[0], 1))\n",
    "\n",
    "    X_train_shuffled = torch.from_numpy(X_train_shuffled).float()\n",
    "    y_train_shuffled = torch.from_numpy(y_train_shuffled).long()\n",
    "\n",
    "    return X_train_shuffled, y_train_shuffled"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "X_train_shuffled, y_train_shuffled = shuffle(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "type(X_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "type(y_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "def fit_model(model, X_train, y_train, epochs = 100, n_chunks = 1000, learning_rate = 0.003, weight_decay = 0, optimizer = 'SGD'):\n",
    "\n",
    "    print(\"Fitting model with epochs = {epochs}, learning rate = {lr}\\n\"\\\n",
    "    .format(epochs = epochs, lr = learning_rate))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if (optimizer == 'SGD'):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "\n",
    "    print_every = 100\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        images = torch.chunk(X_train, n_chunks)\n",
    "        labels = torch.chunk(y_train, n_chunks)\n",
    "\n",
    "        for i in range(n_chunks):\n",
    "            steps += 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward passes\n",
    "            output = model.forward(images[i])\n",
    "            loss = criterion(output, labels[i].squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                      \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "\n",
    "                running_loss = 0\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "fit_model(rnn_model, X_train_shuffled, y_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting model with epochs = 100, learning rate = 0.003\n",
      "\n",
      "Epoch: 1/100...  Loss: 2.2537\n",
      "Epoch: 1/100...  Loss: 2.0904\n",
      "Epoch: 1/100...  Loss: 1.9694\n",
      "Epoch: 1/100...  Loss: 1.8898\n",
      "Epoch: 1/100...  Loss: 1.8384\n",
      "Epoch: 1/100...  Loss: 1.7660\n",
      "Epoch: 1/100...  Loss: 1.7301\n",
      "Epoch: 1/100...  Loss: 1.7217\n",
      "Epoch: 1/100...  Loss: 1.6917\n",
      "Epoch: 1/100...  Loss: 1.6503\n",
      "Epoch: 2/100...  Loss: 1.6063\n",
      "Epoch: 2/100...  Loss: 1.6096\n",
      "Epoch: 2/100...  Loss: 1.5696\n",
      "Epoch: 2/100...  Loss: 1.5251\n",
      "Epoch: 2/100...  Loss: 1.5405\n",
      "Epoch: 2/100...  Loss: 1.4765\n",
      "Epoch: 2/100...  Loss: 1.4631\n",
      "Epoch: 2/100...  Loss: 1.4929\n",
      "Epoch: 2/100...  Loss: 1.4936\n",
      "Epoch: 2/100...  Loss: 1.4398\n",
      "Epoch: 3/100...  Loss: 1.4254\n",
      "Epoch: 3/100...  Loss: 1.4438\n",
      "Epoch: 3/100...  Loss: 1.4103\n",
      "Epoch: 3/100...  Loss: 1.3643\n",
      "Epoch: 3/100...  Loss: 1.3967\n",
      "Epoch: 3/100...  Loss: 1.3275\n",
      "Epoch: 3/100...  Loss: 1.3228\n",
      "Epoch: 3/100...  Loss: 1.3605\n",
      "Epoch: 3/100...  Loss: 1.3801\n",
      "Epoch: 3/100...  Loss: 1.3096\n",
      "Epoch: 4/100...  Loss: 1.3054\n",
      "Epoch: 4/100...  Loss: 1.3340\n",
      "Epoch: 4/100...  Loss: 1.2989\n",
      "Epoch: 4/100...  Loss: 1.2570\n",
      "Epoch: 4/100...  Loss: 1.2980\n",
      "Epoch: 4/100...  Loss: 1.2281\n",
      "Epoch: 4/100...  Loss: 1.2252\n",
      "Epoch: 4/100...  Loss: 1.2625\n",
      "Epoch: 4/100...  Loss: 1.2901\n",
      "Epoch: 4/100...  Loss: 1.2116\n",
      "Epoch: 5/100...  Loss: 1.2140\n",
      "Epoch: 5/100...  Loss: 1.2461\n",
      "Epoch: 5/100...  Loss: 1.2093\n",
      "Epoch: 5/100...  Loss: 1.1679\n",
      "Epoch: 5/100...  Loss: 1.2159\n",
      "Epoch: 5/100...  Loss: 1.1459\n",
      "Epoch: 5/100...  Loss: 1.1421\n",
      "Epoch: 5/100...  Loss: 1.1827\n",
      "Epoch: 5/100...  Loss: 1.2113\n",
      "Epoch: 5/100...  Loss: 1.1277\n",
      "Epoch: 6/100...  Loss: 1.1347\n",
      "Epoch: 6/100...  Loss: 1.1663\n",
      "Epoch: 6/100...  Loss: 1.1290\n",
      "Epoch: 6/100...  Loss: 1.0927\n",
      "Epoch: 6/100...  Loss: 1.1416\n",
      "Epoch: 6/100...  Loss: 1.0716\n",
      "Epoch: 6/100...  Loss: 1.0679\n",
      "Epoch: 6/100...  Loss: 1.1083\n",
      "Epoch: 6/100...  Loss: 1.1352\n",
      "Epoch: 6/100...  Loss: 1.0530\n",
      "Epoch: 7/100...  Loss: 1.0643\n",
      "Epoch: 7/100...  Loss: 1.0924\n",
      "Epoch: 7/100...  Loss: 1.0578\n",
      "Epoch: 7/100...  Loss: 1.0240\n",
      "Epoch: 7/100...  Loss: 1.0683\n",
      "Epoch: 7/100...  Loss: 1.0048\n",
      "Epoch: 7/100...  Loss: 0.9972\n",
      "Epoch: 7/100...  Loss: 1.0373\n",
      "Epoch: 7/100...  Loss: 1.0660\n",
      "Epoch: 7/100...  Loss: 0.9827\n",
      "Epoch: 8/100...  Loss: 0.9987\n",
      "Epoch: 8/100...  Loss: 1.0197\n",
      "Epoch: 8/100...  Loss: 0.9902\n",
      "Epoch: 8/100...  Loss: 0.9588\n",
      "Epoch: 8/100...  Loss: 0.9992\n",
      "Epoch: 8/100...  Loss: 0.9409\n",
      "Epoch: 8/100...  Loss: 0.9314\n",
      "Epoch: 8/100...  Loss: 0.9715\n",
      "Epoch: 8/100...  Loss: 0.9988\n",
      "Epoch: 8/100...  Loss: 0.9176\n",
      "Epoch: 9/100...  Loss: 0.9369\n",
      "Epoch: 9/100...  Loss: 0.9531\n",
      "Epoch: 9/100...  Loss: 0.9266\n",
      "Epoch: 9/100...  Loss: 0.8969\n",
      "Epoch: 9/100...  Loss: 0.9339\n",
      "Epoch: 9/100...  Loss: 0.8817\n",
      "Epoch: 9/100...  Loss: 0.8687\n",
      "Epoch: 9/100...  Loss: 0.9068\n",
      "Epoch: 9/100...  Loss: 0.9318\n",
      "Epoch: 9/100...  Loss: 0.8572\n",
      "Epoch: 10/100...  Loss: 0.8737\n",
      "Epoch: 10/100...  Loss: 0.8903\n",
      "Epoch: 10/100...  Loss: 0.8648\n",
      "Epoch: 10/100...  Loss: 0.8386\n",
      "Epoch: 10/100...  Loss: 0.8761\n",
      "Epoch: 10/100...  Loss: 0.8214\n",
      "Epoch: 10/100...  Loss: 0.8089\n",
      "Epoch: 10/100...  Loss: 0.8485\n",
      "Epoch: 10/100...  Loss: 0.8701\n",
      "Epoch: 10/100...  Loss: 0.8010\n",
      "Epoch: 11/100...  Loss: 0.8131\n",
      "Epoch: 11/100...  Loss: 0.8318\n",
      "Epoch: 11/100...  Loss: 0.8008\n",
      "Epoch: 11/100...  Loss: 0.7831\n",
      "Epoch: 11/100...  Loss: 0.8190\n",
      "Epoch: 11/100...  Loss: 0.7669\n",
      "Epoch: 11/100...  Loss: 0.7543\n",
      "Epoch: 11/100...  Loss: 0.7917\n",
      "Epoch: 11/100...  Loss: 0.8069\n",
      "Epoch: 11/100...  Loss: 0.7425\n",
      "Epoch: 12/100...  Loss: 0.7573\n",
      "Epoch: 12/100...  Loss: 0.7775\n",
      "Epoch: 12/100...  Loss: 0.7418\n",
      "Epoch: 12/100...  Loss: 0.7315\n",
      "Epoch: 12/100...  Loss: 0.7626\n",
      "Epoch: 12/100...  Loss: 0.7115\n",
      "Epoch: 12/100...  Loss: 0.7022\n",
      "Epoch: 12/100...  Loss: 0.7356\n",
      "Epoch: 12/100...  Loss: 0.7451\n",
      "Epoch: 12/100...  Loss: 0.6849\n",
      "Epoch: 13/100...  Loss: 0.7008\n",
      "Epoch: 13/100...  Loss: 0.7225\n",
      "Epoch: 13/100...  Loss: 0.6870\n",
      "Epoch: 13/100...  Loss: 0.6795\n",
      "Epoch: 13/100...  Loss: 0.7037\n",
      "Epoch: 13/100...  Loss: 0.6581\n",
      "Epoch: 13/100...  Loss: 0.6478\n",
      "Epoch: 13/100...  Loss: 0.6776\n",
      "Epoch: 13/100...  Loss: 0.6883\n",
      "Epoch: 13/100...  Loss: 0.6376\n",
      "Epoch: 14/100...  Loss: 0.6494\n",
      "Epoch: 14/100...  Loss: 0.6659\n",
      "Epoch: 14/100...  Loss: 0.6375\n",
      "Epoch: 14/100...  Loss: 0.6326\n",
      "Epoch: 14/100...  Loss: 0.6490\n",
      "Epoch: 14/100...  Loss: 0.6100\n",
      "Epoch: 14/100...  Loss: 0.6008\n",
      "Epoch: 14/100...  Loss: 0.6281\n",
      "Epoch: 14/100...  Loss: 0.6383\n",
      "Epoch: 14/100...  Loss: 0.5880\n",
      "Epoch: 15/100...  Loss: 0.5964\n",
      "Epoch: 15/100...  Loss: 0.6177\n",
      "Epoch: 15/100...  Loss: 0.5891\n",
      "Epoch: 15/100...  Loss: 0.5851\n",
      "Epoch: 15/100...  Loss: 0.5974\n",
      "Epoch: 15/100...  Loss: 0.5621\n",
      "Epoch: 15/100...  Loss: 0.5514\n",
      "Epoch: 15/100...  Loss: 0.5773\n",
      "Epoch: 15/100...  Loss: 0.5856\n",
      "Epoch: 15/100...  Loss: 0.5414\n",
      "Epoch: 16/100...  Loss: 0.5531\n",
      "Epoch: 16/100...  Loss: 0.5675\n",
      "Epoch: 16/100...  Loss: 0.5421\n",
      "Epoch: 16/100...  Loss: 0.5428\n",
      "Epoch: 16/100...  Loss: 0.5501\n",
      "Epoch: 16/100...  Loss: 0.5170\n",
      "Epoch: 16/100...  Loss: 0.5035\n",
      "Epoch: 16/100...  Loss: 0.5270\n",
      "Epoch: 16/100...  Loss: 0.5393\n",
      "Epoch: 16/100...  Loss: 0.4968\n",
      "Epoch: 17/100...  Loss: 0.5108\n",
      "Epoch: 17/100...  Loss: 0.5211\n",
      "Epoch: 17/100...  Loss: 0.4987\n",
      "Epoch: 17/100...  Loss: 0.4977\n",
      "Epoch: 17/100...  Loss: 0.5000\n",
      "Epoch: 17/100...  Loss: 0.4749\n",
      "Epoch: 17/100...  Loss: 0.4631\n",
      "Epoch: 17/100...  Loss: 0.4806\n",
      "Epoch: 17/100...  Loss: 0.4910\n",
      "Epoch: 17/100...  Loss: 0.4583\n",
      "Epoch: 18/100...  Loss: 0.4692\n",
      "Epoch: 18/100...  Loss: 0.4773\n",
      "Epoch: 18/100...  Loss: 0.4555\n",
      "Epoch: 18/100...  Loss: 0.4592\n",
      "Epoch: 18/100...  Loss: 0.4556\n",
      "Epoch: 18/100...  Loss: 0.4318\n",
      "Epoch: 18/100...  Loss: 0.4187\n",
      "Epoch: 18/100...  Loss: 0.4383\n",
      "Epoch: 18/100...  Loss: 0.4447\n",
      "Epoch: 18/100...  Loss: 0.4118\n",
      "Epoch: 19/100...  Loss: 0.4280\n",
      "Epoch: 19/100...  Loss: 0.4365\n",
      "Epoch: 19/100...  Loss: 0.4160\n",
      "Epoch: 19/100...  Loss: 0.4196\n",
      "Epoch: 19/100...  Loss: 0.4170\n",
      "Epoch: 19/100...  Loss: 0.3935\n",
      "Epoch: 19/100...  Loss: 0.3817\n",
      "Epoch: 19/100...  Loss: 0.3904\n",
      "Epoch: 19/100...  Loss: 0.4067\n",
      "Epoch: 19/100...  Loss: 0.3764\n",
      "Epoch: 20/100...  Loss: 0.3905\n",
      "Epoch: 20/100...  Loss: 0.3979\n",
      "Epoch: 20/100...  Loss: 0.3712\n",
      "Epoch: 20/100...  Loss: 0.3838\n",
      "Epoch: 20/100...  Loss: 0.3786\n",
      "Epoch: 20/100...  Loss: 0.3529\n",
      "Epoch: 20/100...  Loss: 0.3453\n",
      "Epoch: 20/100...  Loss: 0.3535\n",
      "Epoch: 20/100...  Loss: 0.3659\n",
      "Epoch: 20/100...  Loss: 0.3438\n",
      "Epoch: 21/100...  Loss: 0.3517\n",
      "Epoch: 21/100...  Loss: 0.3624\n",
      "Epoch: 21/100...  Loss: 0.3370\n",
      "Epoch: 21/100...  Loss: 0.3527\n",
      "Epoch: 21/100...  Loss: 0.3357\n",
      "Epoch: 21/100...  Loss: 0.3180\n",
      "Epoch: 21/100...  Loss: 0.3085\n",
      "Epoch: 21/100...  Loss: 0.3191\n",
      "Epoch: 21/100...  Loss: 0.3290\n",
      "Epoch: 21/100...  Loss: 0.3094\n",
      "Epoch: 22/100...  Loss: 0.3151\n",
      "Epoch: 22/100...  Loss: 0.3250\n",
      "Epoch: 22/100...  Loss: 0.2963\n",
      "Epoch: 22/100...  Loss: 0.3202\n",
      "Epoch: 22/100...  Loss: 0.2959\n",
      "Epoch: 22/100...  Loss: 0.2892\n",
      "Epoch: 22/100...  Loss: 0.2750\n",
      "Epoch: 22/100...  Loss: 0.2881\n",
      "Epoch: 22/100...  Loss: 0.2935\n",
      "Epoch: 22/100...  Loss: 0.2738\n",
      "Epoch: 23/100...  Loss: 0.2838\n",
      "Epoch: 23/100...  Loss: 0.2961\n",
      "Epoch: 23/100...  Loss: 0.2674\n",
      "Epoch: 23/100...  Loss: 0.2901\n",
      "Epoch: 23/100...  Loss: 0.2698\n",
      "Epoch: 23/100...  Loss: 0.2592\n",
      "Epoch: 23/100...  Loss: 0.2417\n",
      "Epoch: 23/100...  Loss: 0.2607\n",
      "Epoch: 23/100...  Loss: 0.2577\n",
      "Epoch: 23/100...  Loss: 0.2443\n",
      "Epoch: 24/100...  Loss: 0.2554\n",
      "Epoch: 24/100...  Loss: 0.2653\n",
      "Epoch: 24/100...  Loss: 0.2404\n",
      "Epoch: 24/100...  Loss: 0.2579\n",
      "Epoch: 24/100...  Loss: 0.2343\n",
      "Epoch: 24/100...  Loss: 0.2339\n",
      "Epoch: 24/100...  Loss: 0.2153\n",
      "Epoch: 24/100...  Loss: 0.2310\n",
      "Epoch: 24/100...  Loss: 0.2357\n",
      "Epoch: 24/100...  Loss: 0.2193\n",
      "Epoch: 25/100...  Loss: 0.2245\n",
      "Epoch: 25/100...  Loss: 0.2291\n",
      "Epoch: 25/100...  Loss: 0.2062\n",
      "Epoch: 25/100...  Loss: 0.2269\n",
      "Epoch: 25/100...  Loss: 0.2165\n",
      "Epoch: 25/100...  Loss: 0.2099\n",
      "Epoch: 25/100...  Loss: 0.1855\n",
      "Epoch: 25/100...  Loss: 0.1982\n",
      "Epoch: 25/100...  Loss: 0.2100\n",
      "Epoch: 25/100...  Loss: 0.1930\n",
      "Epoch: 26/100...  Loss: 0.1955\n",
      "Epoch: 26/100...  Loss: 0.2059\n",
      "Epoch: 26/100...  Loss: 0.1810\n",
      "Epoch: 26/100...  Loss: 0.2005\n",
      "Epoch: 26/100...  Loss: 0.1926\n",
      "Epoch: 26/100...  Loss: 0.1922\n",
      "Epoch: 26/100...  Loss: 0.1633\n",
      "Epoch: 26/100...  Loss: 0.1770\n",
      "Epoch: 26/100...  Loss: 0.1804\n",
      "Epoch: 26/100...  Loss: 0.1697\n",
      "Epoch: 27/100...  Loss: 0.1713\n",
      "Epoch: 27/100...  Loss: 0.1763\n",
      "Epoch: 27/100...  Loss: 0.1627\n",
      "Epoch: 27/100...  Loss: 0.1828\n",
      "Epoch: 27/100...  Loss: 0.1660\n",
      "Epoch: 27/100...  Loss: 0.1628\n",
      "Epoch: 27/100...  Loss: 0.1430\n",
      "Epoch: 27/100...  Loss: 0.1565\n",
      "Epoch: 27/100...  Loss: 0.1546\n",
      "Epoch: 27/100...  Loss: 0.1439\n",
      "Epoch: 28/100...  Loss: 0.1484\n",
      "Epoch: 28/100...  Loss: 0.1548\n",
      "Epoch: 28/100...  Loss: 0.1397\n",
      "Epoch: 28/100...  Loss: 0.1561\n",
      "Epoch: 28/100...  Loss: 0.1414\n",
      "Epoch: 28/100...  Loss: 0.1465\n",
      "Epoch: 28/100...  Loss: 0.1224\n",
      "Epoch: 28/100...  Loss: 0.1318\n",
      "Epoch: 28/100...  Loss: 0.1319\n",
      "Epoch: 28/100...  Loss: 0.1256\n",
      "Epoch: 29/100...  Loss: 0.1228\n",
      "Epoch: 29/100...  Loss: 0.1338\n",
      "Epoch: 29/100...  Loss: 0.1209\n",
      "Epoch: 29/100...  Loss: 0.1305\n",
      "Epoch: 29/100...  Loss: 0.1278\n",
      "Epoch: 29/100...  Loss: 0.1235\n",
      "Epoch: 29/100...  Loss: 0.1086\n",
      "Epoch: 29/100...  Loss: 0.1147\n",
      "Epoch: 29/100...  Loss: 0.1146\n",
      "Epoch: 29/100...  Loss: 0.1082\n",
      "Epoch: 30/100...  Loss: 0.1071\n",
      "Epoch: 30/100...  Loss: 0.1128\n",
      "Epoch: 30/100...  Loss: 0.1015\n",
      "Epoch: 30/100...  Loss: 0.1145\n",
      "Epoch: 30/100...  Loss: 0.1079\n",
      "Epoch: 30/100...  Loss: 0.1048\n",
      "Epoch: 30/100...  Loss: 0.0909\n",
      "Epoch: 30/100...  Loss: 0.1029\n",
      "Epoch: 30/100...  Loss: 0.0986\n",
      "Epoch: 30/100...  Loss: 0.0950\n",
      "Epoch: 31/100...  Loss: 0.0931\n",
      "Epoch: 31/100...  Loss: 0.0963\n",
      "Epoch: 31/100...  Loss: 0.0895\n",
      "Epoch: 31/100...  Loss: 0.0977\n",
      "Epoch: 31/100...  Loss: 0.0925\n",
      "Epoch: 31/100...  Loss: 0.0932\n",
      "Epoch: 31/100...  Loss: 0.0798\n",
      "Epoch: 31/100...  Loss: 0.0869\n",
      "Epoch: 31/100...  Loss: 0.0849\n",
      "Epoch: 31/100...  Loss: 0.0796\n",
      "Epoch: 32/100...  Loss: 0.0764\n",
      "Epoch: 32/100...  Loss: 0.0814\n",
      "Epoch: 32/100...  Loss: 0.0761\n",
      "Epoch: 32/100...  Loss: 0.0820\n",
      "Epoch: 32/100...  Loss: 0.0801\n",
      "Epoch: 32/100...  Loss: 0.0780\n",
      "Epoch: 32/100...  Loss: 0.0686\n",
      "Epoch: 32/100...  Loss: 0.0744\n",
      "Epoch: 32/100...  Loss: 0.0716\n",
      "Epoch: 32/100...  Loss: 0.0666\n",
      "Epoch: 33/100...  Loss: 0.0658\n",
      "Epoch: 33/100...  Loss: 0.0685\n",
      "Epoch: 33/100...  Loss: 0.0647\n",
      "Epoch: 33/100...  Loss: 0.0706\n",
      "Epoch: 33/100...  Loss: 0.0687\n",
      "Epoch: 33/100...  Loss: 0.0662\n",
      "Epoch: 33/100...  Loss: 0.0578\n",
      "Epoch: 33/100...  Loss: 0.0660\n",
      "Epoch: 33/100...  Loss: 0.0606\n",
      "Epoch: 33/100...  Loss: 0.0575\n",
      "Epoch: 34/100...  Loss: 0.0562\n",
      "Epoch: 34/100...  Loss: 0.0584\n",
      "Epoch: 34/100...  Loss: 0.0544\n",
      "Epoch: 34/100...  Loss: 0.0611\n",
      "Epoch: 34/100...  Loss: 0.0581\n",
      "Epoch: 34/100...  Loss: 0.0567\n",
      "Epoch: 34/100...  Loss: 0.0488\n",
      "Epoch: 34/100...  Loss: 0.0559\n",
      "Epoch: 34/100...  Loss: 0.0524\n",
      "Epoch: 34/100...  Loss: 0.0498\n",
      "Epoch: 35/100...  Loss: 0.0480\n",
      "Epoch: 35/100...  Loss: 0.0497\n",
      "Epoch: 35/100...  Loss: 0.0469\n",
      "Epoch: 35/100...  Loss: 0.0519\n",
      "Epoch: 35/100...  Loss: 0.0502\n",
      "Epoch: 35/100...  Loss: 0.0497\n",
      "Epoch: 35/100...  Loss: 0.0424\n",
      "Epoch: 35/100...  Loss: 0.0489\n",
      "Epoch: 35/100...  Loss: 0.0453\n",
      "Epoch: 35/100...  Loss: 0.0432\n",
      "Epoch: 36/100...  Loss: 0.0424\n",
      "Epoch: 36/100...  Loss: 0.0435\n",
      "Epoch: 36/100...  Loss: 0.0402\n",
      "Epoch: 36/100...  Loss: 0.0459\n",
      "Epoch: 36/100...  Loss: 0.0434\n",
      "Epoch: 36/100...  Loss: 0.0438\n",
      "Epoch: 36/100...  Loss: 0.0368\n",
      "Epoch: 36/100...  Loss: 0.0429\n",
      "Epoch: 36/100...  Loss: 0.0397\n",
      "Epoch: 36/100...  Loss: 0.0388\n",
      "Epoch: 37/100...  Loss: 0.0370\n",
      "Epoch: 37/100...  Loss: 0.0385\n",
      "Epoch: 37/100...  Loss: 0.0356\n",
      "Epoch: 37/100...  Loss: 0.0406\n",
      "Epoch: 37/100...  Loss: 0.0391\n",
      "Epoch: 37/100...  Loss: 0.0387\n",
      "Epoch: 37/100...  Loss: 0.0326\n",
      "Epoch: 37/100...  Loss: 0.0378\n",
      "Epoch: 37/100...  Loss: 0.0349\n",
      "Epoch: 37/100...  Loss: 0.0342\n",
      "Epoch: 38/100...  Loss: 0.0331\n",
      "Epoch: 38/100...  Loss: 0.0340\n",
      "Epoch: 38/100...  Loss: 0.0315\n",
      "Epoch: 38/100...  Loss: 0.0362\n",
      "Epoch: 38/100...  Loss: 0.0348\n",
      "Epoch: 38/100...  Loss: 0.0341\n",
      "Epoch: 38/100...  Loss: 0.0289\n",
      "Epoch: 38/100...  Loss: 0.0322\n",
      "Epoch: 38/100...  Loss: 0.0312\n",
      "Epoch: 38/100...  Loss: 0.0306\n",
      "Epoch: 39/100...  Loss: 0.0296\n",
      "Epoch: 39/100...  Loss: 0.0302\n",
      "Epoch: 39/100...  Loss: 0.0286\n",
      "Epoch: 39/100...  Loss: 0.0321\n",
      "Epoch: 39/100...  Loss: 0.0314\n",
      "Epoch: 39/100...  Loss: 0.0309\n",
      "Epoch: 39/100...  Loss: 0.0261\n",
      "Epoch: 39/100...  Loss: 0.0289\n",
      "Epoch: 39/100...  Loss: 0.0282\n",
      "Epoch: 39/100...  Loss: 0.0274\n",
      "Epoch: 40/100...  Loss: 0.0268\n",
      "Epoch: 40/100...  Loss: 0.0273\n",
      "Epoch: 40/100...  Loss: 0.0258\n",
      "Epoch: 40/100...  Loss: 0.0294\n",
      "Epoch: 40/100...  Loss: 0.0283\n",
      "Epoch: 40/100...  Loss: 0.0282\n",
      "Epoch: 40/100...  Loss: 0.0237\n",
      "Epoch: 40/100...  Loss: 0.0264\n",
      "Epoch: 40/100...  Loss: 0.0256\n",
      "Epoch: 40/100...  Loss: 0.0248\n",
      "Epoch: 41/100...  Loss: 0.0243\n",
      "Epoch: 41/100...  Loss: 0.0242\n",
      "Epoch: 41/100...  Loss: 0.0238\n",
      "Epoch: 41/100...  Loss: 0.0266\n",
      "Epoch: 41/100...  Loss: 0.0253\n",
      "Epoch: 41/100...  Loss: 0.0257\n",
      "Epoch: 41/100...  Loss: 0.0216\n",
      "Epoch: 41/100...  Loss: 0.0239\n",
      "Epoch: 41/100...  Loss: 0.0234\n",
      "Epoch: 41/100...  Loss: 0.0224\n",
      "Epoch: 42/100...  Loss: 0.0223\n",
      "Epoch: 42/100...  Loss: 0.0219\n",
      "Epoch: 42/100...  Loss: 0.0219\n",
      "Epoch: 42/100...  Loss: 0.0244\n",
      "Epoch: 42/100...  Loss: 0.0228\n",
      "Epoch: 42/100...  Loss: 0.0238\n",
      "Epoch: 42/100...  Loss: 0.0199\n",
      "Epoch: 42/100...  Loss: 0.0220\n",
      "Epoch: 42/100...  Loss: 0.0214\n",
      "Epoch: 42/100...  Loss: 0.0205\n",
      "Epoch: 43/100...  Loss: 0.0202\n",
      "Epoch: 43/100...  Loss: 0.0203\n",
      "Epoch: 43/100...  Loss: 0.0200\n",
      "Epoch: 43/100...  Loss: 0.0225\n",
      "Epoch: 43/100...  Loss: 0.0208\n",
      "Epoch: 43/100...  Loss: 0.0218\n",
      "Epoch: 43/100...  Loss: 0.0185\n",
      "Epoch: 43/100...  Loss: 0.0203\n",
      "Epoch: 43/100...  Loss: 0.0196\n",
      "Epoch: 43/100...  Loss: 0.0187\n",
      "Epoch: 44/100...  Loss: 0.0187\n",
      "Epoch: 44/100...  Loss: 0.0186\n",
      "Epoch: 44/100...  Loss: 0.0186\n",
      "Epoch: 44/100...  Loss: 0.0208\n",
      "Epoch: 44/100...  Loss: 0.0192\n",
      "Epoch: 44/100...  Loss: 0.0199\n",
      "Epoch: 44/100...  Loss: 0.0172\n",
      "Epoch: 44/100...  Loss: 0.0188\n",
      "Epoch: 44/100...  Loss: 0.0181\n",
      "Epoch: 44/100...  Loss: 0.0171\n",
      "Epoch: 45/100...  Loss: 0.0174\n",
      "Epoch: 45/100...  Loss: 0.0172\n",
      "Epoch: 45/100...  Loss: 0.0171\n",
      "Epoch: 45/100...  Loss: 0.0193\n",
      "Epoch: 45/100...  Loss: 0.0176\n",
      "Epoch: 45/100...  Loss: 0.0184\n",
      "Epoch: 45/100...  Loss: 0.0160\n",
      "Epoch: 45/100...  Loss: 0.0174\n",
      "Epoch: 45/100...  Loss: 0.0167\n",
      "Epoch: 45/100...  Loss: 0.0159\n",
      "Epoch: 46/100...  Loss: 0.0161\n",
      "Epoch: 46/100...  Loss: 0.0160\n",
      "Epoch: 46/100...  Loss: 0.0159\n",
      "Epoch: 46/100...  Loss: 0.0180\n",
      "Epoch: 46/100...  Loss: 0.0163\n",
      "Epoch: 46/100...  Loss: 0.0172\n",
      "Epoch: 46/100...  Loss: 0.0150\n",
      "Epoch: 46/100...  Loss: 0.0163\n",
      "Epoch: 46/100...  Loss: 0.0156\n",
      "Epoch: 46/100...  Loss: 0.0147\n",
      "Epoch: 47/100...  Loss: 0.0150\n",
      "Epoch: 47/100...  Loss: 0.0149\n",
      "Epoch: 47/100...  Loss: 0.0148\n",
      "Epoch: 47/100...  Loss: 0.0167\n",
      "Epoch: 47/100...  Loss: 0.0151\n",
      "Epoch: 47/100...  Loss: 0.0161\n",
      "Epoch: 47/100...  Loss: 0.0141\n",
      "Epoch: 47/100...  Loss: 0.0152\n",
      "Epoch: 47/100...  Loss: 0.0145\n",
      "Epoch: 47/100...  Loss: 0.0137\n",
      "Epoch: 48/100...  Loss: 0.0141\n",
      "Epoch: 48/100...  Loss: 0.0139\n",
      "Epoch: 48/100...  Loss: 0.0141\n",
      "Epoch: 48/100...  Loss: 0.0157\n",
      "Epoch: 48/100...  Loss: 0.0140\n",
      "Epoch: 48/100...  Loss: 0.0150\n",
      "Epoch: 48/100...  Loss: 0.0132\n",
      "Epoch: 48/100...  Loss: 0.0142\n",
      "Epoch: 48/100...  Loss: 0.0136\n",
      "Epoch: 48/100...  Loss: 0.0129\n",
      "Epoch: 49/100...  Loss: 0.0132\n",
      "Epoch: 49/100...  Loss: 0.0132\n",
      "Epoch: 49/100...  Loss: 0.0132\n",
      "Epoch: 49/100...  Loss: 0.0148\n",
      "Epoch: 49/100...  Loss: 0.0131\n",
      "Epoch: 49/100...  Loss: 0.0140\n",
      "Epoch: 49/100...  Loss: 0.0125\n",
      "Epoch: 49/100...  Loss: 0.0134\n",
      "Epoch: 49/100...  Loss: 0.0128\n",
      "Epoch: 49/100...  Loss: 0.0121\n",
      "Epoch: 50/100...  Loss: 0.0124\n",
      "Epoch: 50/100...  Loss: 0.0124\n",
      "Epoch: 50/100...  Loss: 0.0124\n",
      "Epoch: 50/100...  Loss: 0.0139\n",
      "Epoch: 50/100...  Loss: 0.0123\n",
      "Epoch: 50/100...  Loss: 0.0131\n",
      "Epoch: 50/100...  Loss: 0.0118\n",
      "Epoch: 50/100...  Loss: 0.0126\n",
      "Epoch: 50/100...  Loss: 0.0121\n",
      "Epoch: 50/100...  Loss: 0.0114\n",
      "Epoch: 51/100...  Loss: 0.0118\n",
      "Epoch: 51/100...  Loss: 0.0116\n",
      "Epoch: 51/100...  Loss: 0.0117\n",
      "Epoch: 51/100...  Loss: 0.0132\n",
      "Epoch: 51/100...  Loss: 0.0116\n",
      "Epoch: 51/100...  Loss: 0.0124\n",
      "Epoch: 51/100...  Loss: 0.0111\n",
      "Epoch: 51/100...  Loss: 0.0119\n",
      "Epoch: 51/100...  Loss: 0.0114\n",
      "Epoch: 51/100...  Loss: 0.0108\n",
      "Epoch: 52/100...  Loss: 0.0111\n",
      "Epoch: 52/100...  Loss: 0.0110\n",
      "Epoch: 52/100...  Loss: 0.0111\n",
      "Epoch: 52/100...  Loss: 0.0125\n",
      "Epoch: 52/100...  Loss: 0.0110\n",
      "Epoch: 52/100...  Loss: 0.0117\n",
      "Epoch: 52/100...  Loss: 0.0105\n",
      "Epoch: 52/100...  Loss: 0.0113\n",
      "Epoch: 52/100...  Loss: 0.0108\n",
      "Epoch: 52/100...  Loss: 0.0102\n",
      "Epoch: 53/100...  Loss: 0.0105\n",
      "Epoch: 53/100...  Loss: 0.0105\n",
      "Epoch: 53/100...  Loss: 0.0106\n",
      "Epoch: 53/100...  Loss: 0.0119\n",
      "Epoch: 53/100...  Loss: 0.0104\n",
      "Epoch: 53/100...  Loss: 0.0111\n",
      "Epoch: 53/100...  Loss: 0.0100\n",
      "Epoch: 53/100...  Loss: 0.0106\n",
      "Epoch: 53/100...  Loss: 0.0103\n",
      "Epoch: 53/100...  Loss: 0.0097\n",
      "Epoch: 54/100...  Loss: 0.0100\n",
      "Epoch: 54/100...  Loss: 0.0099\n",
      "Epoch: 54/100...  Loss: 0.0100\n",
      "Epoch: 54/100...  Loss: 0.0114\n",
      "Epoch: 54/100...  Loss: 0.0099\n",
      "Epoch: 54/100...  Loss: 0.0104\n",
      "Epoch: 54/100...  Loss: 0.0095\n",
      "Epoch: 54/100...  Loss: 0.0102\n",
      "Epoch: 54/100...  Loss: 0.0097\n",
      "Epoch: 54/100...  Loss: 0.0093\n",
      "Epoch: 55/100...  Loss: 0.0094\n",
      "Epoch: 55/100...  Loss: 0.0095\n",
      "Epoch: 55/100...  Loss: 0.0096\n",
      "Epoch: 55/100...  Loss: 0.0109\n",
      "Epoch: 55/100...  Loss: 0.0095\n",
      "Epoch: 55/100...  Loss: 0.0099\n",
      "Epoch: 55/100...  Loss: 0.0091\n",
      "Epoch: 55/100...  Loss: 0.0097\n",
      "Epoch: 55/100...  Loss: 0.0093\n",
      "Epoch: 55/100...  Loss: 0.0088\n",
      "Epoch: 56/100...  Loss: 0.0090\n",
      "Epoch: 56/100...  Loss: 0.0090\n",
      "Epoch: 56/100...  Loss: 0.0091\n",
      "Epoch: 56/100...  Loss: 0.0103\n",
      "Epoch: 56/100...  Loss: 0.0090\n",
      "Epoch: 56/100...  Loss: 0.0095\n",
      "Epoch: 56/100...  Loss: 0.0087\n",
      "Epoch: 56/100...  Loss: 0.0092\n",
      "Epoch: 56/100...  Loss: 0.0089\n",
      "Epoch: 56/100...  Loss: 0.0084\n",
      "Epoch: 57/100...  Loss: 0.0086\n",
      "Epoch: 57/100...  Loss: 0.0086\n",
      "Epoch: 57/100...  Loss: 0.0087\n",
      "Epoch: 57/100...  Loss: 0.0098\n",
      "Epoch: 57/100...  Loss: 0.0087\n",
      "Epoch: 57/100...  Loss: 0.0090\n",
      "Epoch: 57/100...  Loss: 0.0083\n",
      "Epoch: 57/100...  Loss: 0.0089\n",
      "Epoch: 57/100...  Loss: 0.0085\n",
      "Epoch: 57/100...  Loss: 0.0081\n",
      "Epoch: 58/100...  Loss: 0.0082\n",
      "Epoch: 58/100...  Loss: 0.0082\n",
      "Epoch: 58/100...  Loss: 0.0083\n",
      "Epoch: 58/100...  Loss: 0.0094\n",
      "Epoch: 58/100...  Loss: 0.0083\n",
      "Epoch: 58/100...  Loss: 0.0087\n",
      "Epoch: 58/100...  Loss: 0.0079\n",
      "Epoch: 58/100...  Loss: 0.0085\n",
      "Epoch: 58/100...  Loss: 0.0082\n",
      "Epoch: 58/100...  Loss: 0.0077\n",
      "Epoch: 59/100...  Loss: 0.0079\n",
      "Epoch: 59/100...  Loss: 0.0079\n",
      "Epoch: 59/100...  Loss: 0.0079\n",
      "Epoch: 59/100...  Loss: 0.0090\n",
      "Epoch: 59/100...  Loss: 0.0080\n",
      "Epoch: 59/100...  Loss: 0.0083\n",
      "Epoch: 59/100...  Loss: 0.0076\n",
      "Epoch: 59/100...  Loss: 0.0081\n",
      "Epoch: 59/100...  Loss: 0.0078\n",
      "Epoch: 59/100...  Loss: 0.0074\n",
      "Epoch: 60/100...  Loss: 0.0075\n",
      "Epoch: 60/100...  Loss: 0.0076\n",
      "Epoch: 60/100...  Loss: 0.0076\n",
      "Epoch: 60/100...  Loss: 0.0086\n",
      "Epoch: 60/100...  Loss: 0.0076\n",
      "Epoch: 60/100...  Loss: 0.0080\n",
      "Epoch: 60/100...  Loss: 0.0073\n",
      "Epoch: 60/100...  Loss: 0.0078\n",
      "Epoch: 60/100...  Loss: 0.0075\n",
      "Epoch: 60/100...  Loss: 0.0071\n",
      "Epoch: 61/100...  Loss: 0.0072\n",
      "Epoch: 61/100...  Loss: 0.0073\n",
      "Epoch: 61/100...  Loss: 0.0073\n",
      "Epoch: 61/100...  Loss: 0.0083\n",
      "Epoch: 61/100...  Loss: 0.0074\n",
      "Epoch: 61/100...  Loss: 0.0076\n",
      "Epoch: 61/100...  Loss: 0.0070\n",
      "Epoch: 61/100...  Loss: 0.0075\n",
      "Epoch: 61/100...  Loss: 0.0072\n",
      "Epoch: 61/100...  Loss: 0.0069\n",
      "Epoch: 62/100...  Loss: 0.0070\n",
      "Epoch: 62/100...  Loss: 0.0070\n",
      "Epoch: 62/100...  Loss: 0.0070\n",
      "Epoch: 62/100...  Loss: 0.0079\n",
      "Epoch: 62/100...  Loss: 0.0071\n",
      "Epoch: 62/100...  Loss: 0.0073\n",
      "Epoch: 62/100...  Loss: 0.0067\n",
      "Epoch: 62/100...  Loss: 0.0073\n",
      "Epoch: 62/100...  Loss: 0.0069\n",
      "Epoch: 62/100...  Loss: 0.0066\n",
      "Epoch: 63/100...  Loss: 0.0067\n",
      "Epoch: 63/100...  Loss: 0.0067\n",
      "Epoch: 63/100...  Loss: 0.0067\n",
      "Epoch: 63/100...  Loss: 0.0075\n",
      "Epoch: 63/100...  Loss: 0.0069\n",
      "Epoch: 63/100...  Loss: 0.0071\n",
      "Epoch: 63/100...  Loss: 0.0064\n",
      "Epoch: 63/100...  Loss: 0.0070\n",
      "Epoch: 63/100...  Loss: 0.0067\n",
      "Epoch: 63/100...  Loss: 0.0064\n",
      "Epoch: 64/100...  Loss: 0.0065\n",
      "Epoch: 64/100...  Loss: 0.0065\n",
      "Epoch: 64/100...  Loss: 0.0065\n",
      "Epoch: 64/100...  Loss: 0.0071\n",
      "Epoch: 64/100...  Loss: 0.0066\n",
      "Epoch: 64/100...  Loss: 0.0068\n",
      "Epoch: 64/100...  Loss: 0.0062\n",
      "Epoch: 64/100...  Loss: 0.0068\n",
      "Epoch: 64/100...  Loss: 0.0064\n",
      "Epoch: 64/100...  Loss: 0.0062\n",
      "Epoch: 65/100...  Loss: 0.0062\n",
      "Epoch: 65/100...  Loss: 0.0062\n",
      "Epoch: 65/100...  Loss: 0.0063\n",
      "Epoch: 65/100...  Loss: 0.0068\n",
      "Epoch: 65/100...  Loss: 0.0064\n",
      "Epoch: 65/100...  Loss: 0.0066\n",
      "Epoch: 65/100...  Loss: 0.0060\n",
      "Epoch: 65/100...  Loss: 0.0065\n",
      "Epoch: 65/100...  Loss: 0.0062\n",
      "Epoch: 65/100...  Loss: 0.0059\n",
      "Epoch: 66/100...  Loss: 0.0060\n",
      "Epoch: 66/100...  Loss: 0.0060\n",
      "Epoch: 66/100...  Loss: 0.0061\n",
      "Epoch: 66/100...  Loss: 0.0065\n",
      "Epoch: 66/100...  Loss: 0.0062\n",
      "Epoch: 66/100...  Loss: 0.0064\n",
      "Epoch: 66/100...  Loss: 0.0058\n",
      "Epoch: 66/100...  Loss: 0.0063\n",
      "Epoch: 66/100...  Loss: 0.0060\n",
      "Epoch: 66/100...  Loss: 0.0057\n",
      "Epoch: 67/100...  Loss: 0.0058\n",
      "Epoch: 67/100...  Loss: 0.0058\n",
      "Epoch: 67/100...  Loss: 0.0059\n",
      "Epoch: 67/100...  Loss: 0.0063\n",
      "Epoch: 67/100...  Loss: 0.0060\n",
      "Epoch: 67/100...  Loss: 0.0062\n",
      "Epoch: 67/100...  Loss: 0.0056\n",
      "Epoch: 67/100...  Loss: 0.0061\n",
      "Epoch: 67/100...  Loss: 0.0058\n",
      "Epoch: 67/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0056\n",
      "Epoch: 68/100...  Loss: 0.0057\n",
      "Epoch: 68/100...  Loss: 0.0060\n",
      "Epoch: 68/100...  Loss: 0.0058\n",
      "Epoch: 68/100...  Loss: 0.0060\n",
      "Epoch: 68/100...  Loss: 0.0054\n",
      "Epoch: 68/100...  Loss: 0.0059\n",
      "Epoch: 68/100...  Loss: 0.0057\n",
      "Epoch: 68/100...  Loss: 0.0054\n",
      "Epoch: 69/100...  Loss: 0.0055\n",
      "Epoch: 69/100...  Loss: 0.0055\n",
      "Epoch: 69/100...  Loss: 0.0055\n",
      "Epoch: 69/100...  Loss: 0.0058\n",
      "Epoch: 69/100...  Loss: 0.0056\n",
      "Epoch: 69/100...  Loss: 0.0058\n",
      "Epoch: 69/100...  Loss: 0.0052\n",
      "Epoch: 69/100...  Loss: 0.0058\n",
      "Epoch: 69/100...  Loss: 0.0055\n",
      "Epoch: 69/100...  Loss: 0.0052\n",
      "Epoch: 70/100...  Loss: 0.0053\n",
      "Epoch: 70/100...  Loss: 0.0053\n",
      "Epoch: 70/100...  Loss: 0.0053\n",
      "Epoch: 70/100...  Loss: 0.0056\n",
      "Epoch: 70/100...  Loss: 0.0054\n",
      "Epoch: 70/100...  Loss: 0.0056\n",
      "Epoch: 70/100...  Loss: 0.0050\n",
      "Epoch: 70/100...  Loss: 0.0056\n",
      "Epoch: 70/100...  Loss: 0.0053\n",
      "Epoch: 70/100...  Loss: 0.0050\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0055\n",
      "Epoch: 71/100...  Loss: 0.0053\n",
      "Epoch: 71/100...  Loss: 0.0054\n",
      "Epoch: 71/100...  Loss: 0.0049\n",
      "Epoch: 71/100...  Loss: 0.0054\n",
      "Epoch: 71/100...  Loss: 0.0051\n",
      "Epoch: 71/100...  Loss: 0.0049\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0053\n",
      "Epoch: 72/100...  Loss: 0.0051\n",
      "Epoch: 72/100...  Loss: 0.0053\n",
      "Epoch: 72/100...  Loss: 0.0047\n",
      "Epoch: 72/100...  Loss: 0.0053\n",
      "Epoch: 72/100...  Loss: 0.0050\n",
      "Epoch: 72/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0048\n",
      "Epoch: 73/100...  Loss: 0.0051\n",
      "Epoch: 73/100...  Loss: 0.0050\n",
      "Epoch: 73/100...  Loss: 0.0051\n",
      "Epoch: 73/100...  Loss: 0.0046\n",
      "Epoch: 73/100...  Loss: 0.0051\n",
      "Epoch: 73/100...  Loss: 0.0049\n",
      "Epoch: 73/100...  Loss: 0.0046\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0050\n",
      "Epoch: 74/100...  Loss: 0.0048\n",
      "Epoch: 74/100...  Loss: 0.0050\n",
      "Epoch: 74/100...  Loss: 0.0045\n",
      "Epoch: 74/100...  Loss: 0.0050\n",
      "Epoch: 74/100...  Loss: 0.0047\n",
      "Epoch: 74/100...  Loss: 0.0045\n",
      "Epoch: 75/100...  Loss: 0.0046\n",
      "Epoch: 75/100...  Loss: 0.0045\n",
      "Epoch: 75/100...  Loss: 0.0046\n",
      "Epoch: 75/100...  Loss: 0.0048\n",
      "Epoch: 75/100...  Loss: 0.0047\n",
      "Epoch: 75/100...  Loss: 0.0048\n",
      "Epoch: 75/100...  Loss: 0.0043\n",
      "Epoch: 75/100...  Loss: 0.0048\n",
      "Epoch: 75/100...  Loss: 0.0046\n",
      "Epoch: 75/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0045\n",
      "Epoch: 76/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0044\n",
      "Epoch: 76/100...  Loss: 0.0047\n",
      "Epoch: 76/100...  Loss: 0.0046\n",
      "Epoch: 76/100...  Loss: 0.0047\n",
      "Epoch: 76/100...  Loss: 0.0042\n",
      "Epoch: 76/100...  Loss: 0.0047\n",
      "Epoch: 76/100...  Loss: 0.0045\n",
      "Epoch: 76/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0043\n",
      "Epoch: 77/100...  Loss: 0.0045\n",
      "Epoch: 77/100...  Loss: 0.0045\n",
      "Epoch: 77/100...  Loss: 0.0046\n",
      "Epoch: 77/100...  Loss: 0.0041\n",
      "Epoch: 77/100...  Loss: 0.0046\n",
      "Epoch: 77/100...  Loss: 0.0044\n",
      "Epoch: 77/100...  Loss: 0.0041\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0044\n",
      "Epoch: 78/100...  Loss: 0.0044\n",
      "Epoch: 78/100...  Loss: 0.0045\n",
      "Epoch: 78/100...  Loss: 0.0040\n",
      "Epoch: 78/100...  Loss: 0.0044\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0040\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0043\n",
      "Epoch: 79/100...  Loss: 0.0042\n",
      "Epoch: 79/100...  Loss: 0.0043\n",
      "Epoch: 79/100...  Loss: 0.0039\n",
      "Epoch: 79/100...  Loss: 0.0043\n",
      "Epoch: 79/100...  Loss: 0.0042\n",
      "Epoch: 79/100...  Loss: 0.0039\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0042\n",
      "Epoch: 80/100...  Loss: 0.0041\n",
      "Epoch: 80/100...  Loss: 0.0042\n",
      "Epoch: 80/100...  Loss: 0.0038\n",
      "Epoch: 80/100...  Loss: 0.0042\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0038\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0041\n",
      "Epoch: 81/100...  Loss: 0.0040\n",
      "Epoch: 81/100...  Loss: 0.0041\n",
      "Epoch: 81/100...  Loss: 0.0037\n",
      "Epoch: 81/100...  Loss: 0.0041\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0037\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0040\n",
      "Epoch: 82/100...  Loss: 0.0039\n",
      "Epoch: 82/100...  Loss: 0.0040\n",
      "Epoch: 82/100...  Loss: 0.0036\n",
      "Epoch: 82/100...  Loss: 0.0040\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0036\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0039\n",
      "Epoch: 83/100...  Loss: 0.0039\n",
      "Epoch: 83/100...  Loss: 0.0039\n",
      "Epoch: 83/100...  Loss: 0.0035\n",
      "Epoch: 83/100...  Loss: 0.0039\n",
      "Epoch: 83/100...  Loss: 0.0038\n",
      "Epoch: 83/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0038\n",
      "Epoch: 84/100...  Loss: 0.0038\n",
      "Epoch: 84/100...  Loss: 0.0039\n",
      "Epoch: 84/100...  Loss: 0.0034\n",
      "Epoch: 84/100...  Loss: 0.0038\n",
      "Epoch: 84/100...  Loss: 0.0037\n",
      "Epoch: 84/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0036\n",
      "Epoch: 85/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0037\n",
      "Epoch: 85/100...  Loss: 0.0037\n",
      "Epoch: 85/100...  Loss: 0.0038\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 85/100...  Loss: 0.0037\n",
      "Epoch: 85/100...  Loss: 0.0036\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0035\n",
      "Epoch: 86/100...  Loss: 0.0035\n",
      "Epoch: 86/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0036\n",
      "Epoch: 86/100...  Loss: 0.0036\n",
      "Epoch: 86/100...  Loss: 0.0037\n",
      "Epoch: 86/100...  Loss: 0.0033\n",
      "Epoch: 86/100...  Loss: 0.0037\n",
      "Epoch: 86/100...  Loss: 0.0035\n",
      "Epoch: 86/100...  Loss: 0.0033\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0035\n",
      "Epoch: 87/100...  Loss: 0.0035\n",
      "Epoch: 87/100...  Loss: 0.0036\n",
      "Epoch: 87/100...  Loss: 0.0032\n",
      "Epoch: 87/100...  Loss: 0.0036\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0032\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0034\n",
      "Epoch: 88/100...  Loss: 0.0034\n",
      "Epoch: 88/100...  Loss: 0.0035\n",
      "Epoch: 88/100...  Loss: 0.0031\n",
      "Epoch: 88/100...  Loss: 0.0035\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0034\n",
      "Epoch: 89/100...  Loss: 0.0034\n",
      "Epoch: 89/100...  Loss: 0.0035\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 89/100...  Loss: 0.0034\n",
      "Epoch: 89/100...  Loss: 0.0033\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0032\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0033\n",
      "Epoch: 90/100...  Loss: 0.0033\n",
      "Epoch: 90/100...  Loss: 0.0034\n",
      "Epoch: 90/100...  Loss: 0.0030\n",
      "Epoch: 90/100...  Loss: 0.0033\n",
      "Epoch: 90/100...  Loss: 0.0032\n",
      "Epoch: 90/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0032\n",
      "Epoch: 91/100...  Loss: 0.0032\n",
      "Epoch: 91/100...  Loss: 0.0033\n",
      "Epoch: 91/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0033\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0031\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0031\n",
      "Epoch: 92/100...  Loss: 0.0032\n",
      "Epoch: 92/100...  Loss: 0.0032\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 92/100...  Loss: 0.0032\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0030\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0031\n",
      "Epoch: 93/100...  Loss: 0.0031\n",
      "Epoch: 93/100...  Loss: 0.0032\n",
      "Epoch: 93/100...  Loss: 0.0028\n",
      "Epoch: 93/100...  Loss: 0.0031\n",
      "Epoch: 93/100...  Loss: 0.0030\n",
      "Epoch: 93/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0030\n",
      "Epoch: 94/100...  Loss: 0.0030\n",
      "Epoch: 94/100...  Loss: 0.0031\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0031\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0029\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0029\n",
      "Epoch: 95/100...  Loss: 0.0030\n",
      "Epoch: 95/100...  Loss: 0.0031\n",
      "Epoch: 95/100...  Loss: 0.0027\n",
      "Epoch: 95/100...  Loss: 0.0030\n",
      "Epoch: 95/100...  Loss: 0.0029\n",
      "Epoch: 95/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0029\n",
      "Epoch: 96/100...  Loss: 0.0029\n",
      "Epoch: 96/100...  Loss: 0.0030\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0030\n",
      "Epoch: 96/100...  Loss: 0.0028\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0028\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0028\n",
      "Epoch: 97/100...  Loss: 0.0029\n",
      "Epoch: 97/100...  Loss: 0.0029\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 97/100...  Loss: 0.0029\n",
      "Epoch: 97/100...  Loss: 0.0028\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0028\n",
      "Epoch: 98/100...  Loss: 0.0028\n",
      "Epoch: 98/100...  Loss: 0.0029\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0028\n",
      "Epoch: 98/100...  Loss: 0.0027\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0027\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0027\n",
      "Epoch: 99/100...  Loss: 0.0028\n",
      "Epoch: 99/100...  Loss: 0.0028\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 99/100...  Loss: 0.0028\n",
      "Epoch: 99/100...  Loss: 0.0027\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0027\n",
      "Epoch: 100/100...  Loss: 0.0027\n",
      "Epoch: 100/100...  Loss: 0.0028\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0027\n",
      "Epoch: 100/100...  Loss: 0.0026\n",
      "Epoch: 100/100...  Loss: 0.0025\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "def load_model(architecture = 'nn', filepath = 'checkpoint.pth'):\n",
    "    \"\"\"\n",
    "    Function loads the model from checkpoint.\n",
    "\n",
    "    INPUT:\n",
    "        architecture - model architecture ('nn' - for fully connected neural network, 'conv' - for convolutional neural\n",
    "        network)\n",
    "        filepath - path for the saved model\n",
    "\n",
    "    OUTPUT:\n",
    "        model - loaded pytorch model\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading model from {} \\n\".format(filepath))\n",
    "\n",
    "    if architecture == 'nn':\n",
    "        checkpoint = torch.load(filepath)\n",
    "        input_size = checkpoint['input_size']\n",
    "        output_size = checkpoint['output_size']\n",
    "        hidden_sizes = checkpoint['hidden_layers']\n",
    "        dropout = checkpoint['dropout']\n",
    "        model = nn.Sequential(OrderedDict([\n",
    "                              ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                              ('relu1', nn.ReLU()),\n",
    "                              ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                              ('bn2', nn.BatchNorm1d(num_features=hidden_sizes[1])),\n",
    "                              ('relu2', nn.ReLU()),\n",
    "                              ('dropout', nn.Dropout(dropout)),\n",
    "                              ('fc3', nn.Linear(hidden_sizes[1], hidden_sizes[2])),\n",
    "                              ('bn3', nn.BatchNorm1d(num_features=hidden_sizes[2])),\n",
    "                              ('relu3', nn.ReLU()),\n",
    "                              ('logits', nn.Linear(hidden_sizes[2], output_size))]))\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    else:\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model = SimpleCNN()\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "model = load_model(architecture = 'nn', filepath = 'checkpoint.pth')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading model from checkpoint.pth \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "def evaluate_model(model, train, y_train, test, y_test, architecture = 'nn'):\n",
    "    \"\"\"\n",
    "    Function to print out train and test accuracy of the model.\n",
    "\n",
    "    INPUT:\n",
    "        model - pytorch model\n",
    "        train - (tensor) train dataset\n",
    "        y_train - (numpy) labels for train dataset\n",
    "        test - (tensor) test dataset\n",
    "        y_test - (numpy) labels for test dataset\n",
    "\n",
    "    OUTPUT:\n",
    "        accuracy_train - accuracy on train dataset\n",
    "        accuracy_test - accuracy on test dataset\n",
    "    \"\"\"\n",
    "    train_pred = get_preds(model, train, architecture)\n",
    "    train_pred_labels = get_labels(train_pred)\n",
    "\n",
    "    test_pred = get_preds(model, test, architecture)\n",
    "    test_pred_labels = get_labels(test_pred)\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, train_pred_labels)\n",
    "    accuracy_test = accuracy_score(y_test, test_pred_labels)\n",
    "\n",
    "    print(\"Accuracy score for train set is {} \\n\".format(accuracy_train))\n",
    "    print(\"Accuracy score for test set is {} \\n\".format(accuracy_test))\n",
    "\n",
    "    return accuracy_train, accuracy_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "evaluate_model(model, X_train_shuffled, y_train_shuffled, X_test, y_test, architecture='nn')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X_test_shuffled' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-69ce99b75209>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_shuffled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_shuffled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test_shuffled' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "evaluate_model"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('dl-env': conda)"
  },
  "interpreter": {
   "hash": "2aac1d2b359798efc30b59d804167ffd538e3092f84ffa86abb6645ab952d97e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}