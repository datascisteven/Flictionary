{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import linear algebra and data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import matplotlib for plotting\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import helper libraries\n",
    "import requests\n",
    "from io import BytesIO # Use When expecting bytes-like objects\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from os import path\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "# import PIL for image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import image_utils\n",
    "from image_utils import add_flipped_and_rotated_images\n",
    "from simple_conv_nn import SimpleCNN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def load_data():\n",
    "    print(\"Loading data \\n\")\n",
    "\n",
    "    # Check for already loaded datasets\n",
    "    if not(path.exists('xtrain_doodle.pickle')):\n",
    "        # Load from web\n",
    "        print(\"Loading data from the web \\n\")\n",
    "\n",
    "        # Classes we will load\n",
    "        categories = ['bee', 'cat', 'cow', 'dog', 'duck', 'horse', 'pig', 'rabbit', 'snake', 'whale']\n",
    "\n",
    "        # Dictionary for URL and class labels\n",
    "        URL_DATA = {}\n",
    "        for category in categories:\n",
    "            URL_DATA[category] = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/' + category +'.npy'\n",
    "\n",
    "        # Load data for classes in dictionary\n",
    "        classes_dict = {}\n",
    "        for key, value in URL_DATA.items():\n",
    "            response = requests.get(value)\n",
    "            classes_dict[key] = np.load(BytesIO(response.content))\n",
    "\n",
    "        # Generate labels and add labels to loaded data\n",
    "        for i, (key, value) in enumerate(classes_dict.items()):\n",
    "            value = value.astype('float32')/255.\n",
    "            if i == 0:\n",
    "                classes_dict[key] = np.c_[value, np.zeros(len(value))]\n",
    "            else:\n",
    "                classes_dict[key] = np.c_[value,i*np.ones(len(value))]\n",
    "\n",
    "        # Create a dict with label codes\n",
    "        label_dict = {0:'bee', 1:'cat', 2:'cow', 3:'dog', 4:'duck',\n",
    "                      5:'horse', 6:'pig', 7:'rabbit', 8:'snake', 9:'whale'}\n",
    "\n",
    "        lst = []\n",
    "        for key, value in classes_dict.items():\n",
    "            lst.append(value[:3000])\n",
    "        doodles = np.concatenate(lst)\n",
    "\n",
    "        # Split the data into features and class labels (X & y respectively)\n",
    "        y = doodles[:,-1].astype('float32')\n",
    "        X = doodles[:,:784]\n",
    "\n",
    "        # Split each dataset into train/test splits\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "    else:\n",
    "        # Load data from pickle files\n",
    "        print(\"Loading data from pickle files \\n\")\n",
    "\n",
    "        file = open(\"xtrain_doodle.pickle\",'rb')\n",
    "        X_train = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"xtest_doodle.pickle\",'rb')\n",
    "        X_test = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"ytrain_doodle.pickle\",'rb')\n",
    "        y_train = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"ytest_doodle.pickle\",'rb')\n",
    "        y_test = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, classes_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "categories = ['bee', 'cat', 'cow', 'dog', 'duck', 'horse', 'pig', 'rabbit', 'snake', 'whale']\n",
    "\n",
    "URL_DATA = {}\n",
    "for category in categories:\n",
    "    URL_DATA[category] = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/' + category +'.npy'\n",
    "\n",
    "URL_DATA"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bee': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bee.npy',\n",
       " 'cat': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cat.npy',\n",
       " 'cow': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cow.npy',\n",
       " 'dog': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy',\n",
       " 'duck': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/duck.npy',\n",
       " 'horse': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/horse.npy',\n",
       " 'pig': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pig.npy',\n",
       " 'rabbit': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rabbit.npy',\n",
       " 'snake': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/snake.npy',\n",
       " 'whale': 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/whale.npy'}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "classes_dict = {}\n",
    "for key, value in URL_DATA.items():\n",
    "    response = requests.get(value)\n",
    "    classes_dict[key] = np.load(BytesIO(response.content))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "classes_dict"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bee': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'cat': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'cow': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'dog': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'duck': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'horse': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'pig': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'rabbit': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'snake': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'whale': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "len(classes_dict['dog'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "152159"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "len(classes_dict['cat'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "123202"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Generate labels and add labels to loaded data\n",
    "for i, (key, value) in enumerate(classes_dict.items()):\n",
    "    value = value.astype('float32')/255.\n",
    "    if i == 0:\n",
    "        classes_dict[key] = np.c_[value, np.zeros(len(value))]\n",
    "    else:\n",
    "        classes_dict[key] = np.c_[value,i*np.ones(len(value))]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "classes_dict['bee']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "classes_dict['bee'][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.33725491,\n",
       "       0.40000001, 0.40000001, 0.21960784, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00784314,\n",
       "       0.5411765 , 0.99607843, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.79215688, 0.23529412, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.34509805, 1.        , 0.5529412 ,\n",
       "       0.13333334, 0.09019608, 0.07843138, 0.38039216, 0.84313726,\n",
       "       0.96078432, 0.16078432, 0.        , 0.12156863, 0.23529412,\n",
       "       0.34117648, 0.1882353 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.72156864, 0.8509804 , 0.00392157, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.92941177, 0.66274512,\n",
       "       0.7019608 , 1.        , 1.        , 1.        , 0.97647059,\n",
       "       0.30980393, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10588235, 0.98823529, 0.48235294,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00392157, 0.92156863, 0.99215686, 0.9254902 , 0.4509804 ,\n",
       "       0.24313726, 0.16470589, 0.75294119, 0.97647059, 0.21176471,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.46666667, 0.99215686, 0.11764706, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.36078432, 1.        ,\n",
       "       0.80784315, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.10980392, 1.        , 0.39215687, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.76862746, 0.75686276,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01176471, 0.90980393, 1.        , 0.17254902, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 1.        ,\n",
       "       0.39215687, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.9254902 , 0.56078434, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.41176471, 1.        ,\n",
       "       0.80392158, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00784314, 0.69411767, 0.97647059, 0.18039216, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08235294, 1.        ,\n",
       "       0.40392157, 0.        , 0.12156863, 0.33725491, 0.40000001,\n",
       "       0.56862748, 0.98039216, 1.        , 0.46666667, 0.        ,\n",
       "       0.        , 0.        , 0.00784314, 0.56078434, 1.        ,\n",
       "       0.3764706 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.24313726, 1.        , 0.51764709, 0.72156864,\n",
       "       0.97254902, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.98039216, 0.95294118, 0.70980394, 0.29411766,\n",
       "       0.78431374, 1.        , 0.52941179, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.39607844,\n",
       "       1.        , 0.96078432, 1.        , 0.86274511, 0.94509804,\n",
       "       0.7647059 , 1.        , 0.74117649, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99607843, 0.9254902 , 0.32941177,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32549021, 0.96470588, 1.        ,\n",
       "       0.627451  , 0.89019608, 0.64313728, 0.88627452, 1.        ,\n",
       "       0.44313726, 0.28235295, 0.90588236, 1.        , 0.92941177,\n",
       "       0.93333334, 1.        , 0.67450982, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.61960787, 0.96862745, 1.        , 0.34901962, 1.        ,\n",
       "       0.32941177, 0.74901962, 1.        , 0.29019609, 0.12156863,\n",
       "       0.98431373, 1.        , 0.43921569, 0.00392157, 0.56470591,\n",
       "       1.        , 0.04705882, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.9137255 , 0.8509804 ,\n",
       "       0.90980393, 0.49803922, 0.97254902, 0.09803922, 0.98039216,\n",
       "       1.        , 0.07843138, 0.54509807, 0.97254902, 1.        ,\n",
       "       0.47843137, 0.6901961 , 0.67058825, 0.96078432, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.64313728, 0.49019608, 0.36470589, 0.16862746, 0.00784314,\n",
       "       0.05490196, 1.        , 0.93333334, 0.63529414, 0.70588237,\n",
       "       0.79215688, 0.36078432, 1.        , 0.87450981, 0.03921569,\n",
       "       0.92941177, 0.66274512, 1.        , 0.85882354, 0.96862745,\n",
       "       0.88235295, 0.77254903, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.18039216, 0.80392158, 0.9137255 ,\n",
       "       1.        , 1.        , 0.94509804, 0.65882355, 1.        ,\n",
       "       1.        , 0.36862746, 0.89019608, 0.60392159, 0.69411767,\n",
       "       0.98431373, 0.61960787, 0.41960785, 1.        , 0.37254903,\n",
       "       1.        , 0.83529413, 0.76862746, 0.99607843, 0.44705883,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 0.33725491,\n",
       "       0.55686277, 0.59607846, 1.        , 1.        , 0.27058825,\n",
       "       1.        , 0.4509804 , 0.96862745, 1.        , 0.44705883,\n",
       "       0.84313726, 0.74901962, 0.3137255 , 1.        , 0.26666668,\n",
       "       0.52156866, 0.99607843, 0.12156863, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.18431373,\n",
       "       1.        , 1.        , 0.36862746, 1.        , 0.50980395,\n",
       "       1.        , 1.        , 0.52156866, 1.        , 0.3137255 ,\n",
       "       0.41568628, 1.        , 0.07058824, 0.76862746, 0.78431374,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10980392, 0.98039216, 1.        ,\n",
       "       0.49019608, 0.99607843, 0.65098041, 0.9137255 , 1.        ,\n",
       "       0.76078433, 0.87058824, 0.01176471, 0.51372552, 0.96862745,\n",
       "       0.13333334, 0.99215686, 0.4509804 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.65490198, 1.        , 0.8509804 , 0.87058824,\n",
       "       0.85490197, 0.79215688, 1.        , 0.96470588, 0.53333336,\n",
       "       0.        , 0.6156863 , 0.86666667, 0.74509805, 0.93725491,\n",
       "       0.09411765, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.52941179,\n",
       "       0.97254902, 1.        , 0.94117647, 0.99607843, 0.73333335,\n",
       "       0.96862745, 1.        , 0.32549021, 0.        , 0.72941178,\n",
       "       0.90588236, 1.        , 0.34117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.60000002, 0.88235295, 0.98431373,\n",
       "       0.96862745, 1.        , 0.98823529, 0.98431373, 1.        ,\n",
       "       0.89803922, 0.81960785, 0.98431373, 0.99607843, 0.50980395,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.6156863 , 0.78823531, 0.77254903, 0.60784316, 0.97647059,\n",
       "       0.52549022, 0.66274512, 1.        , 0.66274512, 0.66666669,\n",
       "       0.93725491, 0.61176473, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.05882353, 0.09411765,\n",
       "       0.        , 0.10588235, 0.26666668, 0.        , 0.01568628,\n",
       "       0.24705882, 0.        , 0.        , 0.05882353, 0.01568628,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "label_dict = {0:'bee', 1:'cat', 2:'cow', 3:'dog', 4:'duck',\n",
    "              5:'horse', 6:'pig', 7:'rabbit', 8:'snake', 9:'whale'}\n",
    "\n",
    "lst = []\n",
    "for key, value in classes_dict.items():\n",
    "    lst.append(value[:50000])\n",
    "doodles = np.concatenate(lst)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "len(doodles)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "doodles[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.33725491,\n",
       "       0.40000001, 0.40000001, 0.21960784, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00784314,\n",
       "       0.5411765 , 0.99607843, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.79215688, 0.23529412, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.34509805, 1.        , 0.5529412 ,\n",
       "       0.13333334, 0.09019608, 0.07843138, 0.38039216, 0.84313726,\n",
       "       0.96078432, 0.16078432, 0.        , 0.12156863, 0.23529412,\n",
       "       0.34117648, 0.1882353 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.72156864, 0.8509804 , 0.00392157, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.92941177, 0.66274512,\n",
       "       0.7019608 , 1.        , 1.        , 1.        , 0.97647059,\n",
       "       0.30980393, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10588235, 0.98823529, 0.48235294,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00392157, 0.92156863, 0.99215686, 0.9254902 , 0.4509804 ,\n",
       "       0.24313726, 0.16470589, 0.75294119, 0.97647059, 0.21176471,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.46666667, 0.99215686, 0.11764706, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.36078432, 1.        ,\n",
       "       0.80784315, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.10980392, 1.        , 0.39215687, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.76862746, 0.75686276,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01176471, 0.90980393, 1.        , 0.17254902, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 1.        ,\n",
       "       0.39215687, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.9254902 , 0.56078434, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.41176471, 1.        ,\n",
       "       0.80392158, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00784314, 0.69411767, 0.97647059, 0.18039216, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08235294, 1.        ,\n",
       "       0.40392157, 0.        , 0.12156863, 0.33725491, 0.40000001,\n",
       "       0.56862748, 0.98039216, 1.        , 0.46666667, 0.        ,\n",
       "       0.        , 0.        , 0.00784314, 0.56078434, 1.        ,\n",
       "       0.3764706 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.24313726, 1.        , 0.51764709, 0.72156864,\n",
       "       0.97254902, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.98039216, 0.95294118, 0.70980394, 0.29411766,\n",
       "       0.78431374, 1.        , 0.52941179, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.39607844,\n",
       "       1.        , 0.96078432, 1.        , 0.86274511, 0.94509804,\n",
       "       0.7647059 , 1.        , 0.74117649, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99607843, 0.9254902 , 0.32941177,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32549021, 0.96470588, 1.        ,\n",
       "       0.627451  , 0.89019608, 0.64313728, 0.88627452, 1.        ,\n",
       "       0.44313726, 0.28235295, 0.90588236, 1.        , 0.92941177,\n",
       "       0.93333334, 1.        , 0.67450982, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.61960787, 0.96862745, 1.        , 0.34901962, 1.        ,\n",
       "       0.32941177, 0.74901962, 1.        , 0.29019609, 0.12156863,\n",
       "       0.98431373, 1.        , 0.43921569, 0.00392157, 0.56470591,\n",
       "       1.        , 0.04705882, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.9137255 , 0.8509804 ,\n",
       "       0.90980393, 0.49803922, 0.97254902, 0.09803922, 0.98039216,\n",
       "       1.        , 0.07843138, 0.54509807, 0.97254902, 1.        ,\n",
       "       0.47843137, 0.6901961 , 0.67058825, 0.96078432, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.64313728, 0.49019608, 0.36470589, 0.16862746, 0.00784314,\n",
       "       0.05490196, 1.        , 0.93333334, 0.63529414, 0.70588237,\n",
       "       0.79215688, 0.36078432, 1.        , 0.87450981, 0.03921569,\n",
       "       0.92941177, 0.66274512, 1.        , 0.85882354, 0.96862745,\n",
       "       0.88235295, 0.77254903, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.18039216, 0.80392158, 0.9137255 ,\n",
       "       1.        , 1.        , 0.94509804, 0.65882355, 1.        ,\n",
       "       1.        , 0.36862746, 0.89019608, 0.60392159, 0.69411767,\n",
       "       0.98431373, 0.61960787, 0.41960785, 1.        , 0.37254903,\n",
       "       1.        , 0.83529413, 0.76862746, 0.99607843, 0.44705883,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10980392, 0.33725491,\n",
       "       0.55686277, 0.59607846, 1.        , 1.        , 0.27058825,\n",
       "       1.        , 0.4509804 , 0.96862745, 1.        , 0.44705883,\n",
       "       0.84313726, 0.74901962, 0.3137255 , 1.        , 0.26666668,\n",
       "       0.52156866, 0.99607843, 0.12156863, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.18431373,\n",
       "       1.        , 1.        , 0.36862746, 1.        , 0.50980395,\n",
       "       1.        , 1.        , 0.52156866, 1.        , 0.3137255 ,\n",
       "       0.41568628, 1.        , 0.07058824, 0.76862746, 0.78431374,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.10980392, 0.98039216, 1.        ,\n",
       "       0.49019608, 0.99607843, 0.65098041, 0.9137255 , 1.        ,\n",
       "       0.76078433, 0.87058824, 0.01176471, 0.51372552, 0.96862745,\n",
       "       0.13333334, 0.99215686, 0.4509804 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.65490198, 1.        , 0.8509804 , 0.87058824,\n",
       "       0.85490197, 0.79215688, 1.        , 0.96470588, 0.53333336,\n",
       "       0.        , 0.6156863 , 0.86666667, 0.74509805, 0.93725491,\n",
       "       0.09411765, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.52941179,\n",
       "       0.97254902, 1.        , 0.94117647, 0.99607843, 0.73333335,\n",
       "       0.96862745, 1.        , 0.32549021, 0.        , 0.72941178,\n",
       "       0.90588236, 1.        , 0.34117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.60000002, 0.88235295, 0.98431373,\n",
       "       0.96862745, 1.        , 0.98823529, 0.98431373, 1.        ,\n",
       "       0.89803922, 0.81960785, 0.98431373, 0.99607843, 0.50980395,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.6156863 , 0.78823531, 0.77254903, 0.60784316, 0.97647059,\n",
       "       0.52549022, 0.66274512, 1.        , 0.66274512, 0.66666669,\n",
       "       0.93725491, 0.61176473, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.05882353, 0.09411765,\n",
       "       0.        , 0.10588235, 0.26666668, 0.        , 0.01568628,\n",
       "       0.24705882, 0.        , 0.        , 0.05882353, 0.01568628,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "y = doodles[:,-1].astype('float32')\n",
    "X = doodles[:,:784]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 9., 9., 9.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "len(y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "X_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "len(X_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "350000"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "X_train[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00784314, 0.06666667, 0.09411765, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09803922, 0.60392159, 0.95686275, 1.        ,\n",
       "       1.        , 0.34117648, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.25098041, 0.93333334,\n",
       "       0.99215686, 0.93725491, 0.41176471, 1.        , 0.44705883,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.11764706, 0.96862745, 0.85882354, 0.65098041, 0.02745098,\n",
       "       0.01960784, 0.99215686, 0.48627451, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.0627451 , 0.29803923, 0.42352942, 0.5411765 ,\n",
       "       0.65490198, 0.77254903, 0.85882354, 0.71372551, 0.51372552,\n",
       "       0.3137255 , 0.11764706, 0.14117648, 0.69803923, 0.97254902,\n",
       "       0.48235294, 0.58039218, 0.4627451 , 0.5529412 , 1.        ,\n",
       "       0.25490198, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.57254905,\n",
       "       1.        , 1.        , 0.94509804, 0.82745099, 0.7019608 ,\n",
       "       0.627451  , 0.79215688, 0.97254902, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.9137255 ,\n",
       "       0.99607843, 1.        , 0.92941177, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.05490196, 0.33333334, 0.09803922,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01960784, 0.19215687, 0.37254903, 0.92941177, 0.8392157 ,\n",
       "       0.3019608 , 0.07058824, 0.        , 0.05098039, 0.74901962,\n",
       "       1.        , 0.26274511, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.03921569, 0.97647059, 0.41960785, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09803922, 1.        , 0.4627451 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.09411765, 1.        , 0.38039216,\n",
       "       0.        , 0.37254903, 0.81568629, 0.90980393, 0.32156864,\n",
       "       0.        , 0.        , 0.        , 0.20784314, 1.        ,\n",
       "       0.31764707, 0.        , 0.        , 0.        , 0.20392157,\n",
       "       0.00392157, 0.        , 0.        , 0.32941177, 0.86666667,\n",
       "       0.82352942, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.02745098, 1.        , 0.44705883, 0.03529412, 0.91764706,\n",
       "       0.98039216, 1.        , 0.20392157, 0.        , 0.        ,\n",
       "       0.        , 0.38039216, 1.        , 0.96470588, 0.30980393,\n",
       "       0.        , 0.32549021, 1.        , 0.21568628, 0.02745098,\n",
       "       0.        , 0.76078433, 0.81568629, 1.        , 0.25882354,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.92941177,\n",
       "       0.72156864, 0.67843139, 1.        , 0.87843138, 0.9137255 ,\n",
       "       0.00392157, 0.        , 0.        , 0.06666667, 0.8392157 ,\n",
       "       0.95294118, 1.        , 0.39215687, 0.00392157, 0.81960785,\n",
       "       1.        , 0.47450981, 0.96078432, 0.03529412, 0.88627452,\n",
       "       0.60392159, 0.88627452, 0.81960785, 0.04705882, 0.        ,\n",
       "       0.        , 0.        , 0.46666667, 1.        , 0.95294118,\n",
       "       0.70588237, 0.99607843, 0.47843137, 0.        , 0.        ,\n",
       "       0.        , 0.37254903, 1.        , 0.85490197, 1.        ,\n",
       "       0.39215687, 0.33333334, 1.        , 1.        , 0.86666667,\n",
       "       0.78039217, 0.01960784, 0.98823529, 0.47450981, 0.24313726,\n",
       "       0.97647059, 0.69411767, 0.00784314, 0.        , 0.        ,\n",
       "       0.00784314, 0.21960784, 0.35686275, 0.96470588, 0.80392158,\n",
       "       0.02352941, 0.        , 0.        , 0.        , 0.19607843,\n",
       "       0.91764706, 0.68235296, 1.        , 0.63137257, 0.89803922,\n",
       "       1.        , 1.        , 1.        , 0.29803923, 0.13333334,\n",
       "       1.        , 0.34901962, 0.        , 0.38431373, 1.        ,\n",
       "       0.87843138, 0.27450982, 0.        , 0.01568628, 0.63529414,\n",
       "       0.99215686, 0.74117649, 0.0627451 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.04705882,\n",
       "       0.97254902, 1.        , 0.84705883, 0.60392159, 0.98431373,\n",
       "       1.        , 0.3764706 , 0.25490198, 1.        , 0.22745098,\n",
       "       0.        , 0.        , 0.4509804 , 1.        , 0.89019608,\n",
       "       0.16470589, 0.71764708, 1.        , 0.69803923, 0.02352941,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.14901961, 0.20784314,\n",
       "       0.00784314, 0.05882353, 0.56078434, 0.86666667, 0.58431375,\n",
       "       0.25098041, 0.95686275, 0.07843138, 0.        , 0.        ,\n",
       "       0.09019608, 1.        , 0.95294118, 1.        , 1.        ,\n",
       "       0.94901961, 0.00784314, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.01568628,\n",
       "       0.        , 0.        , 0.        , 0.14117648, 1.        ,\n",
       "       0.41176471, 0.5529412 , 0.95686275, 0.57647061, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.19607843, 1.        , 0.27843139, 0.29411766,\n",
       "       1.        , 0.25882354, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.24705882,\n",
       "       1.        , 0.24313726, 0.69803923, 0.9254902 , 0.01568628,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.15686275, 0.85490197, 0.32549021,\n",
       "       0.99607843, 0.50196081, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.05490196, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "len(X_train[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "X_train[0][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "X_train[0][1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "len(y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "350000"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def save_data(X_train, y_train, X_test, y_test, force = False):\n",
    "    \n",
    "    # Check for already saved files\n",
    "    if not(path.exists('../data/xtrain.pickle')) or force:\n",
    "        # Save X_train dataset as a pickle file\n",
    "        with open('../data/xtrain.pickle', 'wb') as f:\n",
    "            pickle.dump(X_train, f)\n",
    "\n",
    "        # Save X_test dataset as a pickle file\n",
    "        with open('../data/xtest.pickle', 'wb') as f:\n",
    "            pickle.dump(X_test, f)\n",
    "\n",
    "        # Save y_train dataset as a pickle file\n",
    "        with open('../data/ytrain.pickle', 'wb') as f:\n",
    "            pickle.dump(y_train, f)\n",
    "\n",
    "        # Save y_test dataset as a pickle file\n",
    "        with open('../data/ytest.pickle', 'wb') as f:\n",
    "            pickle.dump(y_test, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "save_data(X_train, y_train, X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "type(X_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "type(X_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "type(y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "len(X_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "350000"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "len(X_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def build_model_rnn(input_size, output_size, dropout = 0.0):\n",
    "    model = nn.Sequential(OrderedDict([\n",
    "                         ('fc1', nn.Linear(input_size, 128)),\n",
    "                         ('relu1', nn.ReLU()),\n",
    "                         ('fc2', nn.Linear(128, 100)),\n",
    "                         ('bn2', nn.BatchNorm1d(num_features=100)),\n",
    "                         ('relu2', nn.ReLU()),\n",
    "                         ('dropout', nn.Dropout(dropout)),\n",
    "                         ('fc3', nn.Linear(100, 64)),\n",
    "                         ('bn3', nn.BatchNorm1d(num_features=64)),\n",
    "                         ('relu3', nn.ReLU()),\n",
    "                         ('logits', nn.Linear(64, output_size))]))\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "rnn_model = build_model_rnn(input_size=784, output_size=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "rnn_model   "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=100, bias=True)\n",
       "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (fc3): Linear(in_features=100, out_features=64, bias=True)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (logits): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "cnn_model = build_model(input_size = 784, output_size=10, architecture='conv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "cnn_model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(1, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3528, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "X_train = pickle.load(open('data/xtrain.pickle', 'rb'))\n",
    "X_test = pickle.load(open('data/xtest.pickle', 'rb'))\n",
    "y_train = pickle.load(open('data/ytrain.pickle', 'rb'))\n",
    "y_test = pickle.load(open('data/ytest.pickle', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "from sklearn.utils import shuffle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "def shuffle_dataset(X_train, y_train):\n",
    "    \n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train, y_train)\n",
    "    y_train_shuffled = y_train_shuffled.reshape((X_train.shape[0], 1))\n",
    "\n",
    "    X_train_shuffled = torch.from_numpy(X_train_shuffled).float()\n",
    "    y_train_shuffled = torch.from_numpy(y_train_shuffled).long()\n",
    "\n",
    "    return X_train_shuffled, y_train_shuffled"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "X_train_shuffled, y_train_shuffled = shuffle_dataset(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "X_train_shuffled[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0118, 0.2667, 0.4431, 0.1294, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.2706, 0.9020, 1.0000, 1.0000, 0.7725, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.3216, 0.6902, 0.5451, 0.0824, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.4392, 0.9882, 0.8000, 0.2902, 0.4392, 1.0000, 0.1333, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.3137, 0.9882, 0.8824, 0.9647, 0.8118, 0.0627, 0.0000, 0.0000,\n",
       "        0.0000, 0.2431, 0.9922, 0.6510, 0.0314, 0.0000, 0.3647, 1.0000, 0.1176,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.7961, 0.8000, 0.0314, 0.2392, 0.9608, 0.7098, 0.0000,\n",
       "        0.0000, 0.0000, 0.7176, 0.8627, 0.0314, 0.0000, 0.0000, 0.4627, 0.9961,\n",
       "        0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0235, 0.9922, 0.4745, 0.0000, 0.0000, 0.4235, 1.0000,\n",
       "        0.2039, 0.0000, 0.0549, 0.9725, 0.5137, 0.0000, 0.0000, 0.0000, 0.6157,\n",
       "        0.9059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0745, 1.0000, 0.3961, 0.0000, 0.0000, 0.0392,\n",
       "        0.9176, 0.6627, 0.0000, 0.3333, 1.0000, 0.2039, 0.0000, 0.0000, 0.0235,\n",
       "        0.9294, 0.6118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.1255, 1.0000, 0.3451, 0.0000, 0.0000,\n",
       "        0.0000, 0.4980, 0.9882, 0.1412, 0.4667, 1.0000, 0.0118, 0.0000, 0.0000,\n",
       "        0.3059, 1.0000, 0.2627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 1.0000, 0.3059, 0.0000,\n",
       "        0.0000, 0.0000, 0.0745, 0.9569, 0.5843, 0.4980, 0.9804, 0.0000, 0.0000,\n",
       "        0.0000, 0.6588, 0.9020, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0549, 0.9686, 0.5686,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.5725, 0.9647, 0.5647, 0.9490, 0.0000,\n",
       "        0.0000, 0.0431, 0.9569, 0.5529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5725,\n",
       "        0.9843, 0.2235, 0.0000, 0.0000, 0.0000, 0.1294, 0.9843, 0.7804, 0.9216,\n",
       "        0.0000, 0.0000, 0.3608, 1.0000, 0.2039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0392, 0.8510, 0.8588, 0.0667, 0.0000, 0.0000, 0.0000, 0.6510, 0.9686,\n",
       "        0.8941, 0.0000, 0.0000, 0.7059, 0.8549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2118, 0.9686, 0.8941, 0.2980, 0.0000, 0.0000, 0.2627,\n",
       "        1.0000, 0.8980, 0.0000, 0.1725, 0.9882, 0.4902, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.1686, 0.7882, 1.0000, 0.6667, 0.0784,\n",
       "        0.0392, 0.9922, 0.8784, 0.0000, 0.7176, 0.9137, 0.0549, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.4667, 0.9686,\n",
       "        0.9059, 0.7412, 1.0000, 0.9294, 0.7412, 1.0000, 0.5137, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.1569, 0.8314, 1.0000, 0.7961, 1.0000, 0.9529, 0.9961, 0.9294, 0.0431,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0902, 0.5804, 0.9333, 1.0000, 0.9882, 0.6039, 1.0000, 0.4353, 1.0000,\n",
       "        0.4157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.1216, 0.8588, 0.9569, 0.6118, 0.4667, 0.6941, 0.6157, 0.7412, 0.0000,\n",
       "        0.8510, 0.6353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.7686, 0.9137, 0.1843, 0.0000, 0.0000, 0.0000, 0.6118, 0.9725,\n",
       "        0.1373, 0.7961, 0.6745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.1804, 1.0000, 0.3882, 0.0588, 0.7529, 0.3137, 0.0000, 0.6000,\n",
       "        0.9804, 0.1412, 0.8314, 0.6667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.3725, 1.0000, 0.1294, 0.3216, 1.0000, 0.8667, 0.4588,\n",
       "        0.1216, 0.2471, 0.1294, 0.9882, 0.4627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.4667, 1.0000, 0.0902, 0.0980, 0.7294, 1.0000,\n",
       "        1.0000, 0.7922, 1.0000, 0.8314, 0.9412, 0.0863, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.1255, 0.9843, 0.5216, 0.0000, 0.1647,\n",
       "        1.0000, 0.9843, 0.2431, 0.5882, 1.0000, 0.4431, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6275, 0.9961, 0.5843,\n",
       "        0.4000, 1.0000, 1.0000, 0.9569, 0.9961, 0.6980, 0.0196, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.5098,\n",
       "        0.9882, 1.0000, 1.0000, 1.0000, 0.9490, 0.5137, 0.0196, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.1333, 0.5490, 0.4941, 0.2980, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000])"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "type(X_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "type(y_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "y_train_numpy = y_train_shuffled.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "type(y_train_numpy)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "def fit_model(model, X_train, y_train, epochs = 100, n_chunks = 1000, learning_rate = 0.003, weight_decay = 0, optimizer = 'SGD'):\n",
    "\n",
    "    print(\"Fitting model with epochs = {epochs}, learning rate = {lr}\\n\"\\\n",
    "    .format(epochs = epochs, lr = learning_rate))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if (optimizer == 'SGD'):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "    print_every = 100\n",
    "    steps = 0\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        images = torch.chunk(X_train, n_chunks)\n",
    "        labels = torch.chunk(y_train, n_chunks)\n",
    "        for i in range(n_chunks):\n",
    "            steps += 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward passes\n",
    "            output = model.forward(images[i])\n",
    "            loss = criterion(output, labels[i].squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if steps % print_every == 0:\n",
    "                print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                      \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "                running_loss = 0\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "fit_model(rnn_model, X_train_shuffled, y_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting model with epochs = 100, learning rate = 0.003\n",
      "\n",
      "Epoch: 1/100...  Loss: 2.2500\n",
      "Epoch: 1/100...  Loss: 2.0532\n",
      "Epoch: 1/100...  Loss: 1.9560\n",
      "Epoch: 1/100...  Loss: 1.9172\n",
      "Epoch: 1/100...  Loss: 1.8319\n",
      "Epoch: 1/100...  Loss: 1.7964\n",
      "Epoch: 1/100...  Loss: 1.7501\n",
      "Epoch: 1/100...  Loss: 1.7265\n",
      "Epoch: 1/100...  Loss: 1.7047\n",
      "Epoch: 1/100...  Loss: 1.6432\n",
      "Epoch: 2/100...  Loss: 1.5954\n",
      "Epoch: 2/100...  Loss: 1.5887\n",
      "Epoch: 2/100...  Loss: 1.5604\n",
      "Epoch: 2/100...  Loss: 1.5827\n",
      "Epoch: 2/100...  Loss: 1.5197\n",
      "Epoch: 2/100...  Loss: 1.5310\n",
      "Epoch: 2/100...  Loss: 1.5061\n",
      "Epoch: 2/100...  Loss: 1.5030\n",
      "Epoch: 2/100...  Loss: 1.5110\n",
      "Epoch: 2/100...  Loss: 1.4452\n",
      "Epoch: 3/100...  Loss: 1.4268\n",
      "Epoch: 3/100...  Loss: 1.4248\n",
      "Epoch: 3/100...  Loss: 1.4078\n",
      "Epoch: 3/100...  Loss: 1.4383\n",
      "Epoch: 3/100...  Loss: 1.3877\n",
      "Epoch: 3/100...  Loss: 1.4022\n",
      "Epoch: 3/100...  Loss: 1.3766\n",
      "Epoch: 3/100...  Loss: 1.3779\n",
      "Epoch: 3/100...  Loss: 1.3853\n",
      "Epoch: 3/100...  Loss: 1.3242\n",
      "Epoch: 4/100...  Loss: 1.3144\n",
      "Epoch: 4/100...  Loss: 1.3117\n",
      "Epoch: 4/100...  Loss: 1.3001\n",
      "Epoch: 4/100...  Loss: 1.3320\n",
      "Epoch: 4/100...  Loss: 1.2897\n",
      "Epoch: 4/100...  Loss: 1.3096\n",
      "Epoch: 4/100...  Loss: 1.2797\n",
      "Epoch: 4/100...  Loss: 1.2870\n",
      "Epoch: 4/100...  Loss: 1.2838\n",
      "Epoch: 4/100...  Loss: 1.2312\n",
      "Epoch: 5/100...  Loss: 1.2250\n",
      "Epoch: 5/100...  Loss: 1.2234\n",
      "Epoch: 5/100...  Loss: 1.2116\n",
      "Epoch: 5/100...  Loss: 1.2462\n",
      "Epoch: 5/100...  Loss: 1.2081\n",
      "Epoch: 5/100...  Loss: 1.2325\n",
      "Epoch: 5/100...  Loss: 1.2002\n",
      "Epoch: 5/100...  Loss: 1.2123\n",
      "Epoch: 5/100...  Loss: 1.1964\n",
      "Epoch: 5/100...  Loss: 1.1539\n",
      "Epoch: 6/100...  Loss: 1.1480\n",
      "Epoch: 6/100...  Loss: 1.1437\n",
      "Epoch: 6/100...  Loss: 1.1359\n",
      "Epoch: 6/100...  Loss: 1.1697\n",
      "Epoch: 6/100...  Loss: 1.1321\n",
      "Epoch: 6/100...  Loss: 1.1652\n",
      "Epoch: 6/100...  Loss: 1.1265\n",
      "Epoch: 6/100...  Loss: 1.1457\n",
      "Epoch: 6/100...  Loss: 1.1179\n",
      "Epoch: 6/100...  Loss: 1.0871\n",
      "Epoch: 7/100...  Loss: 1.0748\n",
      "Epoch: 7/100...  Loss: 1.0731\n",
      "Epoch: 7/100...  Loss: 1.0656\n",
      "Epoch: 7/100...  Loss: 1.0981\n",
      "Epoch: 7/100...  Loss: 1.0616\n",
      "Epoch: 7/100...  Loss: 1.1001\n",
      "Epoch: 7/100...  Loss: 1.0623\n",
      "Epoch: 7/100...  Loss: 1.0819\n",
      "Epoch: 7/100...  Loss: 1.0458\n",
      "Epoch: 7/100...  Loss: 1.0217\n",
      "Epoch: 8/100...  Loss: 1.0131\n",
      "Epoch: 8/100...  Loss: 1.0067\n",
      "Epoch: 8/100...  Loss: 1.0016\n",
      "Epoch: 8/100...  Loss: 1.0356\n",
      "Epoch: 8/100...  Loss: 0.9977\n",
      "Epoch: 8/100...  Loss: 1.0381\n",
      "Epoch: 8/100...  Loss: 0.9955\n",
      "Epoch: 8/100...  Loss: 1.0196\n",
      "Epoch: 8/100...  Loss: 0.9823\n",
      "Epoch: 8/100...  Loss: 0.9591\n",
      "Epoch: 9/100...  Loss: 0.9527\n",
      "Epoch: 9/100...  Loss: 0.9459\n",
      "Epoch: 9/100...  Loss: 0.9385\n",
      "Epoch: 9/100...  Loss: 0.9736\n",
      "Epoch: 9/100...  Loss: 0.9337\n",
      "Epoch: 9/100...  Loss: 0.9763\n",
      "Epoch: 9/100...  Loss: 0.9374\n",
      "Epoch: 9/100...  Loss: 0.9573\n",
      "Epoch: 9/100...  Loss: 0.9209\n",
      "Epoch: 9/100...  Loss: 0.9018\n",
      "Epoch: 10/100...  Loss: 0.8958\n",
      "Epoch: 10/100...  Loss: 0.8852\n",
      "Epoch: 10/100...  Loss: 0.8818\n",
      "Epoch: 10/100...  Loss: 0.9153\n",
      "Epoch: 10/100...  Loss: 0.8745\n",
      "Epoch: 10/100...  Loss: 0.9153\n",
      "Epoch: 10/100...  Loss: 0.8842\n",
      "Epoch: 10/100...  Loss: 0.8968\n",
      "Epoch: 10/100...  Loss: 0.8589\n",
      "Epoch: 10/100...  Loss: 0.8476\n",
      "Epoch: 11/100...  Loss: 0.8394\n",
      "Epoch: 11/100...  Loss: 0.8256\n",
      "Epoch: 11/100...  Loss: 0.8305\n",
      "Epoch: 11/100...  Loss: 0.8576\n",
      "Epoch: 11/100...  Loss: 0.8148\n",
      "Epoch: 11/100...  Loss: 0.8532\n",
      "Epoch: 11/100...  Loss: 0.8265\n",
      "Epoch: 11/100...  Loss: 0.8427\n",
      "Epoch: 11/100...  Loss: 0.7972\n",
      "Epoch: 11/100...  Loss: 0.7908\n",
      "Epoch: 12/100...  Loss: 0.7845\n",
      "Epoch: 12/100...  Loss: 0.7733\n",
      "Epoch: 12/100...  Loss: 0.7770\n",
      "Epoch: 12/100...  Loss: 0.8006\n",
      "Epoch: 12/100...  Loss: 0.7562\n",
      "Epoch: 12/100...  Loss: 0.7912\n",
      "Epoch: 12/100...  Loss: 0.7731\n",
      "Epoch: 12/100...  Loss: 0.7819\n",
      "Epoch: 12/100...  Loss: 0.7404\n",
      "Epoch: 12/100...  Loss: 0.7341\n",
      "Epoch: 13/100...  Loss: 0.7365\n",
      "Epoch: 13/100...  Loss: 0.7238\n",
      "Epoch: 13/100...  Loss: 0.7254\n",
      "Epoch: 13/100...  Loss: 0.7484\n",
      "Epoch: 13/100...  Loss: 0.7010\n",
      "Epoch: 13/100...  Loss: 0.7311\n",
      "Epoch: 13/100...  Loss: 0.7224\n",
      "Epoch: 13/100...  Loss: 0.7258\n",
      "Epoch: 13/100...  Loss: 0.6863\n",
      "Epoch: 13/100...  Loss: 0.6865\n",
      "Epoch: 14/100...  Loss: 0.6842\n",
      "Epoch: 14/100...  Loss: 0.6685\n",
      "Epoch: 14/100...  Loss: 0.6730\n",
      "Epoch: 14/100...  Loss: 0.6966\n",
      "Epoch: 14/100...  Loss: 0.6522\n",
      "Epoch: 14/100...  Loss: 0.6712\n",
      "Epoch: 14/100...  Loss: 0.6715\n",
      "Epoch: 14/100...  Loss: 0.6739\n",
      "Epoch: 14/100...  Loss: 0.6360\n",
      "Epoch: 14/100...  Loss: 0.6376\n",
      "Epoch: 15/100...  Loss: 0.6314\n",
      "Epoch: 15/100...  Loss: 0.6173\n",
      "Epoch: 15/100...  Loss: 0.6242\n",
      "Epoch: 15/100...  Loss: 0.6387\n",
      "Epoch: 15/100...  Loss: 0.6001\n",
      "Epoch: 15/100...  Loss: 0.6168\n",
      "Epoch: 15/100...  Loss: 0.6186\n",
      "Epoch: 15/100...  Loss: 0.6231\n",
      "Epoch: 15/100...  Loss: 0.5845\n",
      "Epoch: 15/100...  Loss: 0.5906\n",
      "Epoch: 16/100...  Loss: 0.5857\n",
      "Epoch: 16/100...  Loss: 0.5677\n",
      "Epoch: 16/100...  Loss: 0.5746\n",
      "Epoch: 16/100...  Loss: 0.5887\n",
      "Epoch: 16/100...  Loss: 0.5559\n",
      "Epoch: 16/100...  Loss: 0.5632\n",
      "Epoch: 16/100...  Loss: 0.5767\n",
      "Epoch: 16/100...  Loss: 0.5742\n",
      "Epoch: 16/100...  Loss: 0.5381\n",
      "Epoch: 16/100...  Loss: 0.5440\n",
      "Epoch: 17/100...  Loss: 0.5421\n",
      "Epoch: 17/100...  Loss: 0.5184\n",
      "Epoch: 17/100...  Loss: 0.5303\n",
      "Epoch: 17/100...  Loss: 0.5428\n",
      "Epoch: 17/100...  Loss: 0.5119\n",
      "Epoch: 17/100...  Loss: 0.5179\n",
      "Epoch: 17/100...  Loss: 0.5250\n",
      "Epoch: 17/100...  Loss: 0.5257\n",
      "Epoch: 17/100...  Loss: 0.4925\n",
      "Epoch: 17/100...  Loss: 0.5010\n",
      "Epoch: 18/100...  Loss: 0.4989\n",
      "Epoch: 18/100...  Loss: 0.4745\n",
      "Epoch: 18/100...  Loss: 0.4888\n",
      "Epoch: 18/100...  Loss: 0.4969\n",
      "Epoch: 18/100...  Loss: 0.4720\n",
      "Epoch: 18/100...  Loss: 0.4709\n",
      "Epoch: 18/100...  Loss: 0.4805\n",
      "Epoch: 18/100...  Loss: 0.4801\n",
      "Epoch: 18/100...  Loss: 0.4479\n",
      "Epoch: 18/100...  Loss: 0.4545\n",
      "Epoch: 19/100...  Loss: 0.4572\n",
      "Epoch: 19/100...  Loss: 0.4309\n",
      "Epoch: 19/100...  Loss: 0.4563\n",
      "Epoch: 19/100...  Loss: 0.4516\n",
      "Epoch: 19/100...  Loss: 0.4295\n",
      "Epoch: 19/100...  Loss: 0.4278\n",
      "Epoch: 19/100...  Loss: 0.4382\n",
      "Epoch: 19/100...  Loss: 0.4342\n",
      "Epoch: 19/100...  Loss: 0.4129\n",
      "Epoch: 19/100...  Loss: 0.4102\n",
      "Epoch: 20/100...  Loss: 0.4145\n",
      "Epoch: 20/100...  Loss: 0.3957\n",
      "Epoch: 20/100...  Loss: 0.4165\n",
      "Epoch: 20/100...  Loss: 0.4106\n",
      "Epoch: 20/100...  Loss: 0.3904\n",
      "Epoch: 20/100...  Loss: 0.3875\n",
      "Epoch: 20/100...  Loss: 0.4006\n",
      "Epoch: 20/100...  Loss: 0.3997\n",
      "Epoch: 20/100...  Loss: 0.3736\n",
      "Epoch: 20/100...  Loss: 0.3707\n",
      "Epoch: 21/100...  Loss: 0.3745\n",
      "Epoch: 21/100...  Loss: 0.3628\n",
      "Epoch: 21/100...  Loss: 0.3769\n",
      "Epoch: 21/100...  Loss: 0.3734\n",
      "Epoch: 21/100...  Loss: 0.3522\n",
      "Epoch: 21/100...  Loss: 0.3508\n",
      "Epoch: 21/100...  Loss: 0.3622\n",
      "Epoch: 21/100...  Loss: 0.3578\n",
      "Epoch: 21/100...  Loss: 0.3347\n",
      "Epoch: 21/100...  Loss: 0.3346\n",
      "Epoch: 22/100...  Loss: 0.3395\n",
      "Epoch: 22/100...  Loss: 0.3257\n",
      "Epoch: 22/100...  Loss: 0.3388\n",
      "Epoch: 22/100...  Loss: 0.3383\n",
      "Epoch: 22/100...  Loss: 0.3146\n",
      "Epoch: 22/100...  Loss: 0.3213\n",
      "Epoch: 22/100...  Loss: 0.3271\n",
      "Epoch: 22/100...  Loss: 0.3217\n",
      "Epoch: 22/100...  Loss: 0.3027\n",
      "Epoch: 22/100...  Loss: 0.3028\n",
      "Epoch: 23/100...  Loss: 0.3022\n",
      "Epoch: 23/100...  Loss: 0.2934\n",
      "Epoch: 23/100...  Loss: 0.3022\n",
      "Epoch: 23/100...  Loss: 0.3004\n",
      "Epoch: 23/100...  Loss: 0.2810\n",
      "Epoch: 23/100...  Loss: 0.2854\n",
      "Epoch: 23/100...  Loss: 0.2884\n",
      "Epoch: 23/100...  Loss: 0.2890\n",
      "Epoch: 23/100...  Loss: 0.2691\n",
      "Epoch: 23/100...  Loss: 0.2728\n",
      "Epoch: 24/100...  Loss: 0.2705\n",
      "Epoch: 24/100...  Loss: 0.2615\n",
      "Epoch: 24/100...  Loss: 0.2702\n",
      "Epoch: 24/100...  Loss: 0.2714\n",
      "Epoch: 24/100...  Loss: 0.2499\n",
      "Epoch: 24/100...  Loss: 0.2530\n",
      "Epoch: 24/100...  Loss: 0.2600\n",
      "Epoch: 24/100...  Loss: 0.2568\n",
      "Epoch: 24/100...  Loss: 0.2392\n",
      "Epoch: 24/100...  Loss: 0.2442\n",
      "Epoch: 25/100...  Loss: 0.2372\n",
      "Epoch: 25/100...  Loss: 0.2325\n",
      "Epoch: 25/100...  Loss: 0.2404\n",
      "Epoch: 25/100...  Loss: 0.2378\n",
      "Epoch: 25/100...  Loss: 0.2222\n",
      "Epoch: 25/100...  Loss: 0.2237\n",
      "Epoch: 25/100...  Loss: 0.2295\n",
      "Epoch: 25/100...  Loss: 0.2264\n",
      "Epoch: 25/100...  Loss: 0.2131\n",
      "Epoch: 25/100...  Loss: 0.2139\n",
      "Epoch: 26/100...  Loss: 0.2096\n",
      "Epoch: 26/100...  Loss: 0.2094\n",
      "Epoch: 26/100...  Loss: 0.2106\n",
      "Epoch: 26/100...  Loss: 0.2169\n",
      "Epoch: 26/100...  Loss: 0.1942\n",
      "Epoch: 26/100...  Loss: 0.1997\n",
      "Epoch: 26/100...  Loss: 0.2040\n",
      "Epoch: 26/100...  Loss: 0.1999\n",
      "Epoch: 26/100...  Loss: 0.1878\n",
      "Epoch: 26/100...  Loss: 0.1850\n",
      "Epoch: 27/100...  Loss: 0.1850\n",
      "Epoch: 27/100...  Loss: 0.1838\n",
      "Epoch: 27/100...  Loss: 0.1891\n",
      "Epoch: 27/100...  Loss: 0.1944\n",
      "Epoch: 27/100...  Loss: 0.1715\n",
      "Epoch: 27/100...  Loss: 0.1743\n",
      "Epoch: 27/100...  Loss: 0.1763\n",
      "Epoch: 27/100...  Loss: 0.1750\n",
      "Epoch: 27/100...  Loss: 0.1687\n",
      "Epoch: 27/100...  Loss: 0.1624\n",
      "Epoch: 28/100...  Loss: 0.1581\n",
      "Epoch: 28/100...  Loss: 0.1564\n",
      "Epoch: 28/100...  Loss: 0.1647\n",
      "Epoch: 28/100...  Loss: 0.1640\n",
      "Epoch: 28/100...  Loss: 0.1466\n",
      "Epoch: 28/100...  Loss: 0.1471\n",
      "Epoch: 28/100...  Loss: 0.1531\n",
      "Epoch: 28/100...  Loss: 0.1501\n",
      "Epoch: 28/100...  Loss: 0.1434\n",
      "Epoch: 28/100...  Loss: 0.1371\n",
      "Epoch: 29/100...  Loss: 0.1342\n",
      "Epoch: 29/100...  Loss: 0.1371\n",
      "Epoch: 29/100...  Loss: 0.1471\n",
      "Epoch: 29/100...  Loss: 0.1453\n",
      "Epoch: 29/100...  Loss: 0.1238\n",
      "Epoch: 29/100...  Loss: 0.1273\n",
      "Epoch: 29/100...  Loss: 0.1356\n",
      "Epoch: 29/100...  Loss: 0.1297\n",
      "Epoch: 29/100...  Loss: 0.1252\n",
      "Epoch: 29/100...  Loss: 0.1177\n",
      "Epoch: 30/100...  Loss: 0.1193\n",
      "Epoch: 30/100...  Loss: 0.1203\n",
      "Epoch: 30/100...  Loss: 0.1251\n",
      "Epoch: 30/100...  Loss: 0.1255\n",
      "Epoch: 30/100...  Loss: 0.1090\n",
      "Epoch: 30/100...  Loss: 0.1106\n",
      "Epoch: 30/100...  Loss: 0.1128\n",
      "Epoch: 30/100...  Loss: 0.1127\n",
      "Epoch: 30/100...  Loss: 0.1078\n",
      "Epoch: 30/100...  Loss: 0.1021\n",
      "Epoch: 31/100...  Loss: 0.1004\n",
      "Epoch: 31/100...  Loss: 0.1018\n",
      "Epoch: 31/100...  Loss: 0.1089\n",
      "Epoch: 31/100...  Loss: 0.1092\n",
      "Epoch: 31/100...  Loss: 0.0908\n",
      "Epoch: 31/100...  Loss: 0.0929\n",
      "Epoch: 31/100...  Loss: 0.0967\n",
      "Epoch: 31/100...  Loss: 0.0942\n",
      "Epoch: 31/100...  Loss: 0.0924\n",
      "Epoch: 31/100...  Loss: 0.0854\n",
      "Epoch: 32/100...  Loss: 0.0844\n",
      "Epoch: 32/100...  Loss: 0.0879\n",
      "Epoch: 32/100...  Loss: 0.0951\n",
      "Epoch: 32/100...  Loss: 0.0951\n",
      "Epoch: 32/100...  Loss: 0.0801\n",
      "Epoch: 32/100...  Loss: 0.0809\n",
      "Epoch: 32/100...  Loss: 0.0829\n",
      "Epoch: 32/100...  Loss: 0.0800\n",
      "Epoch: 32/100...  Loss: 0.0748\n",
      "Epoch: 32/100...  Loss: 0.0727\n",
      "Epoch: 33/100...  Loss: 0.0719\n",
      "Epoch: 33/100...  Loss: 0.0756\n",
      "Epoch: 33/100...  Loss: 0.0794\n",
      "Epoch: 33/100...  Loss: 0.0812\n",
      "Epoch: 33/100...  Loss: 0.0668\n",
      "Epoch: 33/100...  Loss: 0.0684\n",
      "Epoch: 33/100...  Loss: 0.0693\n",
      "Epoch: 33/100...  Loss: 0.0688\n",
      "Epoch: 33/100...  Loss: 0.0645\n",
      "Epoch: 33/100...  Loss: 0.0612\n",
      "Epoch: 34/100...  Loss: 0.0608\n",
      "Epoch: 34/100...  Loss: 0.0616\n",
      "Epoch: 34/100...  Loss: 0.0660\n",
      "Epoch: 34/100...  Loss: 0.0685\n",
      "Epoch: 34/100...  Loss: 0.0556\n",
      "Epoch: 34/100...  Loss: 0.0579\n",
      "Epoch: 34/100...  Loss: 0.0598\n",
      "Epoch: 34/100...  Loss: 0.0601\n",
      "Epoch: 34/100...  Loss: 0.0552\n",
      "Epoch: 34/100...  Loss: 0.0496\n",
      "Epoch: 35/100...  Loss: 0.0518\n",
      "Epoch: 35/100...  Loss: 0.0536\n",
      "Epoch: 35/100...  Loss: 0.0544\n",
      "Epoch: 35/100...  Loss: 0.0583\n",
      "Epoch: 35/100...  Loss: 0.0480\n",
      "Epoch: 35/100...  Loss: 0.0485\n",
      "Epoch: 35/100...  Loss: 0.0503\n",
      "Epoch: 35/100...  Loss: 0.0509\n",
      "Epoch: 35/100...  Loss: 0.0466\n",
      "Epoch: 35/100...  Loss: 0.0425\n",
      "Epoch: 36/100...  Loss: 0.0448\n",
      "Epoch: 36/100...  Loss: 0.0467\n",
      "Epoch: 36/100...  Loss: 0.0468\n",
      "Epoch: 36/100...  Loss: 0.0515\n",
      "Epoch: 36/100...  Loss: 0.0412\n",
      "Epoch: 36/100...  Loss: 0.0423\n",
      "Epoch: 36/100...  Loss: 0.0431\n",
      "Epoch: 36/100...  Loss: 0.0434\n",
      "Epoch: 36/100...  Loss: 0.0397\n",
      "Epoch: 36/100...  Loss: 0.0362\n",
      "Epoch: 37/100...  Loss: 0.0388\n",
      "Epoch: 37/100...  Loss: 0.0406\n",
      "Epoch: 37/100...  Loss: 0.0407\n",
      "Epoch: 37/100...  Loss: 0.0446\n",
      "Epoch: 37/100...  Loss: 0.0366\n",
      "Epoch: 37/100...  Loss: 0.0366\n",
      "Epoch: 37/100...  Loss: 0.0380\n",
      "Epoch: 37/100...  Loss: 0.0384\n",
      "Epoch: 37/100...  Loss: 0.0349\n",
      "Epoch: 37/100...  Loss: 0.0314\n",
      "Epoch: 38/100...  Loss: 0.0346\n",
      "Epoch: 38/100...  Loss: 0.0359\n",
      "Epoch: 38/100...  Loss: 0.0352\n",
      "Epoch: 38/100...  Loss: 0.0392\n",
      "Epoch: 38/100...  Loss: 0.0317\n",
      "Epoch: 38/100...  Loss: 0.0323\n",
      "Epoch: 38/100...  Loss: 0.0336\n",
      "Epoch: 38/100...  Loss: 0.0336\n",
      "Epoch: 38/100...  Loss: 0.0309\n",
      "Epoch: 38/100...  Loss: 0.0273\n",
      "Epoch: 39/100...  Loss: 0.0307\n",
      "Epoch: 39/100...  Loss: 0.0316\n",
      "Epoch: 39/100...  Loss: 0.0312\n",
      "Epoch: 39/100...  Loss: 0.0348\n",
      "Epoch: 39/100...  Loss: 0.0288\n",
      "Epoch: 39/100...  Loss: 0.0293\n",
      "Epoch: 39/100...  Loss: 0.0299\n",
      "Epoch: 39/100...  Loss: 0.0305\n",
      "Epoch: 39/100...  Loss: 0.0278\n",
      "Epoch: 39/100...  Loss: 0.0247\n",
      "Epoch: 40/100...  Loss: 0.0274\n",
      "Epoch: 40/100...  Loss: 0.0284\n",
      "Epoch: 40/100...  Loss: 0.0282\n",
      "Epoch: 40/100...  Loss: 0.0310\n",
      "Epoch: 40/100...  Loss: 0.0258\n",
      "Epoch: 40/100...  Loss: 0.0263\n",
      "Epoch: 40/100...  Loss: 0.0268\n",
      "Epoch: 40/100...  Loss: 0.0274\n",
      "Epoch: 40/100...  Loss: 0.0249\n",
      "Epoch: 40/100...  Loss: 0.0224\n",
      "Epoch: 41/100...  Loss: 0.0251\n",
      "Epoch: 41/100...  Loss: 0.0253\n",
      "Epoch: 41/100...  Loss: 0.0255\n",
      "Epoch: 41/100...  Loss: 0.0277\n",
      "Epoch: 41/100...  Loss: 0.0238\n",
      "Epoch: 41/100...  Loss: 0.0236\n",
      "Epoch: 41/100...  Loss: 0.0244\n",
      "Epoch: 41/100...  Loss: 0.0247\n",
      "Epoch: 41/100...  Loss: 0.0226\n",
      "Epoch: 41/100...  Loss: 0.0203\n",
      "Epoch: 42/100...  Loss: 0.0230\n",
      "Epoch: 42/100...  Loss: 0.0228\n",
      "Epoch: 42/100...  Loss: 0.0232\n",
      "Epoch: 42/100...  Loss: 0.0247\n",
      "Epoch: 42/100...  Loss: 0.0215\n",
      "Epoch: 42/100...  Loss: 0.0214\n",
      "Epoch: 42/100...  Loss: 0.0224\n",
      "Epoch: 42/100...  Loss: 0.0227\n",
      "Epoch: 42/100...  Loss: 0.0207\n",
      "Epoch: 42/100...  Loss: 0.0186\n",
      "Epoch: 43/100...  Loss: 0.0211\n",
      "Epoch: 43/100...  Loss: 0.0206\n",
      "Epoch: 43/100...  Loss: 0.0212\n",
      "Epoch: 43/100...  Loss: 0.0226\n",
      "Epoch: 43/100...  Loss: 0.0199\n",
      "Epoch: 43/100...  Loss: 0.0194\n",
      "Epoch: 43/100...  Loss: 0.0205\n",
      "Epoch: 43/100...  Loss: 0.0208\n",
      "Epoch: 43/100...  Loss: 0.0190\n",
      "Epoch: 43/100...  Loss: 0.0169\n",
      "Epoch: 44/100...  Loss: 0.0194\n",
      "Epoch: 44/100...  Loss: 0.0190\n",
      "Epoch: 44/100...  Loss: 0.0197\n",
      "Epoch: 44/100...  Loss: 0.0205\n",
      "Epoch: 44/100...  Loss: 0.0184\n",
      "Epoch: 44/100...  Loss: 0.0179\n",
      "Epoch: 44/100...  Loss: 0.0189\n",
      "Epoch: 44/100...  Loss: 0.0192\n",
      "Epoch: 44/100...  Loss: 0.0176\n",
      "Epoch: 44/100...  Loss: 0.0157\n",
      "Epoch: 45/100...  Loss: 0.0180\n",
      "Epoch: 45/100...  Loss: 0.0174\n",
      "Epoch: 45/100...  Loss: 0.0183\n",
      "Epoch: 45/100...  Loss: 0.0189\n",
      "Epoch: 45/100...  Loss: 0.0168\n",
      "Epoch: 45/100...  Loss: 0.0165\n",
      "Epoch: 45/100...  Loss: 0.0173\n",
      "Epoch: 45/100...  Loss: 0.0178\n",
      "Epoch: 45/100...  Loss: 0.0164\n",
      "Epoch: 45/100...  Loss: 0.0146\n",
      "Epoch: 46/100...  Loss: 0.0167\n",
      "Epoch: 46/100...  Loss: 0.0159\n",
      "Epoch: 46/100...  Loss: 0.0169\n",
      "Epoch: 46/100...  Loss: 0.0175\n",
      "Epoch: 46/100...  Loss: 0.0157\n",
      "Epoch: 46/100...  Loss: 0.0153\n",
      "Epoch: 46/100...  Loss: 0.0161\n",
      "Epoch: 46/100...  Loss: 0.0167\n",
      "Epoch: 46/100...  Loss: 0.0152\n",
      "Epoch: 46/100...  Loss: 0.0136\n",
      "Epoch: 47/100...  Loss: 0.0160\n",
      "Epoch: 47/100...  Loss: 0.0147\n",
      "Epoch: 47/100...  Loss: 0.0159\n",
      "Epoch: 47/100...  Loss: 0.0162\n",
      "Epoch: 47/100...  Loss: 0.0146\n",
      "Epoch: 47/100...  Loss: 0.0143\n",
      "Epoch: 47/100...  Loss: 0.0148\n",
      "Epoch: 47/100...  Loss: 0.0155\n",
      "Epoch: 47/100...  Loss: 0.0141\n",
      "Epoch: 47/100...  Loss: 0.0128\n",
      "Epoch: 48/100...  Loss: 0.0149\n",
      "Epoch: 48/100...  Loss: 0.0138\n",
      "Epoch: 48/100...  Loss: 0.0149\n",
      "Epoch: 48/100...  Loss: 0.0150\n",
      "Epoch: 48/100...  Loss: 0.0137\n",
      "Epoch: 48/100...  Loss: 0.0134\n",
      "Epoch: 48/100...  Loss: 0.0137\n",
      "Epoch: 48/100...  Loss: 0.0144\n",
      "Epoch: 48/100...  Loss: 0.0133\n",
      "Epoch: 48/100...  Loss: 0.0121\n",
      "Epoch: 49/100...  Loss: 0.0138\n",
      "Epoch: 49/100...  Loss: 0.0128\n",
      "Epoch: 49/100...  Loss: 0.0140\n",
      "Epoch: 49/100...  Loss: 0.0140\n",
      "Epoch: 49/100...  Loss: 0.0128\n",
      "Epoch: 49/100...  Loss: 0.0126\n",
      "Epoch: 49/100...  Loss: 0.0128\n",
      "Epoch: 49/100...  Loss: 0.0135\n",
      "Epoch: 49/100...  Loss: 0.0126\n",
      "Epoch: 49/100...  Loss: 0.0114\n",
      "Epoch: 50/100...  Loss: 0.0131\n",
      "Epoch: 50/100...  Loss: 0.0121\n",
      "Epoch: 50/100...  Loss: 0.0132\n",
      "Epoch: 50/100...  Loss: 0.0132\n",
      "Epoch: 50/100...  Loss: 0.0120\n",
      "Epoch: 50/100...  Loss: 0.0119\n",
      "Epoch: 50/100...  Loss: 0.0120\n",
      "Epoch: 50/100...  Loss: 0.0127\n",
      "Epoch: 50/100...  Loss: 0.0119\n",
      "Epoch: 50/100...  Loss: 0.0108\n",
      "Epoch: 51/100...  Loss: 0.0123\n",
      "Epoch: 51/100...  Loss: 0.0115\n",
      "Epoch: 51/100...  Loss: 0.0125\n",
      "Epoch: 51/100...  Loss: 0.0124\n",
      "Epoch: 51/100...  Loss: 0.0114\n",
      "Epoch: 51/100...  Loss: 0.0112\n",
      "Epoch: 51/100...  Loss: 0.0113\n",
      "Epoch: 51/100...  Loss: 0.0118\n",
      "Epoch: 51/100...  Loss: 0.0112\n",
      "Epoch: 51/100...  Loss: 0.0102\n",
      "Epoch: 52/100...  Loss: 0.0115\n",
      "Epoch: 52/100...  Loss: 0.0108\n",
      "Epoch: 52/100...  Loss: 0.0118\n",
      "Epoch: 52/100...  Loss: 0.0118\n",
      "Epoch: 52/100...  Loss: 0.0108\n",
      "Epoch: 52/100...  Loss: 0.0106\n",
      "Epoch: 52/100...  Loss: 0.0106\n",
      "Epoch: 52/100...  Loss: 0.0111\n",
      "Epoch: 52/100...  Loss: 0.0106\n",
      "Epoch: 52/100...  Loss: 0.0097\n",
      "Epoch: 53/100...  Loss: 0.0109\n",
      "Epoch: 53/100...  Loss: 0.0103\n",
      "Epoch: 53/100...  Loss: 0.0112\n",
      "Epoch: 53/100...  Loss: 0.0112\n",
      "Epoch: 53/100...  Loss: 0.0102\n",
      "Epoch: 53/100...  Loss: 0.0100\n",
      "Epoch: 53/100...  Loss: 0.0101\n",
      "Epoch: 53/100...  Loss: 0.0104\n",
      "Epoch: 53/100...  Loss: 0.0100\n",
      "Epoch: 53/100...  Loss: 0.0092\n",
      "Epoch: 54/100...  Loss: 0.0103\n",
      "Epoch: 54/100...  Loss: 0.0097\n",
      "Epoch: 54/100...  Loss: 0.0107\n",
      "Epoch: 54/100...  Loss: 0.0106\n",
      "Epoch: 54/100...  Loss: 0.0097\n",
      "Epoch: 54/100...  Loss: 0.0095\n",
      "Epoch: 54/100...  Loss: 0.0095\n",
      "Epoch: 54/100...  Loss: 0.0099\n",
      "Epoch: 54/100...  Loss: 0.0095\n",
      "Epoch: 54/100...  Loss: 0.0087\n",
      "Epoch: 55/100...  Loss: 0.0098\n",
      "Epoch: 55/100...  Loss: 0.0092\n",
      "Epoch: 55/100...  Loss: 0.0101\n",
      "Epoch: 55/100...  Loss: 0.0100\n",
      "Epoch: 55/100...  Loss: 0.0092\n",
      "Epoch: 55/100...  Loss: 0.0091\n",
      "Epoch: 55/100...  Loss: 0.0091\n",
      "Epoch: 55/100...  Loss: 0.0094\n",
      "Epoch: 55/100...  Loss: 0.0090\n",
      "Epoch: 55/100...  Loss: 0.0083\n",
      "Epoch: 56/100...  Loss: 0.0093\n",
      "Epoch: 56/100...  Loss: 0.0088\n",
      "Epoch: 56/100...  Loss: 0.0097\n",
      "Epoch: 56/100...  Loss: 0.0095\n",
      "Epoch: 56/100...  Loss: 0.0088\n",
      "Epoch: 56/100...  Loss: 0.0087\n",
      "Epoch: 56/100...  Loss: 0.0087\n",
      "Epoch: 56/100...  Loss: 0.0089\n",
      "Epoch: 56/100...  Loss: 0.0086\n",
      "Epoch: 56/100...  Loss: 0.0080\n",
      "Epoch: 57/100...  Loss: 0.0089\n",
      "Epoch: 57/100...  Loss: 0.0084\n",
      "Epoch: 57/100...  Loss: 0.0092\n",
      "Epoch: 57/100...  Loss: 0.0091\n",
      "Epoch: 57/100...  Loss: 0.0084\n",
      "Epoch: 57/100...  Loss: 0.0083\n",
      "Epoch: 57/100...  Loss: 0.0083\n",
      "Epoch: 57/100...  Loss: 0.0085\n",
      "Epoch: 57/100...  Loss: 0.0082\n",
      "Epoch: 57/100...  Loss: 0.0076\n",
      "Epoch: 58/100...  Loss: 0.0085\n",
      "Epoch: 58/100...  Loss: 0.0080\n",
      "Epoch: 58/100...  Loss: 0.0089\n",
      "Epoch: 58/100...  Loss: 0.0087\n",
      "Epoch: 58/100...  Loss: 0.0080\n",
      "Epoch: 58/100...  Loss: 0.0079\n",
      "Epoch: 58/100...  Loss: 0.0079\n",
      "Epoch: 58/100...  Loss: 0.0081\n",
      "Epoch: 58/100...  Loss: 0.0078\n",
      "Epoch: 58/100...  Loss: 0.0073\n",
      "Epoch: 59/100...  Loss: 0.0081\n",
      "Epoch: 59/100...  Loss: 0.0076\n",
      "Epoch: 59/100...  Loss: 0.0084\n",
      "Epoch: 59/100...  Loss: 0.0083\n",
      "Epoch: 59/100...  Loss: 0.0077\n",
      "Epoch: 59/100...  Loss: 0.0076\n",
      "Epoch: 59/100...  Loss: 0.0076\n",
      "Epoch: 59/100...  Loss: 0.0078\n",
      "Epoch: 59/100...  Loss: 0.0075\n",
      "Epoch: 59/100...  Loss: 0.0070\n",
      "Epoch: 60/100...  Loss: 0.0078\n",
      "Epoch: 60/100...  Loss: 0.0073\n",
      "Epoch: 60/100...  Loss: 0.0081\n",
      "Epoch: 60/100...  Loss: 0.0080\n",
      "Epoch: 60/100...  Loss: 0.0073\n",
      "Epoch: 60/100...  Loss: 0.0073\n",
      "Epoch: 60/100...  Loss: 0.0073\n",
      "Epoch: 60/100...  Loss: 0.0075\n",
      "Epoch: 60/100...  Loss: 0.0072\n",
      "Epoch: 60/100...  Loss: 0.0068\n",
      "Epoch: 61/100...  Loss: 0.0075\n",
      "Epoch: 61/100...  Loss: 0.0070\n",
      "Epoch: 61/100...  Loss: 0.0078\n",
      "Epoch: 61/100...  Loss: 0.0077\n",
      "Epoch: 61/100...  Loss: 0.0070\n",
      "Epoch: 61/100...  Loss: 0.0070\n",
      "Epoch: 61/100...  Loss: 0.0070\n",
      "Epoch: 61/100...  Loss: 0.0072\n",
      "Epoch: 61/100...  Loss: 0.0069\n",
      "Epoch: 61/100...  Loss: 0.0065\n",
      "Epoch: 62/100...  Loss: 0.0072\n",
      "Epoch: 62/100...  Loss: 0.0067\n",
      "Epoch: 62/100...  Loss: 0.0075\n",
      "Epoch: 62/100...  Loss: 0.0074\n",
      "Epoch: 62/100...  Loss: 0.0068\n",
      "Epoch: 62/100...  Loss: 0.0067\n",
      "Epoch: 62/100...  Loss: 0.0067\n",
      "Epoch: 62/100...  Loss: 0.0069\n",
      "Epoch: 62/100...  Loss: 0.0066\n",
      "Epoch: 62/100...  Loss: 0.0063\n",
      "Epoch: 63/100...  Loss: 0.0069\n",
      "Epoch: 63/100...  Loss: 0.0065\n",
      "Epoch: 63/100...  Loss: 0.0072\n",
      "Epoch: 63/100...  Loss: 0.0071\n",
      "Epoch: 63/100...  Loss: 0.0065\n",
      "Epoch: 63/100...  Loss: 0.0065\n",
      "Epoch: 63/100...  Loss: 0.0064\n",
      "Epoch: 63/100...  Loss: 0.0066\n",
      "Epoch: 63/100...  Loss: 0.0063\n",
      "Epoch: 63/100...  Loss: 0.0060\n",
      "Epoch: 64/100...  Loss: 0.0066\n",
      "Epoch: 64/100...  Loss: 0.0062\n",
      "Epoch: 64/100...  Loss: 0.0069\n",
      "Epoch: 64/100...  Loss: 0.0069\n",
      "Epoch: 64/100...  Loss: 0.0063\n",
      "Epoch: 64/100...  Loss: 0.0063\n",
      "Epoch: 64/100...  Loss: 0.0062\n",
      "Epoch: 64/100...  Loss: 0.0064\n",
      "Epoch: 64/100...  Loss: 0.0061\n",
      "Epoch: 64/100...  Loss: 0.0058\n",
      "Epoch: 65/100...  Loss: 0.0064\n",
      "Epoch: 65/100...  Loss: 0.0060\n",
      "Epoch: 65/100...  Loss: 0.0067\n",
      "Epoch: 65/100...  Loss: 0.0066\n",
      "Epoch: 65/100...  Loss: 0.0060\n",
      "Epoch: 65/100...  Loss: 0.0060\n",
      "Epoch: 65/100...  Loss: 0.0060\n",
      "Epoch: 65/100...  Loss: 0.0062\n",
      "Epoch: 65/100...  Loss: 0.0059\n",
      "Epoch: 65/100...  Loss: 0.0056\n",
      "Epoch: 66/100...  Loss: 0.0061\n",
      "Epoch: 66/100...  Loss: 0.0058\n",
      "Epoch: 66/100...  Loss: 0.0064\n",
      "Epoch: 66/100...  Loss: 0.0064\n",
      "Epoch: 66/100...  Loss: 0.0058\n",
      "Epoch: 66/100...  Loss: 0.0058\n",
      "Epoch: 66/100...  Loss: 0.0058\n",
      "Epoch: 66/100...  Loss: 0.0059\n",
      "Epoch: 66/100...  Loss: 0.0057\n",
      "Epoch: 66/100...  Loss: 0.0055\n",
      "Epoch: 67/100...  Loss: 0.0060\n",
      "Epoch: 67/100...  Loss: 0.0056\n",
      "Epoch: 67/100...  Loss: 0.0062\n",
      "Epoch: 67/100...  Loss: 0.0062\n",
      "Epoch: 67/100...  Loss: 0.0056\n",
      "Epoch: 67/100...  Loss: 0.0056\n",
      "Epoch: 67/100...  Loss: 0.0056\n",
      "Epoch: 67/100...  Loss: 0.0057\n",
      "Epoch: 67/100...  Loss: 0.0056\n",
      "Epoch: 67/100...  Loss: 0.0053\n",
      "Epoch: 68/100...  Loss: 0.0057\n",
      "Epoch: 68/100...  Loss: 0.0054\n",
      "Epoch: 68/100...  Loss: 0.0060\n",
      "Epoch: 68/100...  Loss: 0.0060\n",
      "Epoch: 68/100...  Loss: 0.0054\n",
      "Epoch: 68/100...  Loss: 0.0054\n",
      "Epoch: 68/100...  Loss: 0.0054\n",
      "Epoch: 68/100...  Loss: 0.0055\n",
      "Epoch: 68/100...  Loss: 0.0054\n",
      "Epoch: 68/100...  Loss: 0.0051\n",
      "Epoch: 69/100...  Loss: 0.0056\n",
      "Epoch: 69/100...  Loss: 0.0052\n",
      "Epoch: 69/100...  Loss: 0.0058\n",
      "Epoch: 69/100...  Loss: 0.0058\n",
      "Epoch: 69/100...  Loss: 0.0052\n",
      "Epoch: 69/100...  Loss: 0.0053\n",
      "Epoch: 69/100...  Loss: 0.0052\n",
      "Epoch: 69/100...  Loss: 0.0054\n",
      "Epoch: 69/100...  Loss: 0.0052\n",
      "Epoch: 69/100...  Loss: 0.0050\n",
      "Epoch: 70/100...  Loss: 0.0054\n",
      "Epoch: 70/100...  Loss: 0.0051\n",
      "Epoch: 70/100...  Loss: 0.0056\n",
      "Epoch: 70/100...  Loss: 0.0056\n",
      "Epoch: 70/100...  Loss: 0.0050\n",
      "Epoch: 70/100...  Loss: 0.0051\n",
      "Epoch: 70/100...  Loss: 0.0051\n",
      "Epoch: 70/100...  Loss: 0.0052\n",
      "Epoch: 70/100...  Loss: 0.0051\n",
      "Epoch: 70/100...  Loss: 0.0048\n",
      "Epoch: 71/100...  Loss: 0.0052\n",
      "Epoch: 71/100...  Loss: 0.0049\n",
      "Epoch: 71/100...  Loss: 0.0054\n",
      "Epoch: 71/100...  Loss: 0.0054\n",
      "Epoch: 71/100...  Loss: 0.0049\n",
      "Epoch: 71/100...  Loss: 0.0050\n",
      "Epoch: 71/100...  Loss: 0.0049\n",
      "Epoch: 71/100...  Loss: 0.0050\n",
      "Epoch: 71/100...  Loss: 0.0049\n",
      "Epoch: 71/100...  Loss: 0.0047\n",
      "Epoch: 72/100...  Loss: 0.0051\n",
      "Epoch: 72/100...  Loss: 0.0048\n",
      "Epoch: 72/100...  Loss: 0.0052\n",
      "Epoch: 72/100...  Loss: 0.0053\n",
      "Epoch: 72/100...  Loss: 0.0047\n",
      "Epoch: 72/100...  Loss: 0.0048\n",
      "Epoch: 72/100...  Loss: 0.0048\n",
      "Epoch: 72/100...  Loss: 0.0049\n",
      "Epoch: 72/100...  Loss: 0.0048\n",
      "Epoch: 72/100...  Loss: 0.0046\n",
      "Epoch: 73/100...  Loss: 0.0049\n",
      "Epoch: 73/100...  Loss: 0.0046\n",
      "Epoch: 73/100...  Loss: 0.0051\n",
      "Epoch: 73/100...  Loss: 0.0051\n",
      "Epoch: 73/100...  Loss: 0.0046\n",
      "Epoch: 73/100...  Loss: 0.0047\n",
      "Epoch: 73/100...  Loss: 0.0046\n",
      "Epoch: 73/100...  Loss: 0.0047\n",
      "Epoch: 73/100...  Loss: 0.0046\n",
      "Epoch: 73/100...  Loss: 0.0044\n",
      "Epoch: 74/100...  Loss: 0.0048\n",
      "Epoch: 74/100...  Loss: 0.0045\n",
      "Epoch: 74/100...  Loss: 0.0049\n",
      "Epoch: 74/100...  Loss: 0.0050\n",
      "Epoch: 74/100...  Loss: 0.0045\n",
      "Epoch: 74/100...  Loss: 0.0045\n",
      "Epoch: 74/100...  Loss: 0.0045\n",
      "Epoch: 74/100...  Loss: 0.0046\n",
      "Epoch: 74/100...  Loss: 0.0045\n",
      "Epoch: 74/100...  Loss: 0.0043\n",
      "Epoch: 75/100...  Loss: 0.0046\n",
      "Epoch: 75/100...  Loss: 0.0043\n",
      "Epoch: 75/100...  Loss: 0.0048\n",
      "Epoch: 75/100...  Loss: 0.0048\n",
      "Epoch: 75/100...  Loss: 0.0043\n",
      "Epoch: 75/100...  Loss: 0.0044\n",
      "Epoch: 75/100...  Loss: 0.0044\n",
      "Epoch: 75/100...  Loss: 0.0045\n",
      "Epoch: 75/100...  Loss: 0.0044\n",
      "Epoch: 75/100...  Loss: 0.0042\n",
      "Epoch: 76/100...  Loss: 0.0045\n",
      "Epoch: 76/100...  Loss: 0.0042\n",
      "Epoch: 76/100...  Loss: 0.0046\n",
      "Epoch: 76/100...  Loss: 0.0047\n",
      "Epoch: 76/100...  Loss: 0.0042\n",
      "Epoch: 76/100...  Loss: 0.0043\n",
      "Epoch: 76/100...  Loss: 0.0042\n",
      "Epoch: 76/100...  Loss: 0.0043\n",
      "Epoch: 76/100...  Loss: 0.0043\n",
      "Epoch: 76/100...  Loss: 0.0041\n",
      "Epoch: 77/100...  Loss: 0.0044\n",
      "Epoch: 77/100...  Loss: 0.0041\n",
      "Epoch: 77/100...  Loss: 0.0045\n",
      "Epoch: 77/100...  Loss: 0.0046\n",
      "Epoch: 77/100...  Loss: 0.0041\n",
      "Epoch: 77/100...  Loss: 0.0042\n",
      "Epoch: 77/100...  Loss: 0.0041\n",
      "Epoch: 77/100...  Loss: 0.0042\n",
      "Epoch: 77/100...  Loss: 0.0041\n",
      "Epoch: 77/100...  Loss: 0.0040\n",
      "Epoch: 78/100...  Loss: 0.0042\n",
      "Epoch: 78/100...  Loss: 0.0040\n",
      "Epoch: 78/100...  Loss: 0.0044\n",
      "Epoch: 78/100...  Loss: 0.0044\n",
      "Epoch: 78/100...  Loss: 0.0040\n",
      "Epoch: 78/100...  Loss: 0.0041\n",
      "Epoch: 78/100...  Loss: 0.0040\n",
      "Epoch: 78/100...  Loss: 0.0041\n",
      "Epoch: 78/100...  Loss: 0.0040\n",
      "Epoch: 78/100...  Loss: 0.0039\n",
      "Epoch: 79/100...  Loss: 0.0041\n",
      "Epoch: 79/100...  Loss: 0.0039\n",
      "Epoch: 79/100...  Loss: 0.0043\n",
      "Epoch: 79/100...  Loss: 0.0043\n",
      "Epoch: 79/100...  Loss: 0.0039\n",
      "Epoch: 79/100...  Loss: 0.0040\n",
      "Epoch: 79/100...  Loss: 0.0039\n",
      "Epoch: 79/100...  Loss: 0.0040\n",
      "Epoch: 79/100...  Loss: 0.0039\n",
      "Epoch: 79/100...  Loss: 0.0038\n",
      "Epoch: 80/100...  Loss: 0.0040\n",
      "Epoch: 80/100...  Loss: 0.0038\n",
      "Epoch: 80/100...  Loss: 0.0042\n",
      "Epoch: 80/100...  Loss: 0.0042\n",
      "Epoch: 80/100...  Loss: 0.0038\n",
      "Epoch: 80/100...  Loss: 0.0039\n",
      "Epoch: 80/100...  Loss: 0.0038\n",
      "Epoch: 80/100...  Loss: 0.0039\n",
      "Epoch: 80/100...  Loss: 0.0038\n",
      "Epoch: 80/100...  Loss: 0.0037\n",
      "Epoch: 81/100...  Loss: 0.0039\n",
      "Epoch: 81/100...  Loss: 0.0037\n",
      "Epoch: 81/100...  Loss: 0.0040\n",
      "Epoch: 81/100...  Loss: 0.0041\n",
      "Epoch: 81/100...  Loss: 0.0037\n",
      "Epoch: 81/100...  Loss: 0.0038\n",
      "Epoch: 81/100...  Loss: 0.0037\n",
      "Epoch: 81/100...  Loss: 0.0038\n",
      "Epoch: 81/100...  Loss: 0.0037\n",
      "Epoch: 81/100...  Loss: 0.0036\n",
      "Epoch: 82/100...  Loss: 0.0038\n",
      "Epoch: 82/100...  Loss: 0.0036\n",
      "Epoch: 82/100...  Loss: 0.0040\n",
      "Epoch: 82/100...  Loss: 0.0040\n",
      "Epoch: 82/100...  Loss: 0.0036\n",
      "Epoch: 82/100...  Loss: 0.0037\n",
      "Epoch: 82/100...  Loss: 0.0036\n",
      "Epoch: 82/100...  Loss: 0.0037\n",
      "Epoch: 82/100...  Loss: 0.0037\n",
      "Epoch: 82/100...  Loss: 0.0035\n",
      "Epoch: 83/100...  Loss: 0.0037\n",
      "Epoch: 83/100...  Loss: 0.0035\n",
      "Epoch: 83/100...  Loss: 0.0039\n",
      "Epoch: 83/100...  Loss: 0.0039\n",
      "Epoch: 83/100...  Loss: 0.0035\n",
      "Epoch: 83/100...  Loss: 0.0036\n",
      "Epoch: 83/100...  Loss: 0.0035\n",
      "Epoch: 83/100...  Loss: 0.0036\n",
      "Epoch: 83/100...  Loss: 0.0036\n",
      "Epoch: 83/100...  Loss: 0.0034\n",
      "Epoch: 84/100...  Loss: 0.0036\n",
      "Epoch: 84/100...  Loss: 0.0034\n",
      "Epoch: 84/100...  Loss: 0.0038\n",
      "Epoch: 84/100...  Loss: 0.0038\n",
      "Epoch: 84/100...  Loss: 0.0034\n",
      "Epoch: 84/100...  Loss: 0.0035\n",
      "Epoch: 84/100...  Loss: 0.0034\n",
      "Epoch: 84/100...  Loss: 0.0035\n",
      "Epoch: 84/100...  Loss: 0.0035\n",
      "Epoch: 84/100...  Loss: 0.0033\n",
      "Epoch: 85/100...  Loss: 0.0035\n",
      "Epoch: 85/100...  Loss: 0.0033\n",
      "Epoch: 85/100...  Loss: 0.0037\n",
      "Epoch: 85/100...  Loss: 0.0037\n",
      "Epoch: 85/100...  Loss: 0.0033\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 85/100...  Loss: 0.0034\n",
      "Epoch: 85/100...  Loss: 0.0033\n",
      "Epoch: 86/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0032\n",
      "Epoch: 86/100...  Loss: 0.0036\n",
      "Epoch: 86/100...  Loss: 0.0036\n",
      "Epoch: 86/100...  Loss: 0.0033\n",
      "Epoch: 86/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0033\n",
      "Epoch: 86/100...  Loss: 0.0034\n",
      "Epoch: 86/100...  Loss: 0.0033\n",
      "Epoch: 86/100...  Loss: 0.0032\n",
      "Epoch: 87/100...  Loss: 0.0034\n",
      "Epoch: 87/100...  Loss: 0.0032\n",
      "Epoch: 87/100...  Loss: 0.0035\n",
      "Epoch: 87/100...  Loss: 0.0036\n",
      "Epoch: 87/100...  Loss: 0.0032\n",
      "Epoch: 87/100...  Loss: 0.0033\n",
      "Epoch: 87/100...  Loss: 0.0032\n",
      "Epoch: 87/100...  Loss: 0.0033\n",
      "Epoch: 87/100...  Loss: 0.0033\n",
      "Epoch: 87/100...  Loss: 0.0031\n",
      "Epoch: 88/100...  Loss: 0.0033\n",
      "Epoch: 88/100...  Loss: 0.0031\n",
      "Epoch: 88/100...  Loss: 0.0034\n",
      "Epoch: 88/100...  Loss: 0.0035\n",
      "Epoch: 88/100...  Loss: 0.0031\n",
      "Epoch: 88/100...  Loss: 0.0032\n",
      "Epoch: 88/100...  Loss: 0.0031\n",
      "Epoch: 88/100...  Loss: 0.0032\n",
      "Epoch: 88/100...  Loss: 0.0032\n",
      "Epoch: 88/100...  Loss: 0.0030\n",
      "Epoch: 89/100...  Loss: 0.0032\n",
      "Epoch: 89/100...  Loss: 0.0030\n",
      "Epoch: 89/100...  Loss: 0.0033\n",
      "Epoch: 89/100...  Loss: 0.0034\n",
      "Epoch: 89/100...  Loss: 0.0030\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 89/100...  Loss: 0.0031\n",
      "Epoch: 89/100...  Loss: 0.0030\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0030\n",
      "Epoch: 90/100...  Loss: 0.0033\n",
      "Epoch: 90/100...  Loss: 0.0033\n",
      "Epoch: 90/100...  Loss: 0.0030\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0030\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0031\n",
      "Epoch: 90/100...  Loss: 0.0029\n",
      "Epoch: 91/100...  Loss: 0.0031\n",
      "Epoch: 91/100...  Loss: 0.0029\n",
      "Epoch: 91/100...  Loss: 0.0032\n",
      "Epoch: 91/100...  Loss: 0.0032\n",
      "Epoch: 91/100...  Loss: 0.0029\n",
      "Epoch: 91/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0029\n",
      "Epoch: 91/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0030\n",
      "Epoch: 91/100...  Loss: 0.0029\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 92/100...  Loss: 0.0031\n",
      "Epoch: 92/100...  Loss: 0.0032\n",
      "Epoch: 92/100...  Loss: 0.0028\n",
      "Epoch: 92/100...  Loss: 0.0030\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 92/100...  Loss: 0.0029\n",
      "Epoch: 92/100...  Loss: 0.0028\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0028\n",
      "Epoch: 93/100...  Loss: 0.0031\n",
      "Epoch: 93/100...  Loss: 0.0031\n",
      "Epoch: 93/100...  Loss: 0.0028\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0028\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0029\n",
      "Epoch: 93/100...  Loss: 0.0027\n",
      "Epoch: 94/100...  Loss: 0.0029\n",
      "Epoch: 94/100...  Loss: 0.0027\n",
      "Epoch: 94/100...  Loss: 0.0030\n",
      "Epoch: 94/100...  Loss: 0.0030\n",
      "Epoch: 94/100...  Loss: 0.0027\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0028\n",
      "Epoch: 94/100...  Loss: 0.0027\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0027\n",
      "Epoch: 95/100...  Loss: 0.0029\n",
      "Epoch: 95/100...  Loss: 0.0030\n",
      "Epoch: 95/100...  Loss: 0.0027\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0027\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0028\n",
      "Epoch: 95/100...  Loss: 0.0026\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0026\n",
      "Epoch: 96/100...  Loss: 0.0029\n",
      "Epoch: 96/100...  Loss: 0.0029\n",
      "Epoch: 96/100...  Loss: 0.0026\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0027\n",
      "Epoch: 96/100...  Loss: 0.0026\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 97/100...  Loss: 0.0028\n",
      "Epoch: 97/100...  Loss: 0.0029\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 97/100...  Loss: 0.0027\n",
      "Epoch: 97/100...  Loss: 0.0026\n",
      "Epoch: 97/100...  Loss: 0.0025\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0025\n",
      "Epoch: 98/100...  Loss: 0.0028\n",
      "Epoch: 98/100...  Loss: 0.0028\n",
      "Epoch: 98/100...  Loss: 0.0025\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0026\n",
      "Epoch: 98/100...  Loss: 0.0025\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 99/100...  Loss: 0.0027\n",
      "Epoch: 99/100...  Loss: 0.0028\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 99/100...  Loss: 0.0026\n",
      "Epoch: 99/100...  Loss: 0.0025\n",
      "Epoch: 99/100...  Loss: 0.0024\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0024\n",
      "Epoch: 100/100...  Loss: 0.0027\n",
      "Epoch: 100/100...  Loss: 0.0027\n",
      "Epoch: 100/100...  Loss: 0.0024\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0025\n",
      "Epoch: 100/100...  Loss: 0.0024\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "def save_model(model, architecture, input_size, output_size, dropout, filepath = 'checkpoint.pth'):\n",
    "    print(\"Saving model to {}\\n\".format(filepath))\n",
    "    if architecture == 'nn':\n",
    "        checkpoint = {'input_size': input_size,\n",
    "                  'output_size': output_size,\n",
    "                  'dropout': dropout,\n",
    "                  'state_dict': model.state_dict()}\n",
    "\n",
    "        torch.save(checkpoint, filepath)\n",
    "    else:\n",
    "        checkpoint = {'state_dict': model.state_dict()}\n",
    "        torch.save(checkpoint, filepath)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "save_model(rnn_model, 'nn', 784, 10, 0.0, 'checkpoint.pth')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving model to checkpoint.pth\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "def load_model(architecture = 'nn', filepath = 'checkpoint.pth'):\n",
    "    print(\"Loading model from {} \\n\".format(filepath))\n",
    "\n",
    "    if architecture == 'nn':\n",
    "        checkpoint = torch.load(filepath)\n",
    "        input_size = checkpoint['input_size']\n",
    "        output_size = checkpoint['output_size']\n",
    "        dropout = checkpoint['dropout']\n",
    "        model = nn.Sequential(OrderedDict([\n",
    "                              ('fc1', nn.Linear(input_size, 128)),\n",
    "                              ('relu1', nn.ReLU()),\n",
    "                              ('fc2', nn.Linear(128, 100)),\n",
    "                              ('bn2', nn.BatchNorm1d(num_features=100)),\n",
    "                              ('relu2', nn.ReLU()),\n",
    "                              ('dropout', nn.Dropout(dropout)),\n",
    "                              ('fc3', nn.Linear(100, 64)),\n",
    "                              ('bn3', nn.BatchNorm1d(num_features=64)),\n",
    "                              ('relu3', nn.ReLU()),\n",
    "                              ('logits', nn.Linear(64, output_size))]))\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    else:\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model = SimpleCNN()\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "model = load_model(architecture = 'nn', filepath = 'checkpoint.pth')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading model from checkpoint.pth \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "def get_preds(model, input, architecture = 'nn'):\n",
    "    # Turn off gradients to speed up this part\n",
    "    with torch.no_grad():\n",
    "        if architecture == 'nn':\n",
    "            logits = model.forward(input)\n",
    "        else:\n",
    "            # image = input.numpy()\n",
    "            image = input.reshape(input.shape[0], 1, 28, 28)\n",
    "            logits = model.forward(torch.from_numpy(image).float())\n",
    "\n",
    "    ps = F.softmax(logits, dim=1)\n",
    "    return ps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "def evaluate_model(model, train, y_train, test, y_test, architecture = 'nn'):\n",
    "    train_pred = get_preds(model, train, architecture)\n",
    "    train_pred_labels = get_labels(train_pred)\n",
    "\n",
    "    test_pred = get_preds(model, test, architecture)\n",
    "    test_pred_labels = get_labels(test_pred)\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, train_pred_labels)\n",
    "    accuracy_test = accuracy_score(y_test, test_pred_labels)\n",
    "\n",
    "    print(\"Accuracy score for train set is {} \\n\".format(accuracy_train))\n",
    "    print(\"Accuracy score for test set is {} \\n\".format(accuracy_test))\n",
    "\n",
    "    return accuracy_train, accuracy_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "def get_labels(pred):\n",
    "    pred_np = pred.numpy()\n",
    "    pred_values = np.amax(pred_np, axis=1, keepdims=True)\n",
    "    pred_labels = np.array([np.where(pred_np[i, :] == pred_values[i, :])[0] for i in range(pred_np.shape[0])])\n",
    "    pred_labels = pred_labels.reshape(len(pred_np), 1)\n",
    "\n",
    "    return pred_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "y_test = y_test.reshape((X_test.shape[0], 1))\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "y_test_numpy = y_test.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "evaluate_model(model, X_train_shuffled, y_train_shuffled, X_test, y_test, architecture='nn')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy score for train set is 0.9446190476190476 \n",
      "\n",
      "Accuracy score for test set is 0.5308888888888889 \n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.9446190476190476, 0.5308888888888889)"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "evaluate_model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function __main__.evaluate_model(model, train, y_train, test, y_test, architecture='nn')>"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "def plot_learning_curve(input_size, output_size, hidden_sizes, train, labels, y_train, test, y_test, learning_rate = 0.003, weight_decay = 0.0, dropout = 0.0, n_chunks = 1000, optimizer = 'SGD'):\n",
    "    \"\"\"\n",
    "    Function to plot learning curve depending on the number of epochs.\n",
    "\n",
    "    INPUT:\n",
    "        input_size, output_size, hidden_sizes - model parameters\n",
    "        train - (tensor) train dataset\n",
    "        labels - (tensor) labels for train dataset\n",
    "        y_train - (numpy) labels for train dataset\n",
    "        test - (tensor) test dataset\n",
    "        y_test - (numpy) labels for test dataset\n",
    "        learning_rate - learning rate hyperparameter\n",
    "        weight_decay - weight decay (regularization)\n",
    "        dropout - dropout for hidden layer\n",
    "        n_chunks - the number of minibatches to train the model\n",
    "        optimizer - optimizer to be used for training (SGD or Adam)\n",
    "\n",
    "    OUTPUT: None\n",
    "    \"\"\"\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "\n",
    "    for epochs in np.arange(10, 210, 10):\n",
    "        # create model\n",
    "        modle = build_model(input_size, output_size, hidden_sizes, dropout = dropout)\n",
    "\n",
    "        # fit model\n",
    "        fit_model(modle, train, labels, epochs = epochs, n_chunks = n_chunks, learning_rate = learning_rate, weight_decay = weight_decay, optimizer = optimizer)\n",
    "        # get accuracy\n",
    "        accuracy_train, accuracy_test = evaluate_model(modle, train, y_train, test, y_test)\n",
    "\n",
    "        train_acc.append(accuracy_train)\n",
    "        test_acc.append(accuracy_test)\n",
    "\n",
    "    # Plot curve\n",
    "    x = np.arange(10, 210, 10)\n",
    "    plt.plot(x, train_acc)\n",
    "    plt.plot(x, test_acc)\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.title('Accuracy, learning_rate = ' + str(learning_rate), fontsize=20)\n",
    "    plt.xlabel('Number of epochs', fontsize=14)\n",
    "    plt.ylabel('Accuracy', fontsize=14)\n",
    "\n",
    "    ts = time.time()\n",
    "    plt.savefig('learning_curve' + str(ts) + '.png')\n",
    "\n",
    "    df = pd.DataFrame.from_dict({'train' : train_acc, 'test' :test_acc})\n",
    "    df.to_csv('learning_curve_' + str(ts) + '.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "plot_learning_curve(input_size=784, output_size=10, hidden_sizes=[128, 100, 64], \n",
    "                    train=X_train_shuffled, labels=y_train_shuffled, \n",
    "                    y_train=y_train_numpy, \n",
    "                    test=X_test, y_test=y_test)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "UnboundLocalError",
     "evalue": "local variable 'model' referenced before assignment",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-4017d1845fc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_shuffled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train_shuffled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train_numpy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     test=X_test, y_test=y_test)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-9ebd4db13ec6>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[0;34m(input_size, output_size, hidden_sizes, train, labels, y_train, test, y_test, learning_rate, weight_decay, dropout, n_chunks, optimizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m210\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mmodle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-496da0b59cf6>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(input_size, output_size, architecture, dropout)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Build a simple convolutional network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'model' referenced before assignment"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "X_train[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "def view_image(img, filename = 'image'):\n",
    "    fig, ax = plt.subplots(figsize=(6,9))\n",
    "    ax.imshow(img.reshape(28, 28).squeeze())\n",
    "    ax.axis('off')\n",
    "    plt.savefig(filename + '.png')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "view_image(X_train[0], filename='train_0')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x648 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"349.2pt\" version=\"1.1\" viewBox=\"0 0 349.2 349.2\" width=\"349.2pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-21T13:59:54.002363</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 349.2 \nL 349.2 349.2 \nL 349.2 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g clip-path=\"url(#pa6a6541629)\">\n    <image height=\"335\" id=\"imageb534eb6703\" transform=\"scale(1 -1)translate(0 -335)\" width=\"335\" x=\"7.2\" xlink:href=\"data:image/png;base64,\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\" y=\"-7\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pa6a6541629\">\n   <rect height=\"334.8\" width=\"334.8\" x=\"7.2\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "def fit_conv(model, X_train, y_train, epochs = 100, n_chunks = 1000, learning_rate = 0.003, weight_decay = 0, optimizer = 'SGD'):\n",
    "\n",
    "    print(\"Fitting model with epochs = {epochs}, learning rate = {lr}\\n\"\\\n",
    "    .format(epochs = epochs, lr = learning_rate))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if (optimizer == 'SGD'):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "\n",
    "    print_every = 100\n",
    "    steps = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        images = torch.chunk(X_train, n_chunks)\n",
    "        labels = torch.chunk(y_train, n_chunks)\n",
    "\n",
    "        for i in range(n_chunks):\n",
    "            steps += 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward passes\n",
    "            np_images = images[i].numpy()\n",
    "            np_images = np_images.reshape(images[i].shape[0], 1, 28, 28)\n",
    "            img = torch.from_numpy(np_images).float()\n",
    "\n",
    "            output = model.forward(img)\n",
    "            loss = criterion(output, labels[i].squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                      \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "\n",
    "                running_loss = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "X_train_shuffled_numpy = X_train_shuffled.numpy()\n",
    "y_train_shuffled_numpy = y_train_shuffled.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "fit_conv(cnn_model, X_train_shuffled, y_train_shuffled, optimizer='Adam')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting model with epochs = 100, learning rate = 0.003\n",
      "\n",
      "Epoch: 1/100...  Loss: 1.8543\n",
      "Epoch: 1/100...  Loss: 1.5091\n",
      "Epoch: 1/100...  Loss: 1.3295\n",
      "Epoch: 1/100...  Loss: 1.2844\n",
      "Epoch: 1/100...  Loss: 1.1564\n",
      "Epoch: 1/100...  Loss: 1.1527\n",
      "Epoch: 1/100...  Loss: 1.0981\n",
      "Epoch: 1/100...  Loss: 1.1087\n",
      "Epoch: 1/100...  Loss: 1.0981\n",
      "Epoch: 1/100...  Loss: 1.0373\n",
      "Epoch: 2/100...  Loss: 1.0123\n",
      "Epoch: 2/100...  Loss: 0.9992\n",
      "Epoch: 2/100...  Loss: 0.9579\n",
      "Epoch: 2/100...  Loss: 0.9751\n",
      "Epoch: 2/100...  Loss: 0.9313\n",
      "Epoch: 2/100...  Loss: 0.9367\n",
      "Epoch: 2/100...  Loss: 0.9022\n",
      "Epoch: 2/100...  Loss: 0.9105\n",
      "Epoch: 2/100...  Loss: 0.9232\n",
      "Epoch: 2/100...  Loss: 0.8795\n",
      "Epoch: 3/100...  Loss: 0.8707\n",
      "Epoch: 3/100...  Loss: 0.8727\n",
      "Epoch: 3/100...  Loss: 0.8319\n",
      "Epoch: 3/100...  Loss: 0.8449\n",
      "Epoch: 3/100...  Loss: 0.8190\n",
      "Epoch: 3/100...  Loss: 0.8216\n",
      "Epoch: 3/100...  Loss: 0.7981\n",
      "Epoch: 3/100...  Loss: 0.7970\n",
      "Epoch: 3/100...  Loss: 0.8114\n",
      "Epoch: 3/100...  Loss: 0.7729\n",
      "Epoch: 4/100...  Loss: 0.7799\n",
      "Epoch: 4/100...  Loss: 0.7743\n",
      "Epoch: 4/100...  Loss: 0.7200\n",
      "Epoch: 4/100...  Loss: 0.7575\n",
      "Epoch: 4/100...  Loss: 0.7402\n",
      "Epoch: 4/100...  Loss: 0.7288\n",
      "Epoch: 4/100...  Loss: 0.7144\n",
      "Epoch: 4/100...  Loss: 0.7034\n",
      "Epoch: 4/100...  Loss: 0.7155\n",
      "Epoch: 4/100...  Loss: 0.6795\n",
      "Epoch: 5/100...  Loss: 0.6958\n",
      "Epoch: 5/100...  Loss: 0.6886\n",
      "Epoch: 5/100...  Loss: 0.6288\n",
      "Epoch: 5/100...  Loss: 0.6873\n",
      "Epoch: 5/100...  Loss: 0.6569\n",
      "Epoch: 5/100...  Loss: 0.6513\n",
      "Epoch: 5/100...  Loss: 0.6349\n",
      "Epoch: 5/100...  Loss: 0.6228\n",
      "Epoch: 5/100...  Loss: 0.6331\n",
      "Epoch: 5/100...  Loss: 0.5985\n",
      "Epoch: 6/100...  Loss: 0.6106\n",
      "Epoch: 6/100...  Loss: 0.6123\n",
      "Epoch: 6/100...  Loss: 0.5488\n",
      "Epoch: 6/100...  Loss: 0.6213\n",
      "Epoch: 6/100...  Loss: 0.5832\n",
      "Epoch: 6/100...  Loss: 0.5719\n",
      "Epoch: 6/100...  Loss: 0.5612\n",
      "Epoch: 6/100...  Loss: 0.5559\n",
      "Epoch: 6/100...  Loss: 0.5656\n",
      "Epoch: 6/100...  Loss: 0.5257\n",
      "Epoch: 7/100...  Loss: 0.5377\n",
      "Epoch: 7/100...  Loss: 0.5368\n",
      "Epoch: 7/100...  Loss: 0.4785\n",
      "Epoch: 7/100...  Loss: 0.5490\n",
      "Epoch: 7/100...  Loss: 0.5228\n",
      "Epoch: 7/100...  Loss: 0.5127\n",
      "Epoch: 7/100...  Loss: 0.4782\n",
      "Epoch: 7/100...  Loss: 0.4717\n",
      "Epoch: 7/100...  Loss: 0.4911\n",
      "Epoch: 7/100...  Loss: 0.4472\n",
      "Epoch: 8/100...  Loss: 0.4792\n",
      "Epoch: 8/100...  Loss: 0.4637\n",
      "Epoch: 8/100...  Loss: 0.4177\n",
      "Epoch: 8/100...  Loss: 0.4889\n",
      "Epoch: 8/100...  Loss: 0.4628\n",
      "Epoch: 8/100...  Loss: 0.4651\n",
      "Epoch: 8/100...  Loss: 0.4239\n",
      "Epoch: 8/100...  Loss: 0.4112\n",
      "Epoch: 8/100...  Loss: 0.4283\n",
      "Epoch: 8/100...  Loss: 0.3926\n",
      "Epoch: 9/100...  Loss: 0.4332\n",
      "Epoch: 9/100...  Loss: 0.4075\n",
      "Epoch: 9/100...  Loss: 0.3739\n",
      "Epoch: 9/100...  Loss: 0.4246\n",
      "Epoch: 9/100...  Loss: 0.4085\n",
      "Epoch: 9/100...  Loss: 0.4064\n",
      "Epoch: 9/100...  Loss: 0.3686\n",
      "Epoch: 9/100...  Loss: 0.3737\n",
      "Epoch: 9/100...  Loss: 0.3791\n",
      "Epoch: 9/100...  Loss: 0.3486\n",
      "Epoch: 10/100...  Loss: 0.3904\n",
      "Epoch: 10/100...  Loss: 0.3688\n",
      "Epoch: 10/100...  Loss: 0.3260\n",
      "Epoch: 10/100...  Loss: 0.3891\n",
      "Epoch: 10/100...  Loss: 0.3707\n",
      "Epoch: 10/100...  Loss: 0.3477\n",
      "Epoch: 10/100...  Loss: 0.3271\n",
      "Epoch: 10/100...  Loss: 0.3385\n",
      "Epoch: 10/100...  Loss: 0.3456\n",
      "Epoch: 10/100...  Loss: 0.3178\n",
      "Epoch: 11/100...  Loss: 0.3325\n",
      "Epoch: 11/100...  Loss: 0.3186\n",
      "Epoch: 11/100...  Loss: 0.2869\n",
      "Epoch: 11/100...  Loss: 0.3331\n",
      "Epoch: 11/100...  Loss: 0.3121\n",
      "Epoch: 11/100...  Loss: 0.3063\n",
      "Epoch: 11/100...  Loss: 0.3072\n",
      "Epoch: 11/100...  Loss: 0.3185\n",
      "Epoch: 11/100...  Loss: 0.3174\n",
      "Epoch: 11/100...  Loss: 0.2893\n",
      "Epoch: 12/100...  Loss: 0.2728\n",
      "Epoch: 12/100...  Loss: 0.2812\n",
      "Epoch: 12/100...  Loss: 0.2697\n",
      "Epoch: 12/100...  Loss: 0.3265\n",
      "Epoch: 12/100...  Loss: 0.2949\n",
      "Epoch: 12/100...  Loss: 0.2651\n",
      "Epoch: 12/100...  Loss: 0.2929\n",
      "Epoch: 12/100...  Loss: 0.3258\n",
      "Epoch: 12/100...  Loss: 0.2672\n",
      "Epoch: 12/100...  Loss: 0.2662\n",
      "Epoch: 13/100...  Loss: 0.2631\n",
      "Epoch: 13/100...  Loss: 0.2537\n",
      "Epoch: 13/100...  Loss: 0.2407\n",
      "Epoch: 13/100...  Loss: 0.2890\n",
      "Epoch: 13/100...  Loss: 0.2795\n",
      "Epoch: 13/100...  Loss: 0.2808\n",
      "Epoch: 13/100...  Loss: 0.2588\n",
      "Epoch: 13/100...  Loss: 0.2574\n",
      "Epoch: 13/100...  Loss: 0.2603\n",
      "Epoch: 13/100...  Loss: 0.2466\n",
      "Epoch: 14/100...  Loss: 0.2907\n",
      "Epoch: 14/100...  Loss: 0.2596\n",
      "Epoch: 14/100...  Loss: 0.2235\n",
      "Epoch: 14/100...  Loss: 0.2621\n",
      "Epoch: 14/100...  Loss: 0.2383\n",
      "Epoch: 14/100...  Loss: 0.2234\n",
      "Epoch: 14/100...  Loss: 0.2199\n",
      "Epoch: 14/100...  Loss: 0.2321\n",
      "Epoch: 14/100...  Loss: 0.2025\n",
      "Epoch: 14/100...  Loss: 0.1872\n",
      "Epoch: 15/100...  Loss: 0.2141\n",
      "Epoch: 15/100...  Loss: 0.2221\n",
      "Epoch: 15/100...  Loss: 0.1833\n",
      "Epoch: 15/100...  Loss: 0.2414\n",
      "Epoch: 15/100...  Loss: 0.2075\n",
      "Epoch: 15/100...  Loss: 0.1940\n",
      "Epoch: 15/100...  Loss: 0.2106\n",
      "Epoch: 15/100...  Loss: 0.2228\n",
      "Epoch: 15/100...  Loss: 0.1807\n",
      "Epoch: 15/100...  Loss: 0.1752\n",
      "Epoch: 16/100...  Loss: 0.1999\n",
      "Epoch: 16/100...  Loss: 0.1755\n",
      "Epoch: 16/100...  Loss: 0.1897\n",
      "Epoch: 16/100...  Loss: 0.2259\n",
      "Epoch: 16/100...  Loss: 0.2065\n",
      "Epoch: 16/100...  Loss: 0.1920\n",
      "Epoch: 16/100...  Loss: 0.1930\n",
      "Epoch: 16/100...  Loss: 0.2339\n",
      "Epoch: 16/100...  Loss: 0.1977\n",
      "Epoch: 16/100...  Loss: 0.1743\n",
      "Epoch: 17/100...  Loss: 0.2023\n",
      "Epoch: 17/100...  Loss: 0.1832\n",
      "Epoch: 17/100...  Loss: 0.1614\n",
      "Epoch: 17/100...  Loss: 0.1777\n",
      "Epoch: 17/100...  Loss: 0.1747\n",
      "Epoch: 17/100...  Loss: 0.1504\n",
      "Epoch: 17/100...  Loss: 0.1767\n",
      "Epoch: 17/100...  Loss: 0.1882\n",
      "Epoch: 17/100...  Loss: 0.1606\n",
      "Epoch: 17/100...  Loss: 0.1463\n",
      "Epoch: 18/100...  Loss: 0.1541\n",
      "Epoch: 18/100...  Loss: 0.1782\n",
      "Epoch: 18/100...  Loss: 0.1346\n",
      "Epoch: 18/100...  Loss: 0.1655\n",
      "Epoch: 18/100...  Loss: 0.1636\n",
      "Epoch: 18/100...  Loss: 0.1478\n",
      "Epoch: 18/100...  Loss: 0.1624\n",
      "Epoch: 18/100...  Loss: 0.1767\n",
      "Epoch: 18/100...  Loss: 0.1491\n",
      "Epoch: 18/100...  Loss: 0.1119\n",
      "Epoch: 19/100...  Loss: 0.1762\n",
      "Epoch: 19/100...  Loss: 0.1518\n",
      "Epoch: 19/100...  Loss: 0.1504\n",
      "Epoch: 19/100...  Loss: 0.1770\n",
      "Epoch: 19/100...  Loss: 0.1331\n",
      "Epoch: 19/100...  Loss: 0.1614\n",
      "Epoch: 19/100...  Loss: 0.1373\n",
      "Epoch: 19/100...  Loss: 0.1586\n",
      "Epoch: 19/100...  Loss: 0.1337\n",
      "Epoch: 19/100...  Loss: 0.1250\n",
      "Epoch: 20/100...  Loss: 0.1547\n",
      "Epoch: 20/100...  Loss: 0.1597\n",
      "Epoch: 20/100...  Loss: 0.1333\n",
      "Epoch: 20/100...  Loss: 0.1459\n",
      "Epoch: 20/100...  Loss: 0.1581\n",
      "Epoch: 20/100...  Loss: 0.1565\n",
      "Epoch: 20/100...  Loss: 0.1623\n",
      "Epoch: 20/100...  Loss: 0.1442\n",
      "Epoch: 20/100...  Loss: 0.1292\n",
      "Epoch: 20/100...  Loss: 0.1204\n",
      "Epoch: 21/100...  Loss: 0.1517\n",
      "Epoch: 21/100...  Loss: 0.1734\n",
      "Epoch: 21/100...  Loss: 0.1687\n",
      "Epoch: 21/100...  Loss: 0.1626\n",
      "Epoch: 21/100...  Loss: 0.1151\n",
      "Epoch: 21/100...  Loss: 0.1312\n",
      "Epoch: 21/100...  Loss: 0.1121\n",
      "Epoch: 21/100...  Loss: 0.1351\n",
      "Epoch: 21/100...  Loss: 0.1051\n",
      "Epoch: 21/100...  Loss: 0.0956\n",
      "Epoch: 22/100...  Loss: 0.1114\n",
      "Epoch: 22/100...  Loss: 0.1442\n",
      "Epoch: 22/100...  Loss: 0.1271\n",
      "Epoch: 22/100...  Loss: 0.1450\n",
      "Epoch: 22/100...  Loss: 0.1156\n",
      "Epoch: 22/100...  Loss: 0.1382\n",
      "Epoch: 22/100...  Loss: 0.1347\n",
      "Epoch: 22/100...  Loss: 0.1422\n",
      "Epoch: 22/100...  Loss: 0.1371\n",
      "Epoch: 22/100...  Loss: 0.1157\n",
      "Epoch: 23/100...  Loss: 0.1792\n",
      "Epoch: 23/100...  Loss: 0.1257\n",
      "Epoch: 23/100...  Loss: 0.1244\n",
      "Epoch: 23/100...  Loss: 0.1293\n",
      "Epoch: 23/100...  Loss: 0.1259\n",
      "Epoch: 23/100...  Loss: 0.1245\n",
      "Epoch: 23/100...  Loss: 0.1180\n",
      "Epoch: 23/100...  Loss: 0.1156\n",
      "Epoch: 23/100...  Loss: 0.1275\n",
      "Epoch: 23/100...  Loss: 0.1135\n",
      "Epoch: 24/100...  Loss: 0.1144\n",
      "Epoch: 24/100...  Loss: 0.1469\n",
      "Epoch: 24/100...  Loss: 0.1312\n",
      "Epoch: 24/100...  Loss: 0.1290\n",
      "Epoch: 24/100...  Loss: 0.1104\n",
      "Epoch: 24/100...  Loss: 0.1191\n",
      "Epoch: 24/100...  Loss: 0.1255\n",
      "Epoch: 24/100...  Loss: 0.1162\n",
      "Epoch: 24/100...  Loss: 0.1278\n",
      "Epoch: 24/100...  Loss: 0.0839\n",
      "Epoch: 25/100...  Loss: 0.1135\n",
      "Epoch: 25/100...  Loss: 0.1179\n",
      "Epoch: 25/100...  Loss: 0.1300\n",
      "Epoch: 25/100...  Loss: 0.0945\n",
      "Epoch: 25/100...  Loss: 0.0959\n",
      "Epoch: 25/100...  Loss: 0.0947\n",
      "Epoch: 25/100...  Loss: 0.1113\n",
      "Epoch: 25/100...  Loss: 0.1180\n",
      "Epoch: 25/100...  Loss: 0.0690\n",
      "Epoch: 25/100...  Loss: 0.0814\n",
      "Epoch: 26/100...  Loss: 0.1191\n",
      "Epoch: 26/100...  Loss: 0.1384\n",
      "Epoch: 26/100...  Loss: 0.1035\n",
      "Epoch: 26/100...  Loss: 0.1020\n",
      "Epoch: 26/100...  Loss: 0.0998\n",
      "Epoch: 26/100...  Loss: 0.0984\n",
      "Epoch: 26/100...  Loss: 0.1133\n",
      "Epoch: 26/100...  Loss: 0.1044\n",
      "Epoch: 26/100...  Loss: 0.1307\n",
      "Epoch: 26/100...  Loss: 0.1197\n",
      "Epoch: 27/100...  Loss: 0.1046\n",
      "Epoch: 27/100...  Loss: 0.1224\n",
      "Epoch: 27/100...  Loss: 0.1110\n",
      "Epoch: 27/100...  Loss: 0.1208\n",
      "Epoch: 27/100...  Loss: 0.0979\n",
      "Epoch: 27/100...  Loss: 0.1173\n",
      "Epoch: 27/100...  Loss: 0.1075\n",
      "Epoch: 27/100...  Loss: 0.0938\n",
      "Epoch: 27/100...  Loss: 0.0918\n",
      "Epoch: 27/100...  Loss: 0.0824\n",
      "Epoch: 28/100...  Loss: 0.0974\n",
      "Epoch: 28/100...  Loss: 0.1009\n",
      "Epoch: 28/100...  Loss: 0.0784\n",
      "Epoch: 28/100...  Loss: 0.1165\n",
      "Epoch: 28/100...  Loss: 0.0921\n",
      "Epoch: 28/100...  Loss: 0.1039\n",
      "Epoch: 28/100...  Loss: 0.0872\n",
      "Epoch: 28/100...  Loss: 0.0807\n",
      "Epoch: 28/100...  Loss: 0.0827\n",
      "Epoch: 28/100...  Loss: 0.0871\n",
      "Epoch: 29/100...  Loss: 0.1002\n",
      "Epoch: 29/100...  Loss: 0.0953\n",
      "Epoch: 29/100...  Loss: 0.0887\n",
      "Epoch: 29/100...  Loss: 0.1143\n",
      "Epoch: 29/100...  Loss: 0.0959\n",
      "Epoch: 29/100...  Loss: 0.1013\n",
      "Epoch: 29/100...  Loss: 0.0792\n",
      "Epoch: 29/100...  Loss: 0.0977\n",
      "Epoch: 29/100...  Loss: 0.0894\n",
      "Epoch: 29/100...  Loss: 0.0556\n",
      "Epoch: 30/100...  Loss: 0.0995\n",
      "Epoch: 30/100...  Loss: 0.0820\n",
      "Epoch: 30/100...  Loss: 0.0685\n",
      "Epoch: 30/100...  Loss: 0.0984\n",
      "Epoch: 30/100...  Loss: 0.0792\n",
      "Epoch: 30/100...  Loss: 0.0802\n",
      "Epoch: 30/100...  Loss: 0.0868\n",
      "Epoch: 30/100...  Loss: 0.0986\n",
      "Epoch: 30/100...  Loss: 0.0530\n",
      "Epoch: 30/100...  Loss: 0.0973\n",
      "Epoch: 31/100...  Loss: 0.0742\n",
      "Epoch: 31/100...  Loss: 0.0922\n",
      "Epoch: 31/100...  Loss: 0.0876\n",
      "Epoch: 31/100...  Loss: 0.1078\n",
      "Epoch: 31/100...  Loss: 0.0917\n",
      "Epoch: 31/100...  Loss: 0.1041\n",
      "Epoch: 31/100...  Loss: 0.0821\n",
      "Epoch: 31/100...  Loss: 0.1104\n",
      "Epoch: 31/100...  Loss: 0.0966\n",
      "Epoch: 31/100...  Loss: 0.0767\n",
      "Epoch: 32/100...  Loss: 0.1215\n",
      "Epoch: 32/100...  Loss: 0.1117\n",
      "Epoch: 32/100...  Loss: 0.1001\n",
      "Epoch: 32/100...  Loss: 0.0923\n",
      "Epoch: 32/100...  Loss: 0.0948\n",
      "Epoch: 32/100...  Loss: 0.0766\n",
      "Epoch: 32/100...  Loss: 0.0966\n",
      "Epoch: 32/100...  Loss: 0.0947\n",
      "Epoch: 32/100...  Loss: 0.0926\n",
      "Epoch: 32/100...  Loss: 0.0695\n",
      "Epoch: 33/100...  Loss: 0.0762\n",
      "Epoch: 33/100...  Loss: 0.0707\n",
      "Epoch: 33/100...  Loss: 0.0609\n",
      "Epoch: 33/100...  Loss: 0.0673\n",
      "Epoch: 33/100...  Loss: 0.0704\n",
      "Epoch: 33/100...  Loss: 0.0617\n",
      "Epoch: 33/100...  Loss: 0.0632\n",
      "Epoch: 33/100...  Loss: 0.0672\n",
      "Epoch: 33/100...  Loss: 0.0720\n",
      "Epoch: 33/100...  Loss: 0.0675\n",
      "Epoch: 34/100...  Loss: 0.0550\n",
      "Epoch: 34/100...  Loss: 0.0622\n",
      "Epoch: 34/100...  Loss: 0.0576\n",
      "Epoch: 34/100...  Loss: 0.0697\n",
      "Epoch: 34/100...  Loss: 0.0627\n",
      "Epoch: 34/100...  Loss: 0.0615\n",
      "Epoch: 34/100...  Loss: 0.0750\n",
      "Epoch: 34/100...  Loss: 0.0875\n",
      "Epoch: 34/100...  Loss: 0.1117\n",
      "Epoch: 34/100...  Loss: 0.0777\n",
      "Epoch: 35/100...  Loss: 0.0787\n",
      "Epoch: 35/100...  Loss: 0.0945\n",
      "Epoch: 35/100...  Loss: 0.1150\n",
      "Epoch: 35/100...  Loss: 0.1054\n",
      "Epoch: 35/100...  Loss: 0.0940\n",
      "Epoch: 35/100...  Loss: 0.0645\n",
      "Epoch: 35/100...  Loss: 0.0785\n",
      "Epoch: 35/100...  Loss: 0.0853\n",
      "Epoch: 35/100...  Loss: 0.0741\n",
      "Epoch: 35/100...  Loss: 0.0737\n",
      "Epoch: 36/100...  Loss: 0.0793\n",
      "Epoch: 36/100...  Loss: 0.0880\n",
      "Epoch: 36/100...  Loss: 0.0602\n",
      "Epoch: 36/100...  Loss: 0.1223\n",
      "Epoch: 36/100...  Loss: 0.0758\n",
      "Epoch: 36/100...  Loss: 0.0687\n",
      "Epoch: 36/100...  Loss: 0.0697\n",
      "Epoch: 36/100...  Loss: 0.0745\n",
      "Epoch: 36/100...  Loss: 0.0970\n",
      "Epoch: 36/100...  Loss: 0.0695\n",
      "Epoch: 37/100...  Loss: 0.0882\n",
      "Epoch: 37/100...  Loss: 0.1022\n",
      "Epoch: 37/100...  Loss: 0.0797\n",
      "Epoch: 37/100...  Loss: 0.0768\n",
      "Epoch: 37/100...  Loss: 0.0821\n",
      "Epoch: 37/100...  Loss: 0.0751\n",
      "Epoch: 37/100...  Loss: 0.0804\n",
      "Epoch: 37/100...  Loss: 0.0773\n",
      "Epoch: 37/100...  Loss: 0.0645\n",
      "Epoch: 37/100...  Loss: 0.1040\n",
      "Epoch: 38/100...  Loss: 0.0744\n",
      "Epoch: 38/100...  Loss: 0.0839\n",
      "Epoch: 38/100...  Loss: 0.0913\n",
      "Epoch: 38/100...  Loss: 0.0689\n",
      "Epoch: 38/100...  Loss: 0.0658\n",
      "Epoch: 38/100...  Loss: 0.0653\n",
      "Epoch: 38/100...  Loss: 0.0591\n",
      "Epoch: 38/100...  Loss: 0.0644\n",
      "Epoch: 38/100...  Loss: 0.0738\n",
      "Epoch: 38/100...  Loss: 0.0655\n",
      "Epoch: 39/100...  Loss: 0.0641\n",
      "Epoch: 39/100...  Loss: 0.0807\n",
      "Epoch: 39/100...  Loss: 0.0614\n",
      "Epoch: 39/100...  Loss: 0.0620\n",
      "Epoch: 39/100...  Loss: 0.0570\n",
      "Epoch: 39/100...  Loss: 0.0671\n",
      "Epoch: 39/100...  Loss: 0.0639\n",
      "Epoch: 39/100...  Loss: 0.0571\n",
      "Epoch: 39/100...  Loss: 0.1036\n",
      "Epoch: 39/100...  Loss: 0.0908\n",
      "Epoch: 40/100...  Loss: 0.0939\n",
      "Epoch: 40/100...  Loss: 0.0703\n",
      "Epoch: 40/100...  Loss: 0.0551\n",
      "Epoch: 40/100...  Loss: 0.1148\n",
      "Epoch: 40/100...  Loss: 0.0584\n",
      "Epoch: 40/100...  Loss: 0.0874\n",
      "Epoch: 40/100...  Loss: 0.0636\n",
      "Epoch: 40/100...  Loss: 0.0957\n",
      "Epoch: 40/100...  Loss: 0.0692\n",
      "Epoch: 40/100...  Loss: 0.0758\n",
      "Epoch: 41/100...  Loss: 0.0747\n",
      "Epoch: 41/100...  Loss: 0.0732\n",
      "Epoch: 41/100...  Loss: 0.0490\n",
      "Epoch: 41/100...  Loss: 0.0818\n",
      "Epoch: 41/100...  Loss: 0.0747\n",
      "Epoch: 41/100...  Loss: 0.0786\n",
      "Epoch: 41/100...  Loss: 0.0607\n",
      "Epoch: 41/100...  Loss: 0.0622\n",
      "Epoch: 41/100...  Loss: 0.0612\n",
      "Epoch: 41/100...  Loss: 0.0523\n",
      "Epoch: 42/100...  Loss: 0.0712\n",
      "Epoch: 42/100...  Loss: 0.0711\n",
      "Epoch: 42/100...  Loss: 0.0847\n",
      "Epoch: 42/100...  Loss: 0.1032\n",
      "Epoch: 42/100...  Loss: 0.0819\n",
      "Epoch: 42/100...  Loss: 0.0572\n",
      "Epoch: 42/100...  Loss: 0.0340\n",
      "Epoch: 42/100...  Loss: 0.0408\n",
      "Epoch: 42/100...  Loss: 0.0554\n",
      "Epoch: 42/100...  Loss: 0.0627\n",
      "Epoch: 43/100...  Loss: 0.0582\n",
      "Epoch: 43/100...  Loss: 0.0735\n",
      "Epoch: 43/100...  Loss: 0.0575\n",
      "Epoch: 43/100...  Loss: 0.0513\n",
      "Epoch: 43/100...  Loss: 0.0588\n",
      "Epoch: 43/100...  Loss: 0.0513\n",
      "Epoch: 43/100...  Loss: 0.0486\n",
      "Epoch: 43/100...  Loss: 0.0687\n",
      "Epoch: 43/100...  Loss: 0.0575\n",
      "Epoch: 43/100...  Loss: 0.0569\n",
      "Epoch: 44/100...  Loss: 0.0510\n",
      "Epoch: 44/100...  Loss: 0.0849\n",
      "Epoch: 44/100...  Loss: 0.0658\n",
      "Epoch: 44/100...  Loss: 0.0661\n",
      "Epoch: 44/100...  Loss: 0.0779\n",
      "Epoch: 44/100...  Loss: 0.0743\n",
      "Epoch: 44/100...  Loss: 0.0686\n",
      "Epoch: 44/100...  Loss: 0.0969\n",
      "Epoch: 44/100...  Loss: 0.1016\n",
      "Epoch: 44/100...  Loss: 0.0879\n",
      "Epoch: 45/100...  Loss: 0.0988\n",
      "Epoch: 45/100...  Loss: 0.0955\n",
      "Epoch: 45/100...  Loss: 0.0743\n",
      "Epoch: 45/100...  Loss: 0.0853\n",
      "Epoch: 45/100...  Loss: 0.0640\n",
      "Epoch: 45/100...  Loss: 0.0784\n",
      "Epoch: 45/100...  Loss: 0.0504\n",
      "Epoch: 45/100...  Loss: 0.0643\n",
      "Epoch: 45/100...  Loss: 0.1023\n",
      "Epoch: 45/100...  Loss: 0.0782\n",
      "Epoch: 46/100...  Loss: 0.0606\n",
      "Epoch: 46/100...  Loss: 0.0767\n",
      "Epoch: 46/100...  Loss: 0.0688\n",
      "Epoch: 46/100...  Loss: 0.0792\n",
      "Epoch: 46/100...  Loss: 0.0703\n",
      "Epoch: 46/100...  Loss: 0.0445\n",
      "Epoch: 46/100...  Loss: 0.0683\n",
      "Epoch: 46/100...  Loss: 0.0484\n",
      "Epoch: 46/100...  Loss: 0.0554\n",
      "Epoch: 46/100...  Loss: 0.0625\n",
      "Epoch: 47/100...  Loss: 0.0544\n",
      "Epoch: 47/100...  Loss: 0.0596\n",
      "Epoch: 47/100...  Loss: 0.0372\n",
      "Epoch: 47/100...  Loss: 0.0516\n",
      "Epoch: 47/100...  Loss: 0.0494\n",
      "Epoch: 47/100...  Loss: 0.0427\n",
      "Epoch: 47/100...  Loss: 0.0679\n",
      "Epoch: 47/100...  Loss: 0.0496\n",
      "Epoch: 47/100...  Loss: 0.0467\n",
      "Epoch: 47/100...  Loss: 0.0778\n",
      "Epoch: 48/100...  Loss: 0.0656\n",
      "Epoch: 48/100...  Loss: 0.0703\n",
      "Epoch: 48/100...  Loss: 0.0622\n",
      "Epoch: 48/100...  Loss: 0.0583\n",
      "Epoch: 48/100...  Loss: 0.0519\n",
      "Epoch: 48/100...  Loss: 0.0846\n",
      "Epoch: 48/100...  Loss: 0.0881\n",
      "Epoch: 48/100...  Loss: 0.0943\n",
      "Epoch: 48/100...  Loss: 0.0917\n",
      "Epoch: 48/100...  Loss: 0.0716\n",
      "Epoch: 49/100...  Loss: 0.0977\n",
      "Epoch: 49/100...  Loss: 0.0897\n",
      "Epoch: 49/100...  Loss: 0.0736\n",
      "Epoch: 49/100...  Loss: 0.0593\n",
      "Epoch: 49/100...  Loss: 0.0527\n",
      "Epoch: 49/100...  Loss: 0.0509\n",
      "Epoch: 49/100...  Loss: 0.0600\n",
      "Epoch: 49/100...  Loss: 0.0626\n",
      "Epoch: 49/100...  Loss: 0.0632\n",
      "Epoch: 49/100...  Loss: 0.0415\n",
      "Epoch: 50/100...  Loss: 0.0568\n",
      "Epoch: 50/100...  Loss: 0.0433\n",
      "Epoch: 50/100...  Loss: 0.0475\n",
      "Epoch: 50/100...  Loss: 0.0963\n",
      "Epoch: 50/100...  Loss: 0.1043\n",
      "Epoch: 50/100...  Loss: 0.0574\n",
      "Epoch: 50/100...  Loss: 0.0409\n",
      "Epoch: 50/100...  Loss: 0.0804\n",
      "Epoch: 50/100...  Loss: 0.0753\n",
      "Epoch: 50/100...  Loss: 0.0402\n",
      "Epoch: 51/100...  Loss: 0.0554\n",
      "Epoch: 51/100...  Loss: 0.0604\n",
      "Epoch: 51/100...  Loss: 0.0451\n",
      "Epoch: 51/100...  Loss: 0.0467\n",
      "Epoch: 51/100...  Loss: 0.0564\n",
      "Epoch: 51/100...  Loss: 0.0486\n",
      "Epoch: 51/100...  Loss: 0.0504\n",
      "Epoch: 51/100...  Loss: 0.0565\n",
      "Epoch: 51/100...  Loss: 0.0372\n",
      "Epoch: 51/100...  Loss: 0.0569\n",
      "Epoch: 52/100...  Loss: 0.0287\n",
      "Epoch: 52/100...  Loss: 0.0634\n",
      "Epoch: 52/100...  Loss: 0.0492\n",
      "Epoch: 52/100...  Loss: 0.0739\n",
      "Epoch: 52/100...  Loss: 0.0620\n",
      "Epoch: 52/100...  Loss: 0.0366\n",
      "Epoch: 52/100...  Loss: 0.0628\n",
      "Epoch: 52/100...  Loss: 0.0393\n",
      "Epoch: 52/100...  Loss: 0.0700\n",
      "Epoch: 52/100...  Loss: 0.0712\n",
      "Epoch: 53/100...  Loss: 0.0605\n",
      "Epoch: 53/100...  Loss: 0.0670\n",
      "Epoch: 53/100...  Loss: 0.0553\n",
      "Epoch: 53/100...  Loss: 0.0616\n",
      "Epoch: 53/100...  Loss: 0.0550\n",
      "Epoch: 53/100...  Loss: 0.0498\n",
      "Epoch: 53/100...  Loss: 0.0463\n",
      "Epoch: 53/100...  Loss: 0.0709\n",
      "Epoch: 53/100...  Loss: 0.0618\n",
      "Epoch: 53/100...  Loss: 0.0563\n",
      "Epoch: 54/100...  Loss: 0.0484\n",
      "Epoch: 54/100...  Loss: 0.0596\n",
      "Epoch: 54/100...  Loss: 0.0718\n",
      "Epoch: 54/100...  Loss: 0.0785\n",
      "Epoch: 54/100...  Loss: 0.0713\n",
      "Epoch: 54/100...  Loss: 0.0676\n",
      "Epoch: 54/100...  Loss: 0.0551\n",
      "Epoch: 54/100...  Loss: 0.0758\n",
      "Epoch: 54/100...  Loss: 0.0872\n",
      "Epoch: 54/100...  Loss: 0.0727\n",
      "Epoch: 55/100...  Loss: 0.0760\n",
      "Epoch: 55/100...  Loss: 0.0438\n",
      "Epoch: 55/100...  Loss: 0.0494\n",
      "Epoch: 55/100...  Loss: 0.0682\n",
      "Epoch: 55/100...  Loss: 0.0327\n",
      "Epoch: 55/100...  Loss: 0.0460\n",
      "Epoch: 55/100...  Loss: 0.0443\n",
      "Epoch: 55/100...  Loss: 0.0680\n",
      "Epoch: 55/100...  Loss: 0.0417\n",
      "Epoch: 55/100...  Loss: 0.0515\n",
      "Epoch: 56/100...  Loss: 0.0450\n",
      "Epoch: 56/100...  Loss: 0.0550\n",
      "Epoch: 56/100...  Loss: 0.0534\n",
      "Epoch: 56/100...  Loss: 0.0583\n",
      "Epoch: 56/100...  Loss: 0.0480\n",
      "Epoch: 56/100...  Loss: 0.0626\n",
      "Epoch: 56/100...  Loss: 0.0468\n",
      "Epoch: 56/100...  Loss: 0.0721\n",
      "Epoch: 56/100...  Loss: 0.0670\n",
      "Epoch: 56/100...  Loss: 0.0404\n",
      "Epoch: 57/100...  Loss: 0.0678\n",
      "Epoch: 57/100...  Loss: 0.0606\n",
      "Epoch: 57/100...  Loss: 0.0781\n",
      "Epoch: 57/100...  Loss: 0.0625\n",
      "Epoch: 57/100...  Loss: 0.0473\n",
      "Epoch: 57/100...  Loss: 0.0322\n",
      "Epoch: 57/100...  Loss: 0.0286\n",
      "Epoch: 57/100...  Loss: 0.0501\n",
      "Epoch: 57/100...  Loss: 0.0661\n",
      "Epoch: 57/100...  Loss: 0.0739\n",
      "Epoch: 58/100...  Loss: 0.0870\n",
      "Epoch: 58/100...  Loss: 0.0386\n",
      "Epoch: 58/100...  Loss: 0.0279\n",
      "Epoch: 58/100...  Loss: 0.0537\n",
      "Epoch: 58/100...  Loss: 0.0495\n",
      "Epoch: 58/100...  Loss: 0.0606\n",
      "Epoch: 58/100...  Loss: 0.0444\n",
      "Epoch: 58/100...  Loss: 0.0617\n",
      "Epoch: 58/100...  Loss: 0.0411\n",
      "Epoch: 58/100...  Loss: 0.0615\n",
      "Epoch: 59/100...  Loss: 0.0387\n",
      "Epoch: 59/100...  Loss: 0.0753\n",
      "Epoch: 59/100...  Loss: 0.0591\n",
      "Epoch: 59/100...  Loss: 0.0504\n",
      "Epoch: 59/100...  Loss: 0.0416\n",
      "Epoch: 59/100...  Loss: 0.0401\n",
      "Epoch: 59/100...  Loss: 0.0496\n",
      "Epoch: 59/100...  Loss: 0.0322\n",
      "Epoch: 59/100...  Loss: 0.0482\n",
      "Epoch: 59/100...  Loss: 0.0616\n",
      "Epoch: 60/100...  Loss: 0.0985\n",
      "Epoch: 60/100...  Loss: 0.0719\n",
      "Epoch: 60/100...  Loss: 0.0573\n",
      "Epoch: 60/100...  Loss: 0.0648\n",
      "Epoch: 60/100...  Loss: 0.0663\n",
      "Epoch: 60/100...  Loss: 0.0704\n",
      "Epoch: 60/100...  Loss: 0.0445\n",
      "Epoch: 60/100...  Loss: 0.0624\n",
      "Epoch: 60/100...  Loss: 0.0361\n",
      "Epoch: 60/100...  Loss: 0.0450\n",
      "Epoch: 61/100...  Loss: 0.0732\n",
      "Epoch: 61/100...  Loss: 0.0548\n",
      "Epoch: 61/100...  Loss: 0.0510\n",
      "Epoch: 61/100...  Loss: 0.0387\n",
      "Epoch: 61/100...  Loss: 0.0570\n",
      "Epoch: 61/100...  Loss: 0.0394\n",
      "Epoch: 61/100...  Loss: 0.0409\n",
      "Epoch: 61/100...  Loss: 0.0522\n",
      "Epoch: 61/100...  Loss: 0.0322\n",
      "Epoch: 61/100...  Loss: 0.0358\n",
      "Epoch: 62/100...  Loss: 0.0382\n",
      "Epoch: 62/100...  Loss: 0.0644\n",
      "Epoch: 62/100...  Loss: 0.0876\n",
      "Epoch: 62/100...  Loss: 0.0598\n",
      "Epoch: 62/100...  Loss: 0.0391\n",
      "Epoch: 62/100...  Loss: 0.0463\n",
      "Epoch: 62/100...  Loss: 0.0306\n",
      "Epoch: 62/100...  Loss: 0.0407\n",
      "Epoch: 62/100...  Loss: 0.0630\n",
      "Epoch: 62/100...  Loss: 0.0732\n",
      "Epoch: 63/100...  Loss: 0.0369\n",
      "Epoch: 63/100...  Loss: 0.0448\n",
      "Epoch: 63/100...  Loss: 0.0561\n",
      "Epoch: 63/100...  Loss: 0.0670\n",
      "Epoch: 63/100...  Loss: 0.0596\n",
      "Epoch: 63/100...  Loss: 0.0454\n",
      "Epoch: 63/100...  Loss: 0.0344\n",
      "Epoch: 63/100...  Loss: 0.0440\n",
      "Epoch: 63/100...  Loss: 0.0562\n",
      "Epoch: 63/100...  Loss: 0.0613\n",
      "Epoch: 64/100...  Loss: 0.0723\n",
      "Epoch: 64/100...  Loss: 0.0656\n",
      "Epoch: 64/100...  Loss: 0.0360\n",
      "Epoch: 64/100...  Loss: 0.0378\n",
      "Epoch: 64/100...  Loss: 0.0369\n",
      "Epoch: 64/100...  Loss: 0.0614\n",
      "Epoch: 64/100...  Loss: 0.0513\n",
      "Epoch: 64/100...  Loss: 0.0590\n",
      "Epoch: 64/100...  Loss: 0.0405\n",
      "Epoch: 64/100...  Loss: 0.0452\n",
      "Epoch: 65/100...  Loss: 0.0541\n",
      "Epoch: 65/100...  Loss: 0.0359\n",
      "Epoch: 65/100...  Loss: 0.0369\n",
      "Epoch: 65/100...  Loss: 0.0489\n",
      "Epoch: 65/100...  Loss: 0.0565\n",
      "Epoch: 65/100...  Loss: 0.0383\n",
      "Epoch: 65/100...  Loss: 0.0776\n",
      "Epoch: 65/100...  Loss: 0.0708\n",
      "Epoch: 65/100...  Loss: 0.0391\n",
      "Epoch: 65/100...  Loss: 0.0650\n",
      "Epoch: 66/100...  Loss: 0.0710\n",
      "Epoch: 66/100...  Loss: 0.0671\n",
      "Epoch: 66/100...  Loss: 0.0598\n",
      "Epoch: 66/100...  Loss: 0.0660\n",
      "Epoch: 66/100...  Loss: 0.0753\n",
      "Epoch: 66/100...  Loss: 0.0615\n",
      "Epoch: 66/100...  Loss: 0.0448\n",
      "Epoch: 66/100...  Loss: 0.0393\n",
      "Epoch: 66/100...  Loss: 0.0551\n",
      "Epoch: 66/100...  Loss: 0.0438\n",
      "Epoch: 67/100...  Loss: 0.0715\n",
      "Epoch: 67/100...  Loss: 0.0391\n",
      "Epoch: 67/100...  Loss: 0.0323\n",
      "Epoch: 67/100...  Loss: 0.0637\n",
      "Epoch: 67/100...  Loss: 0.0555\n",
      "Epoch: 67/100...  Loss: 0.0369\n",
      "Epoch: 67/100...  Loss: 0.0497\n",
      "Epoch: 67/100...  Loss: 0.0393\n",
      "Epoch: 67/100...  Loss: 0.0500\n",
      "Epoch: 67/100...  Loss: 0.0223\n",
      "Epoch: 68/100...  Loss: 0.0471\n",
      "Epoch: 68/100...  Loss: 0.0654\n",
      "Epoch: 68/100...  Loss: 0.0438\n",
      "Epoch: 68/100...  Loss: 0.1084\n",
      "Epoch: 68/100...  Loss: 0.0586\n",
      "Epoch: 68/100...  Loss: 0.0497\n",
      "Epoch: 68/100...  Loss: 0.0403\n",
      "Epoch: 68/100...  Loss: 0.0404\n",
      "Epoch: 68/100...  Loss: 0.0484\n",
      "Epoch: 68/100...  Loss: 0.0382\n",
      "Epoch: 69/100...  Loss: 0.0511\n",
      "Epoch: 69/100...  Loss: 0.0967\n",
      "Epoch: 69/100...  Loss: 0.0331\n",
      "Epoch: 69/100...  Loss: 0.0380\n",
      "Epoch: 69/100...  Loss: 0.0657\n",
      "Epoch: 69/100...  Loss: 0.0464\n",
      "Epoch: 69/100...  Loss: 0.0407\n",
      "Epoch: 69/100...  Loss: 0.0318\n",
      "Epoch: 69/100...  Loss: 0.0506\n",
      "Epoch: 69/100...  Loss: 0.0481\n",
      "Epoch: 70/100...  Loss: 0.0439\n",
      "Epoch: 70/100...  Loss: 0.0293\n",
      "Epoch: 70/100...  Loss: 0.0293\n",
      "Epoch: 70/100...  Loss: 0.0290\n",
      "Epoch: 70/100...  Loss: 0.0542\n",
      "Epoch: 70/100...  Loss: 0.0484\n",
      "Epoch: 70/100...  Loss: 0.0346\n",
      "Epoch: 70/100...  Loss: 0.0377\n",
      "Epoch: 70/100...  Loss: 0.0488\n",
      "Epoch: 70/100...  Loss: 0.0374\n",
      "Epoch: 71/100...  Loss: 0.0409\n",
      "Epoch: 71/100...  Loss: 0.0541\n",
      "Epoch: 71/100...  Loss: 0.0459\n",
      "Epoch: 71/100...  Loss: 0.0673\n",
      "Epoch: 71/100...  Loss: 0.0482\n",
      "Epoch: 71/100...  Loss: 0.0621\n",
      "Epoch: 71/100...  Loss: 0.0614\n",
      "Epoch: 71/100...  Loss: 0.0573\n",
      "Epoch: 71/100...  Loss: 0.0549\n",
      "Epoch: 71/100...  Loss: 0.0230\n",
      "Epoch: 72/100...  Loss: 0.0420\n",
      "Epoch: 72/100...  Loss: 0.0646\n",
      "Epoch: 72/100...  Loss: 0.0630\n",
      "Epoch: 72/100...  Loss: 0.0497\n",
      "Epoch: 72/100...  Loss: 0.0487\n",
      "Epoch: 72/100...  Loss: 0.0269\n",
      "Epoch: 72/100...  Loss: 0.0371\n",
      "Epoch: 72/100...  Loss: 0.0275\n",
      "Epoch: 72/100...  Loss: 0.0399\n",
      "Epoch: 72/100...  Loss: 0.0277\n",
      "Epoch: 73/100...  Loss: 0.0616\n",
      "Epoch: 73/100...  Loss: 0.0437\n",
      "Epoch: 73/100...  Loss: 0.0335\n",
      "Epoch: 73/100...  Loss: 0.0671\n",
      "Epoch: 73/100...  Loss: 0.0660\n",
      "Epoch: 73/100...  Loss: 0.0505\n",
      "Epoch: 73/100...  Loss: 0.0440\n",
      "Epoch: 73/100...  Loss: 0.0412\n",
      "Epoch: 73/100...  Loss: 0.1014\n",
      "Epoch: 73/100...  Loss: 0.0557\n",
      "Epoch: 74/100...  Loss: 0.0703\n",
      "Epoch: 74/100...  Loss: 0.0475\n",
      "Epoch: 74/100...  Loss: 0.0522\n",
      "Epoch: 74/100...  Loss: 0.0555\n",
      "Epoch: 74/100...  Loss: 0.0451\n",
      "Epoch: 74/100...  Loss: 0.0450\n",
      "Epoch: 74/100...  Loss: 0.0433\n",
      "Epoch: 74/100...  Loss: 0.0536\n",
      "Epoch: 74/100...  Loss: 0.0471\n",
      "Epoch: 74/100...  Loss: 0.0549\n",
      "Epoch: 75/100...  Loss: 0.0477\n",
      "Epoch: 75/100...  Loss: 0.0410\n",
      "Epoch: 75/100...  Loss: 0.0449\n",
      "Epoch: 75/100...  Loss: 0.0492\n",
      "Epoch: 75/100...  Loss: 0.0590\n",
      "Epoch: 75/100...  Loss: 0.0351\n",
      "Epoch: 75/100...  Loss: 0.0419\n",
      "Epoch: 75/100...  Loss: 0.0564\n",
      "Epoch: 75/100...  Loss: 0.0410\n",
      "Epoch: 75/100...  Loss: 0.0329\n",
      "Epoch: 76/100...  Loss: 0.0452\n",
      "Epoch: 76/100...  Loss: 0.0345\n",
      "Epoch: 76/100...  Loss: 0.0531\n",
      "Epoch: 76/100...  Loss: 0.0508\n",
      "Epoch: 76/100...  Loss: 0.0576\n",
      "Epoch: 76/100...  Loss: 0.0477\n",
      "Epoch: 76/100...  Loss: 0.0506\n",
      "Epoch: 76/100...  Loss: 0.0474\n",
      "Epoch: 76/100...  Loss: 0.0430\n",
      "Epoch: 76/100...  Loss: 0.0399\n",
      "Epoch: 77/100...  Loss: 0.0507\n",
      "Epoch: 77/100...  Loss: 0.0285\n",
      "Epoch: 77/100...  Loss: 0.0207\n",
      "Epoch: 77/100...  Loss: 0.0484\n",
      "Epoch: 77/100...  Loss: 0.0432\n",
      "Epoch: 77/100...  Loss: 0.0488\n",
      "Epoch: 77/100...  Loss: 0.0347\n",
      "Epoch: 77/100...  Loss: 0.0376\n",
      "Epoch: 77/100...  Loss: 0.0621\n",
      "Epoch: 77/100...  Loss: 0.0631\n",
      "Epoch: 78/100...  Loss: 0.0484\n",
      "Epoch: 78/100...  Loss: 0.0487\n",
      "Epoch: 78/100...  Loss: 0.0638\n",
      "Epoch: 78/100...  Loss: 0.0765\n",
      "Epoch: 78/100...  Loss: 0.0403\n",
      "Epoch: 78/100...  Loss: 0.0783\n",
      "Epoch: 78/100...  Loss: 0.0392\n",
      "Epoch: 78/100...  Loss: 0.0520\n",
      "Epoch: 78/100...  Loss: 0.0549\n",
      "Epoch: 78/100...  Loss: 0.0363\n",
      "Epoch: 79/100...  Loss: 0.0445\n",
      "Epoch: 79/100...  Loss: 0.0481\n",
      "Epoch: 79/100...  Loss: 0.0393\n",
      "Epoch: 79/100...  Loss: 0.0538\n",
      "Epoch: 79/100...  Loss: 0.0542\n",
      "Epoch: 79/100...  Loss: 0.0417\n",
      "Epoch: 79/100...  Loss: 0.0597\n",
      "Epoch: 79/100...  Loss: 0.0442\n",
      "Epoch: 79/100...  Loss: 0.0494\n",
      "Epoch: 79/100...  Loss: 0.0634\n",
      "Epoch: 80/100...  Loss: 0.0389\n",
      "Epoch: 80/100...  Loss: 0.0358\n",
      "Epoch: 80/100...  Loss: 0.0291\n",
      "Epoch: 80/100...  Loss: 0.0492\n",
      "Epoch: 80/100...  Loss: 0.0459\n",
      "Epoch: 80/100...  Loss: 0.0340\n",
      "Epoch: 80/100...  Loss: 0.0331\n",
      "Epoch: 80/100...  Loss: 0.0532\n",
      "Epoch: 80/100...  Loss: 0.0462\n",
      "Epoch: 80/100...  Loss: 0.0285\n",
      "Epoch: 81/100...  Loss: 0.0328\n",
      "Epoch: 81/100...  Loss: 0.0258\n",
      "Epoch: 81/100...  Loss: 0.0411\n",
      "Epoch: 81/100...  Loss: 0.0468\n",
      "Epoch: 81/100...  Loss: 0.0358\n",
      "Epoch: 81/100...  Loss: 0.0308\n",
      "Epoch: 81/100...  Loss: 0.0387\n",
      "Epoch: 81/100...  Loss: 0.0472\n",
      "Epoch: 81/100...  Loss: 0.0449\n",
      "Epoch: 81/100...  Loss: 0.0902\n",
      "Epoch: 82/100...  Loss: 0.0528\n",
      "Epoch: 82/100...  Loss: 0.0653\n",
      "Epoch: 82/100...  Loss: 0.0425\n",
      "Epoch: 82/100...  Loss: 0.0324\n",
      "Epoch: 82/100...  Loss: 0.0522\n",
      "Epoch: 82/100...  Loss: 0.0272\n",
      "Epoch: 82/100...  Loss: 0.0520\n",
      "Epoch: 82/100...  Loss: 0.0619\n",
      "Epoch: 82/100...  Loss: 0.0531\n",
      "Epoch: 82/100...  Loss: 0.0426\n",
      "Epoch: 83/100...  Loss: 0.0645\n",
      "Epoch: 83/100...  Loss: 0.0861\n",
      "Epoch: 83/100...  Loss: 0.0532\n",
      "Epoch: 83/100...  Loss: 0.0513\n",
      "Epoch: 83/100...  Loss: 0.0418\n",
      "Epoch: 83/100...  Loss: 0.0482\n",
      "Epoch: 83/100...  Loss: 0.0470\n",
      "Epoch: 83/100...  Loss: 0.0568\n",
      "Epoch: 83/100...  Loss: 0.0465\n",
      "Epoch: 83/100...  Loss: 0.0449\n",
      "Epoch: 84/100...  Loss: 0.0621\n",
      "Epoch: 84/100...  Loss: 0.0380\n",
      "Epoch: 84/100...  Loss: 0.0471\n",
      "Epoch: 84/100...  Loss: 0.0517\n",
      "Epoch: 84/100...  Loss: 0.0246\n",
      "Epoch: 84/100...  Loss: 0.0403\n",
      "Epoch: 84/100...  Loss: 0.0374\n",
      "Epoch: 84/100...  Loss: 0.0536\n",
      "Epoch: 84/100...  Loss: 0.0415\n",
      "Epoch: 84/100...  Loss: 0.0311\n",
      "Epoch: 85/100...  Loss: 0.0409\n",
      "Epoch: 85/100...  Loss: 0.0272\n",
      "Epoch: 85/100...  Loss: 0.0437\n",
      "Epoch: 85/100...  Loss: 0.0507\n",
      "Epoch: 85/100...  Loss: 0.0394\n",
      "Epoch: 85/100...  Loss: 0.0404\n",
      "Epoch: 85/100...  Loss: 0.0339\n",
      "Epoch: 85/100...  Loss: 0.0226\n",
      "Epoch: 85/100...  Loss: 0.0289\n",
      "Epoch: 85/100...  Loss: 0.0373\n",
      "Epoch: 86/100...  Loss: 0.0390\n",
      "Epoch: 86/100...  Loss: 0.0314\n",
      "Epoch: 86/100...  Loss: 0.0437\n",
      "Epoch: 86/100...  Loss: 0.0253\n",
      "Epoch: 86/100...  Loss: 0.0393\n",
      "Epoch: 86/100...  Loss: 0.0340\n",
      "Epoch: 86/100...  Loss: 0.0556\n",
      "Epoch: 86/100...  Loss: 0.0398\n",
      "Epoch: 86/100...  Loss: 0.0595\n",
      "Epoch: 86/100...  Loss: 0.0423\n",
      "Epoch: 87/100...  Loss: 0.1098\n",
      "Epoch: 87/100...  Loss: 0.0311\n",
      "Epoch: 87/100...  Loss: 0.0574\n",
      "Epoch: 87/100...  Loss: 0.0553\n",
      "Epoch: 87/100...  Loss: 0.0511\n",
      "Epoch: 87/100...  Loss: 0.0443\n",
      "Epoch: 87/100...  Loss: 0.0364\n",
      "Epoch: 87/100...  Loss: 0.0377\n",
      "Epoch: 87/100...  Loss: 0.0532\n",
      "Epoch: 87/100...  Loss: 0.0347\n",
      "Epoch: 88/100...  Loss: 0.0562\n",
      "Epoch: 88/100...  Loss: 0.0843\n",
      "Epoch: 88/100...  Loss: 0.0313\n",
      "Epoch: 88/100...  Loss: 0.0620\n",
      "Epoch: 88/100...  Loss: 0.0533\n",
      "Epoch: 88/100...  Loss: 0.0379\n",
      "Epoch: 88/100...  Loss: 0.0214\n",
      "Epoch: 88/100...  Loss: 0.0347\n",
      "Epoch: 88/100...  Loss: 0.0520\n",
      "Epoch: 88/100...  Loss: 0.0492\n",
      "Epoch: 89/100...  Loss: 0.0424\n",
      "Epoch: 89/100...  Loss: 0.0328\n",
      "Epoch: 89/100...  Loss: 0.0206\n",
      "Epoch: 89/100...  Loss: 0.0352\n",
      "Epoch: 89/100...  Loss: 0.0565\n",
      "Epoch: 89/100...  Loss: 0.0467\n",
      "Epoch: 89/100...  Loss: 0.0442\n",
      "Epoch: 89/100...  Loss: 0.0338\n",
      "Epoch: 89/100...  Loss: 0.0348\n",
      "Epoch: 89/100...  Loss: 0.0438\n",
      "Epoch: 90/100...  Loss: 0.0239\n",
      "Epoch: 90/100...  Loss: 0.0204\n",
      "Epoch: 90/100...  Loss: 0.0359\n",
      "Epoch: 90/100...  Loss: 0.0271\n",
      "Epoch: 90/100...  Loss: 0.0425\n",
      "Epoch: 90/100...  Loss: 0.0384\n",
      "Epoch: 90/100...  Loss: 0.0596\n",
      "Epoch: 90/100...  Loss: 0.0411\n",
      "Epoch: 90/100...  Loss: 0.0402\n",
      "Epoch: 90/100...  Loss: 0.0418\n",
      "Epoch: 91/100...  Loss: 0.0622\n",
      "Epoch: 91/100...  Loss: 0.0649\n",
      "Epoch: 91/100...  Loss: 0.0454\n",
      "Epoch: 91/100...  Loss: 0.0291\n",
      "Epoch: 91/100...  Loss: 0.0370\n",
      "Epoch: 91/100...  Loss: 0.0310\n",
      "Epoch: 91/100...  Loss: 0.0352\n",
      "Epoch: 91/100...  Loss: 0.0486\n",
      "Epoch: 91/100...  Loss: 0.0291\n",
      "Epoch: 91/100...  Loss: 0.0509\n",
      "Epoch: 92/100...  Loss: 0.0358\n",
      "Epoch: 92/100...  Loss: 0.0574\n",
      "Epoch: 92/100...  Loss: 0.0505\n",
      "Epoch: 92/100...  Loss: 0.0520\n",
      "Epoch: 92/100...  Loss: 0.0352\n",
      "Epoch: 92/100...  Loss: 0.0499\n",
      "Epoch: 92/100...  Loss: 0.0637\n",
      "Epoch: 92/100...  Loss: 0.0552\n",
      "Epoch: 92/100...  Loss: 0.0641\n",
      "Epoch: 92/100...  Loss: 0.0367\n",
      "Epoch: 93/100...  Loss: 0.0473\n",
      "Epoch: 93/100...  Loss: 0.0473\n",
      "Epoch: 93/100...  Loss: 0.0627\n",
      "Epoch: 93/100...  Loss: 0.0960\n",
      "Epoch: 93/100...  Loss: 0.0555\n",
      "Epoch: 93/100...  Loss: 0.0319\n",
      "Epoch: 93/100...  Loss: 0.0425\n",
      "Epoch: 93/100...  Loss: 0.0250\n",
      "Epoch: 93/100...  Loss: 0.0376\n",
      "Epoch: 93/100...  Loss: 0.0317\n",
      "Epoch: 94/100...  Loss: 0.0626\n",
      "Epoch: 94/100...  Loss: 0.0375\n",
      "Epoch: 94/100...  Loss: 0.0171\n",
      "Epoch: 94/100...  Loss: 0.0261\n",
      "Epoch: 94/100...  Loss: 0.0381\n",
      "Epoch: 94/100...  Loss: 0.0288\n",
      "Epoch: 94/100...  Loss: 0.0335\n",
      "Epoch: 94/100...  Loss: 0.0303\n",
      "Epoch: 94/100...  Loss: 0.0265\n",
      "Epoch: 94/100...  Loss: 0.0380\n",
      "Epoch: 95/100...  Loss: 0.0395\n",
      "Epoch: 95/100...  Loss: 0.0486\n",
      "Epoch: 95/100...  Loss: 0.0514\n",
      "Epoch: 95/100...  Loss: 0.0361\n",
      "Epoch: 95/100...  Loss: 0.0470\n",
      "Epoch: 95/100...  Loss: 0.0504\n",
      "Epoch: 95/100...  Loss: 0.0414\n",
      "Epoch: 95/100...  Loss: 0.0557\n",
      "Epoch: 95/100...  Loss: 0.0392\n",
      "Epoch: 95/100...  Loss: 0.0720\n",
      "Epoch: 96/100...  Loss: 0.0413\n",
      "Epoch: 96/100...  Loss: 0.0285\n",
      "Epoch: 96/100...  Loss: 0.0581\n",
      "Epoch: 96/100...  Loss: 0.0581\n",
      "Epoch: 96/100...  Loss: 0.0607\n",
      "Epoch: 96/100...  Loss: 0.0608\n",
      "Epoch: 96/100...  Loss: 0.0565\n",
      "Epoch: 96/100...  Loss: 0.0279\n",
      "Epoch: 96/100...  Loss: 0.0346\n",
      "Epoch: 96/100...  Loss: 0.0364\n",
      "Epoch: 97/100...  Loss: 0.0363\n",
      "Epoch: 97/100...  Loss: 0.0603\n",
      "Epoch: 97/100...  Loss: 0.0415\n",
      "Epoch: 97/100...  Loss: 0.0340\n",
      "Epoch: 97/100...  Loss: 0.0268\n",
      "Epoch: 97/100...  Loss: 0.0164\n",
      "Epoch: 97/100...  Loss: 0.0309\n",
      "Epoch: 97/100...  Loss: 0.0488\n",
      "Epoch: 97/100...  Loss: 0.0360\n",
      "Epoch: 97/100...  Loss: 0.0201\n",
      "Epoch: 98/100...  Loss: 0.0318\n",
      "Epoch: 98/100...  Loss: 0.0267\n",
      "Epoch: 98/100...  Loss: 0.0457\n",
      "Epoch: 98/100...  Loss: 0.0632\n",
      "Epoch: 98/100...  Loss: 0.0412\n",
      "Epoch: 98/100...  Loss: 0.0509\n",
      "Epoch: 98/100...  Loss: 0.0518\n",
      "Epoch: 98/100...  Loss: 0.0495\n",
      "Epoch: 98/100...  Loss: 0.0595\n",
      "Epoch: 98/100...  Loss: 0.0355\n",
      "Epoch: 99/100...  Loss: 0.0390\n",
      "Epoch: 99/100...  Loss: 0.0307\n",
      "Epoch: 99/100...  Loss: 0.0327\n",
      "Epoch: 99/100...  Loss: 0.0500\n",
      "Epoch: 99/100...  Loss: 0.0630\n",
      "Epoch: 99/100...  Loss: 0.0669\n",
      "Epoch: 99/100...  Loss: 0.0650\n",
      "Epoch: 99/100...  Loss: 0.0606\n",
      "Epoch: 99/100...  Loss: 0.0354\n",
      "Epoch: 99/100...  Loss: 0.0454\n",
      "Epoch: 100/100...  Loss: 0.0299\n",
      "Epoch: 100/100...  Loss: 0.0467\n",
      "Epoch: 100/100...  Loss: 0.0280\n",
      "Epoch: 100/100...  Loss: 0.0662\n",
      "Epoch: 100/100...  Loss: 0.0494\n",
      "Epoch: 100/100...  Loss: 0.0614\n",
      "Epoch: 100/100...  Loss: 0.0337\n",
      "Epoch: 100/100...  Loss: 0.0378\n",
      "Epoch: 100/100...  Loss: 0.0396\n",
      "Epoch: 100/100...  Loss: 0.0350\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "type(X_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "type(X_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "type(y_train_shuffled)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "type(y_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "X_test_numpy = X_test.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "evaluate_model(cnn_model, X_train_shuffled_numpy, y_train_shuffled_numpy, X_test_numpy, y_test_numpy, architecture=\"conv\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-2e94f004c07e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_shuffled_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_shuffled_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-13bcbc9a06e9>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, train, y_train, test, y_test, architecture)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitecture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'nn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_pred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-c1c5a60cc26c>\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(model, input, architecture)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('dl-env': conda)"
  },
  "interpreter": {
   "hash": "2aac1d2b359798efc30b59d804167ffd538e3092f84ffa86abb6645ab952d97e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}